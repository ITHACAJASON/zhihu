#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæ‰¹é‡é‡‡é›†æ•°æ®åº“ä¸­æœªå¤„ç†é—®é¢˜çš„ç­”æ¡ˆæ•°æ®
åŒ…å«åçˆ¬å¤„ç†å’Œè‡ªåŠ¨éªŒè¯è§£å†³åŠŸèƒ½
"""

import os
import json
import time
import hashlib
import subprocess
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from loguru import logger

from zhihu_api_crawler import ZhihuAPIAnswerCrawler
from postgres_models import PostgreSQLManager, TaskInfo, Question, Answer


class EnhancedBatchQuestionCrawler:
    """å¢å¼ºç‰ˆæ‰¹é‡é‡‡é›†æ•°æ®åº“ä¸­é—®é¢˜çš„çˆ¬è™«ç±»"""

    def __init__(self):
        self.api_crawler = ZhihuAPIAnswerCrawler()
        self.db_manager = PostgreSQLManager()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("output") / "batch_crawl_enhanced"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åçˆ¬å¤„ç†é…ç½®
        self.max_403_retries = 3
        self.verification_wait_time = 60  # éªŒè¯åç­‰å¾…æ—¶é—´
        
        logger.info("å¢å¼ºç‰ˆæ‰¹é‡é—®é¢˜çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")

    def get_unprocessed_questions(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æœªå¤„ç†çš„é—®é¢˜"""
        try:
            with self.db_manager.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT question_id, title, url, answer_count, task_id
                    FROM questions 
                    WHERE processed = FALSE
                    ORDER BY answer_count DESC
                """)
                
                questions = []
                for row in cursor.fetchall():
                    questions.append({
                        'question_id': row[0],
                        'title': row[1] or 'æœªçŸ¥æ ‡é¢˜',
                        'url': row[2],
                        'answer_count': row[3],
                        'task_id': row[4]
                    })
                
                return questions
                
        except Exception as e:
            logger.error(f"è·å–æœªå¤„ç†é—®é¢˜å¤±è´¥: {e}")
            return []

    def _handle_403_error(self) -> bool:
        """å¤„ç†403é”™è¯¯ï¼Œå°è¯•è‡ªåŠ¨éªŒè¯è§£å†³"""
        logger.warning("âš ï¸ æ£€æµ‹åˆ°403é”™è¯¯ï¼Œå°è¯•è‡ªåŠ¨éªŒè¯è§£å†³...")
        
        try:
            # è¿è¡ŒéªŒè¯è§£å†³è„šæœ¬
            result = subprocess.run(
                ["python3", "resolve_verification.py"],
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            if result.returncode == 0:
                logger.info("âœ… éªŒè¯è§£å†³æˆåŠŸ")
                logger.info(f"ç­‰å¾… {self.verification_wait_time} ç§’åç»§ç»­...")
                time.sleep(self.verification_wait_time)
                return True
            else:
                logger.error(f"éªŒè¯è§£å†³å¤±è´¥: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.error("éªŒè¯è§£å†³è¶…æ—¶")
            return False
        except Exception as e:
            logger.error(f"éªŒè¯è§£å†³è¿‡ç¨‹å‡ºé”™: {e}")
            return False

    def _save_api_response(self, response_data: Dict, question_id: str, page_num: int,
                         cursor: str = None, offset: int = 0) -> str:
        """ä¿å­˜APIå“åº”æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            # åˆ›å»ºé—®é¢˜ä¸“ç”¨ç›®å½•
            question_dir = self.output_dir / f"question_{question_id}"
            question_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            if cursor:
                filename = f"api_response_page_{page_num}_cursor_{cursor[:10]}_{timestamp}.json"
            else:
                filename = f"api_response_page_{page_num}_offset_{offset}_{timestamp}.json"

            filepath = question_dir / filename

            # æ·»åŠ å…ƒæ•°æ®
            response_with_meta = {
                "metadata": {
                    "question_id": question_id,
                    "page_num": page_num,
                    "cursor": cursor,
                    "offset": offset,
                    "timestamp": datetime.now().isoformat(),
                    "response_hash": hashlib.md5(json.dumps(response_data, sort_keys=True).encode()).hexdigest()
                },
                "response": response_data
            }

            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(response_with_meta, f, ensure_ascii=False, indent=2)

            logger.info(f"âœ… APIå“åº”å·²ä¿å­˜: {filepath}")
            return str(filepath)

        except Exception as e:
            logger.error(f"ä¿å­˜APIå“åº”å¤±è´¥: {e}")
            return ""

    def _parse_answers_from_api(self, answers_data: List[Dict], question_id: str, task_id: str) -> List[Answer]:
        """ä»APIå“åº”ä¸­è§£æç­”æ¡ˆ"""
        answers = []
        for answer_data in answers_data:
            answer = self.api_crawler.parse_answer_data(answer_data, question_id, task_id)
            if answer:
                answers.append(answer)
        return answers

    def crawl_question_answers_with_retry(self, question_info: Dict, max_answers: int = None) -> Dict:
        """å¸¦é‡è¯•æœºåˆ¶çš„çˆ¬å–å•ä¸ªé—®é¢˜çš„æ‰€æœ‰ç­”æ¡ˆ"""
        question_id = question_info['question_id']
        question_url = question_info['url']
        task_id = question_info['task_id']
        expected_answers = question_info['answer_count']
        
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–é—®é¢˜ {question_id} çš„æ‰€æœ‰ç­”æ¡ˆ")
        logger.info(f"é—®é¢˜æ ‡é¢˜: {question_info['title']}")
        logger.info(f"é¢„æœŸç­”æ¡ˆæ•°é‡: {expected_answers}")

        start_time = time.time()
        all_answers = []
        saved_files = []
        cursor = None
        offset = 0
        limit = 20
        page_count = 0
        total_api_calls = 0
        consecutive_403_errors = 0

        try:
            while True:
                page_count += 1
                total_api_calls += 1

                logger.info(f"ğŸ“„ è·å–ç¬¬ {page_count} é¡µç­”æ¡ˆ (cursor={cursor}, offset={offset})")

                # è·å–æ•°æ®
                response_data = self.api_crawler.fetch_answers_page(
                    question_id, cursor, offset, limit,
                    save_response_callback=lambda data, page, c, o: self._save_api_response(data, question_id, page, c, o),
                    page_num=page_count
                )
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯403é”™è¯¯å¯¼è‡´çš„ç©ºå“åº”
                if not response_data:
                    consecutive_403_errors += 1
                    logger.warning(f"ç¬¬ {page_count} é¡µæ•°æ®è·å–å¤±è´¥ï¼Œè¿ç»­å¤±è´¥æ¬¡æ•°: {consecutive_403_errors}")
                    
                    if consecutive_403_errors >= self.max_403_retries:
                        logger.error(f"è¿ç»­ {consecutive_403_errors} æ¬¡è·å–å¤±è´¥ï¼Œå°è¯•éªŒè¯è§£å†³...")
                        
                        if self._handle_403_error():
                            consecutive_403_errors = 0  # é‡ç½®å¤±è´¥è®¡æ•°
                            logger.info("éªŒè¯è§£å†³æˆåŠŸï¼Œç»§ç»­çˆ¬å–...")
                            continue
                        else:
                            logger.error("éªŒè¯è§£å†³å¤±è´¥ï¼Œåœæ­¢çˆ¬å–å½“å‰é—®é¢˜")
                            break
                    else:
                        # ç®€å•é‡è¯•
                        logger.info(f"ç­‰å¾…10ç§’åé‡è¯•...")
                        time.sleep(10)
                        continue
                
                # æˆåŠŸè·å–æ•°æ®ï¼Œé‡ç½®å¤±è´¥è®¡æ•°
                consecutive_403_errors = 0
                saved_files.append(f"page_{page_count}_saved")

                # è§£æåˆ†é¡µä¿¡æ¯
                paging = response_data.get('paging', {})
                is_end = paging.get('is_end', True)
                next_url = paging.get('next', '')

                # è·å–ç­”æ¡ˆæ•°æ®
                answers_data = response_data.get('data', [])
                logger.info(f"ğŸ“¦ ç¬¬ {page_count} é¡µè·å–åˆ° {len(answers_data)} ä¸ªç­”æ¡ˆ")

                # è§£æç­”æ¡ˆæ•°æ®
                page_answers = self._parse_answers_from_api(answers_data, question_id, task_id)
                logger.info(f"ğŸ“ æœ¬é¡µè§£æå‡º {len(page_answers)} ä¸ªæœ‰æ•ˆç­”æ¡ˆ")

                # æ·»åŠ åˆ°æ€»ç­”æ¡ˆåˆ—è¡¨
                all_answers.extend(page_answers)

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§ç­”æ¡ˆæ•°é™åˆ¶
                if max_answers and len(all_answers) >= max_answers:
                    logger.info(f"âœ… å·²è¾¾åˆ°æœ€å¤§ç­”æ¡ˆæ•°é™åˆ¶: {max_answers}")
                    break

                # æ£€æŸ¥æ˜¯å¦å·²ç»åˆ°æœ€åä¸€é¡µ
                if is_end:
                    logger.info(f"ğŸ¯ å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break

                # è§£æä¸‹ä¸€é¡µå‚æ•°
                if next_url:
                    next_params = self.api_crawler._parse_next_url_params(next_url)
                    if 'cursor' in next_params:
                        cursor = next_params['cursor']
                        logger.info(f"ğŸ”„ æ›´æ–°cursor: {cursor}")
                    elif 'offset' in next_params:
                        offset = int(next_params['offset'])
                        cursor = None
                        logger.info(f"ğŸ”„ æ›´æ–°offset: {offset}")
                    else:
                        offset += limit
                        cursor = None
                        logger.info(f"ğŸ”„ é€’å¢offset: {offset}")
                else:
                    offset += limit
                    cursor = None
                    logger.info(f"ğŸ”„ é€’å¢offset: {offset}")

                # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(5)  # å¢åŠ å»¶æ—¶

                # å®‰å…¨æ£€æŸ¥ï¼šé¿å…æ— é™å¾ªç¯
                if page_count > 500:
                    logger.warning(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ï¼Œåœæ­¢çˆ¬å–")
                    break

            # ä¿å­˜ç­”æ¡ˆåˆ°æ•°æ®åº“
            saved_to_db = False
            if all_answers:
                saved_to_db = self._save_answers_to_db(all_answers)

            # æ ‡è®°é—®é¢˜ä¸ºå·²å¤„ç†
            if all_answers:
                self._mark_question_processed(question_id, task_id)

            end_time = time.time()
            duration = end_time - start_time

            result = {
                'question_id': question_id,
                'question_url': question_url,
                'task_id': task_id,
                'title': question_info['title'],
                'expected_answers': expected_answers,
                'total_answers': len(all_answers),
                'total_pages': page_count,
                'total_api_calls': total_api_calls,
                'saved_files_count': len(saved_files),
                'saved_files': saved_files,
                'saved_to_db': saved_to_db,
                'duration_seconds': round(duration, 2),
                'crawl_time': datetime.now().isoformat(),
                'completion_rate': round(len(all_answers) / expected_answers * 100, 2) if expected_answers > 0 else 0,
                'had_403_errors': consecutive_403_errors > 0
            }

            logger.info(f"ğŸ‰ é—®é¢˜ {question_id} ç­”æ¡ˆçˆ¬å–å®Œæˆ")
            logger.info(f"ğŸ“Š æ€»å…±è·å–åˆ° {len(all_answers)} ä¸ªç­”æ¡ˆ")
            logger.info(f"ğŸ“„ å…±è¯·æ±‚äº† {page_count} é¡µæ•°æ®")
            logger.info(f"ğŸ’¾ ä¿å­˜äº† {len(saved_files)} ä¸ªAPIå“åº”æ–‡ä»¶")
            logger.info(f"â±ï¸ è€—æ—¶ {duration:.2f} ç§’")
            logger.info(f"ğŸ“ˆ å®Œæˆç‡: {result['completion_rate']}%")

            return result

        except Exception as e:
            logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {
                'question_id': question_id,
                'error': str(e),
                'total_answers': len(all_answers),
                'saved_files_count': len(saved_files),
                'saved_files': saved_files
            }

    def _save_answers_to_db(self, answers: List[Answer]) -> bool:
        """ä¿å­˜ç­”æ¡ˆæ•°æ®åˆ°æ•°æ®åº“"""
        try:
            saved_count = 0
            for answer in answers:
                # ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡
                content_hash = hashlib.md5(answer.content.encode('utf-8')).hexdigest()
                answer.content_hash = content_hash

                if self.db_manager.save_answer(answer):
                    saved_count += 1
                else:
                    logger.warning(f"ä¿å­˜ç­”æ¡ˆå¤±è´¥: {answer.answer_id}")

            logger.info(f"æˆåŠŸä¿å­˜ {saved_count}/{len(answers)} ä¸ªç­”æ¡ˆåˆ°æ•°æ®åº“")
            return saved_count == len(answers)

        except Exception as e:
            logger.error(f"ä¿å­˜ç­”æ¡ˆåˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return False

    def _mark_question_processed(self, question_id: str, task_id: str):
        """æ ‡è®°é—®é¢˜ä¸ºå·²å¤„ç†"""
        try:
            with self.db_manager.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE questions 
                    SET processed = TRUE, updated_at = CURRENT_TIMESTAMP
                    WHERE question_id = %s AND task_id = %s
                """, (question_id, task_id))
                conn.commit()
                logger.info(f"âœ… é—®é¢˜ {question_id} æ ‡è®°ä¸ºå·²å¤„ç†")
                
        except Exception as e:
            logger.error(f"æ ‡è®°é—®é¢˜å¤„ç†çŠ¶æ€å¤±è´¥: {e}")

    def crawl_all_unprocessed_questions(self, max_answers_per_question: int = None) -> Dict:
        """æ‰¹é‡çˆ¬å–æ‰€æœ‰æœªå¤„ç†çš„é—®é¢˜ - å¢å¼ºç‰ˆ"""
        logger.info("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆæ‰¹é‡çˆ¬å–æ‰€æœ‰æœªå¤„ç†çš„é—®é¢˜")
        
        # è·å–æœªå¤„ç†çš„é—®é¢˜
        questions = self.get_unprocessed_questions()
        if not questions:
            logger.info("æ²¡æœ‰æœªå¤„ç†çš„é—®é¢˜")
            return {'total_questions': 0, 'results': []}

        logger.info(f"æ‰¾åˆ° {len(questions)} ä¸ªæœªå¤„ç†çš„é—®é¢˜")
        
        # æ’é™¤å·²ç»å¤„ç†è¿‡çš„é—®é¢˜378706911
        questions = [q for q in questions if q['question_id'] != '378706911']
        logger.info(f"æ’é™¤å·²å¤„ç†é—®é¢˜åï¼Œå‰©ä½™ {len(questions)} ä¸ªé—®é¢˜")

        if not questions:
            logger.info("æ‰€æœ‰é—®é¢˜éƒ½å·²å¤„ç†å®Œæˆ")
            return {'total_questions': 0, 'results': []}

        start_time = time.time()
        results = []
        total_answers = 0
        total_files = 0

        for i, question_info in enumerate(questions, 1):
            logger.info(f"\n{'='*80}")
            logger.info(f"å¤„ç†ç¬¬ {i}/{len(questions)} ä¸ªé—®é¢˜")
            logger.info(f"é—®é¢˜ID: {question_info['question_id']}")
            logger.info(f"é¢„æœŸç­”æ¡ˆæ•°: {question_info['answer_count']}")
            logger.info(f"{'='*80}")

            # çˆ¬å–é—®é¢˜ç­”æ¡ˆ - ä½¿ç”¨å¢å¼ºç‰ˆæ–¹æ³•
            result = self.crawl_question_answers_with_retry(question_info, max_answers_per_question)
            results.append(result)

            # ç»Ÿè®¡
            if 'total_answers' in result:
                total_answers += result['total_answers']
            if 'saved_files_count' in result:
                total_files += result['saved_files_count']

            # é—®é¢˜é—´å»¶æ—¶
            if i < len(questions):
                wait_time = 60 if result.get('had_403_errors', False) else 30
                logger.info(f"â³ ç­‰å¾…{wait_time}ç§’åå¤„ç†ä¸‹ä¸€ä¸ªé—®é¢˜...")
                time.sleep(wait_time)

        end_time = time.time()
        total_duration = end_time - start_time

        summary = {
            'total_questions': len(questions),
            'total_answers_collected': total_answers,
            'total_files_saved': total_files,
            'total_duration_seconds': round(total_duration, 2),
            'average_time_per_question': round(total_duration / len(questions), 2) if questions else 0,
            'results': results,
            'crawl_time': datetime.now().isoformat()
        }

        # ä¿å­˜æ‰¹é‡çˆ¬å–æ‘˜è¦
        self._save_batch_summary(summary)

        logger.info(f"\nğŸ‰ å¢å¼ºç‰ˆæ‰¹é‡çˆ¬å–ä»»åŠ¡å®Œæˆ!")
        logger.info(f"ğŸ“Š å¤„ç†é—®é¢˜æ•°é‡: {len(questions)}")
        logger.info(f"ğŸ“Š æ€»å…±é‡‡é›†ç­”æ¡ˆ: {total_answers}")
        logger.info(f"ğŸ“Š ä¿å­˜æ–‡ä»¶æ•°é‡: {total_files}")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_duration:.2f} ç§’")

        return summary

    def _save_batch_summary(self, summary: Dict) -> str:
        """ä¿å­˜æ‰¹é‡çˆ¬å–æ‘˜è¦"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            summary_file = self.output_dir / f"enhanced_batch_summary_{timestamp}.json"

            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2, default=str)

            logger.info(f"âœ… å¢å¼ºç‰ˆæ‰¹é‡çˆ¬å–æ‘˜è¦å·²ä¿å­˜: {summary_file}")
            return str(summary_file)

        except Exception as e:
            logger.error(f"ä¿å­˜æ‰¹é‡çˆ¬å–æ‘˜è¦å¤±è´¥: {e}")
            return ""


def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆå§‹åŒ–å¢å¼ºç‰ˆæ‰¹é‡çˆ¬è™«
        crawler = EnhancedBatchQuestionCrawler()

        # å¼€å§‹æ‰¹é‡çˆ¬å–
        logger.info("å¼€å§‹æ‰§è¡Œå¢å¼ºç‰ˆæ‰¹é‡é—®é¢˜ç­”æ¡ˆé‡‡é›†ä»»åŠ¡...")
        summary = crawler.crawl_all_unprocessed_questions()

        # è¾“å‡ºæœ€ç»ˆç»“æœ
        print("\n" + "="*100)
        print("å¢å¼ºç‰ˆæ‰¹é‡é—®é¢˜ç­”æ¡ˆé‡‡é›†ä»»åŠ¡å®Œæˆï¼")
        print("="*100)
        print(f"å¤„ç†é—®é¢˜æ•°é‡: {summary['total_questions']}")
        print(f"æ€»å…±é‡‡é›†ç­”æ¡ˆ: {summary['total_answers_collected']}")
        print(f"ä¿å­˜æ–‡ä»¶æ•°é‡: {summary['total_files_saved']}")
        print(f"æ€»è€—æ—¶: {summary['total_duration_seconds']} ç§’")
        print(f"å¹³å‡æ¯ä¸ªé—®é¢˜è€—æ—¶: {summary['average_time_per_question']} ç§’")
        print(f"è¾“å‡ºç›®å½•: {crawler.output_dir}")
        print("="*100)

        # æ˜¾ç¤ºå„é—®é¢˜è¯¦ç»†ç»“æœ
        if summary['results']:
            print("\nå„é—®é¢˜å¤„ç†ç»“æœ:")
            for i, result in enumerate(summary['results'], 1):
                status = "æˆåŠŸ" if result.get('saved_to_db', False) else "å¤±è´¥"
                completion = result.get('completion_rate', 0)
                had_403 = "æœ‰" if result.get('had_403_errors', False) else "æ— "
                print(f"{i}. é—®é¢˜{result['question_id']}: {result.get('total_answers', 0)}ä¸ªç­”æ¡ˆ, "
                      f"å®Œæˆç‡{completion}%, çŠ¶æ€: {status}, 403é”™è¯¯: {had_403}")

        return summary

    except Exception as e:
        logger.error(f"æ‰§è¡Œå¢å¼ºç‰ˆæ‰¹é‡é‡‡é›†ä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None


if __name__ == "__main__":
    main()
