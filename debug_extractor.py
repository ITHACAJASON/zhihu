#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒè¯•å‚æ•°æå–å™¨
"""

import json
import time
from dynamic_params_extractor import DynamicParamsExtractor
from loguru import logger

def debug_extractor():
    """è°ƒè¯•å‚æ•°æå–å™¨"""
    logger.info("ğŸ” å¼€å§‹è°ƒè¯•å‚æ•°æå–å™¨")
    
    # åˆ›å»ºæå–å™¨å®ä¾‹ï¼ˆéæ— å¤´æ¨¡å¼ï¼Œä¾¿äºè§‚å¯Ÿï¼‰
    extractor = DynamicParamsExtractor(headless=False)
    
    try:
        # æµ‹è¯•æå–å‚æ•°
        question_id = "30215562"
        logger.info(f"ğŸ“‹ æµ‹è¯•é—®é¢˜ID: {question_id}")
        
        # æ‰‹åŠ¨è®¾ç½®é©±åŠ¨
        extractor.driver = extractor._setup_driver()
        logger.info("âœ… Chromeé©±åŠ¨åˆ›å»ºæˆåŠŸ")
        
        # åŠ è½½cookies
        logger.info("ğŸª åŠ è½½cookies")
        extractor.driver.get("https://www.zhihu.com")
        
        # è¯»å–å¹¶è®¾ç½®cookies
        import json
        try:
            with open('/Users/jasonlai/Documents/Code/Crawler/zhihu/cookies/zhihu_cookies.json', 'r') as f:
                cookies = json.load(f)
            
            for cookie in cookies:
                try:
                    extractor.driver.add_cookie(cookie)
                except Exception as e:
                    logger.debug(f"è®¾ç½®cookieå¤±è´¥: {cookie['name']} - {e}")
                    
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(cookies)} ä¸ªcookies")
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½cookieså¤±è´¥: {e}")
        
        # è®¿é—®é¡µé¢
        url = f"https://www.zhihu.com/question/{question_id}"
        logger.info(f"ğŸŒ è®¿é—®é¡µé¢: {url}")
        extractor.driver.get(url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        time.sleep(5)
        logger.info("â³ é¡µé¢åŠ è½½å®Œæˆ")
        
        # æ£€æŸ¥ç½‘ç»œæ—¥å¿—
        logs = extractor.driver.get_log('performance')
        logger.info(f"ğŸ“Š è·å–åˆ° {len(logs)} æ¡ç½‘ç»œæ—¥å¿—")
        
        # åˆ†ææ—¥å¿—
        feeds_requests = []
        api_requests = []
        for i, log in enumerate(logs):
            try:
                message = json.loads(log['message'])
                if message.get('message', {}).get('method') == 'Network.requestWillBeSent':
                    request = message['message']['params']['request']
                    url_log = request.get('url', '')
                    
                    # è®°å½•æ‰€æœ‰APIè¯·æ±‚
                    if '/api/' in url_log:
                        api_requests.append(url_log)
                        logger.debug(f"ğŸ” APIè¯·æ±‚: {url_log}")
                    
                    if '/api/v4/questions/' in url_log and '/feeds' in url_log:
                        feeds_requests.append({
                            'url': url_log,
                            'headers': request.get('headers', {})
                        })
                        logger.info(f"ğŸ¯ å‘ç°feedsè¯·æ±‚: {url_log}")
            except Exception as e:
                continue
                
        logger.info(f"ğŸ” æ€»å…±å‘ç° {len(api_requests)} ä¸ªAPIè¯·æ±‚")
        for api_url in api_requests[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            logger.info(f"   - {api_url}")
                
        logger.info(f"ğŸ“ˆ æ‰¾åˆ° {len(feeds_requests)} ä¸ªfeedsè¯·æ±‚")
        
        if not feeds_requests:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°feedsè¯·æ±‚ï¼Œå°è¯•æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸º")
            
            # æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸º - å¤šæ¬¡ä¸‹æ»‘ä»¥è§¦å‘feedsè¯·æ±‚
            logger.info("ğŸ¯ å¼€å§‹æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸º...")
            time.sleep(2)
            
            # å¤šæ¬¡æ»šåŠ¨é¡µé¢ï¼Œæ¯æ¬¡æ»šåŠ¨åç­‰å¾…
            logger.info("ğŸ“œ å¼€å§‹æ»šåŠ¨é¡µé¢ä»¥è§¦å‘feedsè¯·æ±‚...")
            for i in range(8):  # å¢åŠ æ»šåŠ¨æ¬¡æ•°
                logger.info(f"ğŸ“œ ç¬¬ {i+1} æ¬¡æ»šåŠ¨")
                extractor.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)  # æ¯æ¬¡æ»šåŠ¨åç­‰å¾…æ›´é•¿æ—¶é—´
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„feedsè¯·æ±‚
                current_logs = extractor.driver.get_log('performance')
                for log in current_logs:
                    try:
                        message = json.loads(log['message'])
                        if message.get('message', {}).get('method') == 'Network.requestWillBeSent':
                            request = message['message']['params']['request']
                            url_log = request.get('url', '')
                            if '/feeds' in url_log:
                                logger.info(f"ğŸ¯ å‘ç°feedsè¯·æ±‚: {url_log}")
                    except Exception as e:
                        continue
            
            logger.info("ğŸ“œ æ»šåŠ¨å®Œæˆï¼Œç­‰å¾…æœ€ç»ˆåŠ è½½...")
            time.sleep(3)
            
            # è·å–æ‰€æœ‰ç½‘ç»œæ—¥å¿—
            all_logs = extractor.driver.get_log('performance')
            logger.info(f"ğŸ“Š æ€»å…±è·å–åˆ° {len(all_logs)} æ¡ç½‘ç»œæ—¥å¿—")
            
            # åˆ†æç½‘ç»œæ—¥å¿—
            feeds_requests_new = []
            for log in all_logs:
                try:
                    message = json.loads(log['message'])
                    if message.get('message', {}).get('method') == 'Network.requestWillBeSent':
                        request = message['message']['params']['request']
                        url_log = request.get('url', '')
                        
                        # è®°å½•æ‰€æœ‰APIè¯·æ±‚
                        if '/api/' in url_log:
                            api_requests.append(url_log)
                            logger.debug(f"ğŸ” æ–°APIè¯·æ±‚: {url_log}")
                        
                        if '/api/v4/questions/' in url_log and '/feeds' in url_log:
                            feeds_requests_new.append({
                                'url': url_log,
                                'headers': request.get('headers', {})
                            })
                            logger.info(f"ğŸ¯ å‘ç°feedsè¯·æ±‚: {url_log}")
                except Exception as e:
                    continue
            
            feeds_requests.extend(feeds_requests_new)
            logger.info(f"ğŸ” æ¨¡æ‹Ÿè¡Œä¸ºåæ€»å…±å‘ç° {len(feeds_requests_new)} ä¸ªæ–°feedsè¯·æ±‚")
            
            # æ‰‹åŠ¨æ§åˆ¶æƒäº¤è¿˜
            logger.info("\n=== ğŸ® æ‰‹åŠ¨æ§åˆ¶æ¨¡å¼ ===")
            logger.info("æµè§ˆå™¨å·²æ‰“å¼€ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆä»¥ä¸‹æ“ä½œï¼š")
            logger.info("1. å¦‚æœéœ€è¦ç™»å½•ï¼Œè¯·å®Œæˆç™»å½•")
            logger.info("2. ç»§ç»­æ»šåŠ¨é¡µé¢ç›´åˆ°çœ‹åˆ°feedsè¯·æ±‚")
            logger.info("3. å®Œæˆååœ¨æ­¤æ§åˆ¶å°è¾“å…¥ 'done' å¹¶æŒ‰å›è½¦ç»§ç»­")
            
            while True:
                user_input = input("è¯·è¾“å…¥ 'done' ç»§ç»­: ").strip().lower()
                if user_input == 'done':
                    break
                else:
                    logger.info("è¯·è¾“å…¥ 'done' ç»§ç»­...")
            
            # ç”¨æˆ·æ“ä½œå®Œæˆåï¼Œé‡æ–°è·å–ç½‘ç»œæ—¥å¿—
            logger.info("ğŸ”„ ç”¨æˆ·æ“ä½œå®Œæˆï¼Œé‡æ–°åˆ†æç½‘ç»œæ—¥å¿—...")
            final_logs = extractor.driver.get_log('performance')
            logger.info(f"ğŸ“Š æœ€ç»ˆè·å–åˆ° {len(final_logs)} æ¡ç½‘ç»œæ—¥å¿—")
            
            # é‡æ–°åˆ†ææ‰€æœ‰feedsè¯·æ±‚
            feeds_requests = []  # é‡ç½®feedsè¯·æ±‚åˆ—è¡¨
            api_requests = []
            
            for log in final_logs:
                try:
                    message = json.loads(log['message'])
                    if message.get('message', {}).get('method') == 'Network.requestWillBeSent':
                        request = message['message']['params']['request']
                        url_log = request.get('url', '')
                        
                        if '/api/' in url_log:
                            api_requests.append(url_log)
                        
                        if '/api/v4/questions/' in url_log and '/feeds' in url_log:
                            feeds_requests.append({
                                'url': url_log,
                                'headers': request.get('headers', {})
                            })
                            logger.info(f"ğŸ¯ å‘ç°feedsè¯·æ±‚: {url_log}")
                except Exception as e:
                    continue
            
            logger.info(f"ğŸ” æœ€ç»ˆå‘ç° {len(api_requests)} ä¸ªAPIè¯·æ±‚")
            logger.info(f"ğŸ¯ æœ€ç»ˆå‘ç° {len(feeds_requests)} ä¸ªfeedsè¯·æ±‚")
                     
         # åˆ†ææ‰¾åˆ°çš„è¯·æ±‚
        for i, req in enumerate(feeds_requests):
            logger.info(f"ğŸ“‹ è¯·æ±‚ {i+1}:")
            logger.info(f"   URL: {req['url']}")
            headers = req['headers']
            logger.info(f"   x-zse-96: {headers.get('x-zse-96', 'N/A')}")
            logger.info(f"   x-zst-81: {headers.get('x-zst-81', 'N/A')}")
            logger.info(f"   cookie: {headers.get('cookie', 'N/A')[:100]}...")
            
        # å°è¯•æå–å‚æ•°
        params = extractor._extract_params_from_logs()
        if params:
            logger.info(f"âœ… æˆåŠŸæå–å‚æ•°: {params}")
        else:
            logger.warning("âŒ å‚æ•°æå–å¤±è´¥")
            
    except Exception as e:
        logger.error(f"âŒ è°ƒè¯•è¿‡ç¨‹å‡ºé”™: {e}")
        
    finally:
        if extractor.driver:
            # ä¿æŒæµè§ˆå™¨æ‰“å¼€ä¸€æ®µæ—¶é—´ï¼Œä¾¿äºè§‚å¯Ÿ
            logger.info("ğŸ” æµè§ˆå™¨å°†ä¿æŒæ‰“å¼€30ç§’ï¼Œä¾¿äºè§‚å¯Ÿ...")
            time.sleep(30)
            extractor.close()
            
if __name__ == "__main__":
    debug_extractor()