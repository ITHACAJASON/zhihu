#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥ä¹çˆ¬è™«ä»»åŠ¡æ¢å¤è„šæœ¬
ç”¨äºæ¢å¤ä¸­æ–­çš„çˆ¬è™«ä»»åŠ¡

ä½¿ç”¨æ–¹æ³•:
1. æ¢å¤æ‰€æœ‰æœªå®Œæˆä»»åŠ¡: python3 resume_tasks.py --all
2. æ¢å¤ç‰¹å®šå…³é”®è¯ä»»åŠ¡: python3 resume_tasks.py --keyword "åšå£«å›å›½"
3. æŸ¥çœ‹æœªå®Œæˆä»»åŠ¡åˆ—è¡¨: python3 resume_tasks.py --list
4. æ¢å¤ç‰¹å®šä»»åŠ¡ID: python3 resume_tasks.py --task-id "task_id_here"
"""

import argparse
import sys
import time
from datetime import datetime
from postgres_crawler import PostgresZhihuCrawler

def print_banner():
    """æ‰“å°è„šæœ¬æ¨ªå¹…"""
    print("="*60)
    print("           çŸ¥ä¹çˆ¬è™«ä»»åŠ¡æ¢å¤è„šæœ¬")
    print("="*60)
    print(f"æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

def check_system_status(crawler):
    """æ£€æŸ¥ç³»ç»ŸçŠ¶æ€"""
    print("ğŸ” æ£€æŸ¥ç³»ç»ŸçŠ¶æ€...")
    
    # æ£€æŸ¥æ•°æ®åº“è¿æ¥
    try:
        with crawler.db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            cursor.fetchone()
        print("âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return False
    
    # æ£€æŸ¥ç™»å½•çŠ¶æ€
    try:
        if not crawler.check_login_status():
            print("âš ï¸  ç™»å½•çŠ¶æ€æ— æ•ˆï¼Œå°è¯•åŠ è½½cookies...")
            crawler.load_cookies()
            if crawler.check_login_status():
                print("âœ… ç™»å½•çŠ¶æ€å·²æ¢å¤")
            else:
                print("âŒ ç™»å½•çŠ¶æ€æ— æ•ˆï¼Œè¯·æ‰‹åŠ¨ç™»å½•çŸ¥ä¹")
                return False
        else:
            print("âœ… ç™»å½•çŠ¶æ€æœ‰æ•ˆ")
    except Exception as e:
        print(f"âš ï¸  æ— æ³•æ£€æŸ¥ç™»å½•çŠ¶æ€: {e}")
    
    print()
    return True

def list_unfinished_tasks(crawler):
    """åˆ—å‡ºæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡ï¼ˆä½¿ç”¨æ–°çš„ä¸¤é˜¶æ®µçŠ¶æ€åˆ¤æ–­ï¼‰"""
    print("ğŸ“‹ æŸ¥è¯¢ä¸­æ–­ä»»åŠ¡...")
    
    try:
        # ä½¿ç”¨æ–°çš„ä¸­æ–­ä»»åŠ¡æŸ¥è¯¢æ–¹æ³•
        interrupted_tasks = crawler.db.get_interrupted_tasks()
        
        if not interrupted_tasks:
            print("âœ… æ²¡æœ‰ä¸­æ–­çš„ä»»åŠ¡")
            return []
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(interrupted_tasks)} ä¸ªä¸­æ–­ä»»åŠ¡:")
        print("-" * 100)
        print(f"{'åºå·':<4} {'ä»»åŠ¡ID':<20} {'å…³é”®è¯':<15} {'æœç´¢é˜¶æ®µ':<10} {'é—®ç­”é˜¶æ®µ':<10} {'åˆ›å»ºæ—¶é—´':<20}")
        print("-" * 100)
        
        for i, task in enumerate(interrupted_tasks, 1):
            search_status = task.search_stage_status
            qa_status = task.qa_stage_status
            print(f"{i:<4} {task.task_id[:20]:<20} {task.keywords:<15} {search_status:<10} {qa_status:<10} {str(task.created_at)[:16]:<20}")
        
        print("-" * 100)
        print()
        return interrupted_tasks
        
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢ä»»åŠ¡å¤±è´¥: {e}")
        return []

def resume_task_by_keyword(crawler, keyword, start_date=None, end_date=None):
    """æ ¹æ®å…³é”®è¯æ¢å¤ä»»åŠ¡ï¼ˆä½¿ç”¨æ–°çš„ä¸¤é˜¶æ®µæ¢å¤ç­–ç•¥ï¼‰"""
    print(f"ğŸ” æŸ¥æ‰¾å…³é”®è¯ '{keyword}' çš„ä¸­æ–­ä»»åŠ¡...")
    
    try:
        tasks = crawler.db.get_tasks_by_keyword(keyword)
        
        if not tasks:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°å…³é”®è¯ '{keyword}' çš„ä»»åŠ¡ï¼Œåˆ›å»ºæ–°ä»»åŠ¡")
            # è®¾ç½®é»˜è®¤æ—¥æœŸèŒƒå›´
            if not start_date:
                start_date = "2024-01-01"
            if not end_date:
                end_date = "2024-12-31"
            
            result = crawler.crawl_by_keyword(
                keyword=keyword,
                start_date=start_date,
                end_date=end_date,
                process_immediately=True
            )
            print(f"âœ… æ–°ä»»åŠ¡ '{keyword}' åˆ›å»ºå®Œæˆ: {result}")
            return True
        
        # æ‰¾åˆ°ä¸­æ–­çš„ä»»åŠ¡
        interrupted_tasks = []
        for task in tasks:
            if not (task.search_stage_status == 'completed' and task.qa_stage_status == 'completed'):
                interrupted_tasks.append(task)
        
        if not interrupted_tasks:
            print(f"âœ… å…³é”®è¯ '{keyword}' çš„æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆ")
            return True
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(interrupted_tasks)} ä¸ªä¸­æ–­ä»»åŠ¡")
        
        # æ¢å¤æ¯ä¸ªä¸­æ–­çš„ä»»åŠ¡
        success_count = 0
        for task in interrupted_tasks:
            try:
                if resume_single_task(crawler, task.task_id):
                    success_count += 1
            except Exception as e:
                print(f"âŒ ä»»åŠ¡ {task.task_id} æ¢å¤å¤±è´¥: {e}")
                # å°è¯•é‡ç½®é©±åŠ¨åé‡è¯•
                print("ğŸ”„ é‡ç½®æµè§ˆå™¨é©±åŠ¨åé‡è¯•...")
                try:
                    crawler.reset_driver()
                    time.sleep(10)
                    if resume_single_task(crawler, task.task_id):
                        success_count += 1
                except Exception as retry_error:
                    print(f"âŒ é‡è¯•ä¹Ÿå¤±è´¥: {retry_error}")
        
        print(f"\nğŸ“ˆ æ¢å¤ç»“æœ: {success_count}/{len(interrupted_tasks)} ä¸ªä»»åŠ¡æˆåŠŸæ¢å¤")
        return success_count == len(interrupted_tasks)
        
    except Exception as e:
        print(f"âŒ æ¢å¤ä»»åŠ¡å¤±è´¥: {e}")
        return False

def resume_single_task(crawler, task_id):
    """æ¢å¤å•ä¸ªä»»åŠ¡ï¼ˆä½¿ç”¨æ–°çš„ä¸¤é˜¶æ®µæ¢å¤ç­–ç•¥ï¼‰"""
    print(f"ğŸ”„ æ¢å¤ä»»åŠ¡: {task_id}")
    
    try:
        # è·å–ä»»åŠ¡ä¿¡æ¯
        task_info = crawler.db.get_task_info(task_id)
        if not task_info:
            print(f"âŒ ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
            return False
        
        print(f"ğŸ“‹ ä»»åŠ¡ä¿¡æ¯: {task_info.keywords} (æœç´¢é˜¶æ®µ: {task_info.search_stage_status}, é—®ç­”é˜¶æ®µ: {task_info.qa_stage_status})")
        
        # å¦‚æœä»»åŠ¡å·²å®Œæˆï¼Œè·³è¿‡
        if task_info.search_stage_status == 'completed' and task_info.qa_stage_status == 'completed':
            print(f"âœ… ä»»åŠ¡ {task_id} å·²å®Œæˆï¼Œè·³è¿‡")
            return True
        
        # ç¡®å®šæ¢å¤ç­–ç•¥
        strategy_info = crawler.db.determine_task_resume_strategy(task_id)
        strategy = strategy_info.get('strategy', 'unknown')
        print(f"ğŸ“‹ æ¢å¤ç­–ç•¥: {strategy_info.get('message', strategy)}")
        
        # æ ¹æ®ç­–ç•¥æ¢å¤ä»»åŠ¡
        if strategy == 'resume_search':
            print("ğŸ” ç»§ç»­æœç´¢é˜¶æ®µ...")
            # æ›´æ–°æœç´¢é˜¶æ®µçŠ¶æ€ä¸ºè¿›è¡Œä¸­
            crawler.db.update_stage_status(task_id, 'search_stage_status', 'in_progress')
            result = crawler.resume_search_stage(task_id)
        elif strategy == 'resume_qa':
            print("ğŸ’¬ ç»§ç»­é—®ç­”é‡‡é›†é˜¶æ®µ...")
            # æ›´æ–°é—®ç­”é˜¶æ®µçŠ¶æ€ä¸ºè¿›è¡Œä¸­
            crawler.db.update_stage_status(task_id, 'qa_stage_status', 'in_progress')
            result = crawler.resume_qa_stage(task_id)
        elif strategy == 'completed':
            print("âœ… ä»»åŠ¡å·²å®Œæˆï¼Œæ— éœ€æ¢å¤")
            return True
        else:
            print(f"ğŸ”„ ä»å¤´å¼€å§‹ä»»åŠ¡...")
            # é‡ç½®ä¸¤ä¸ªé˜¶æ®µçŠ¶æ€
            crawler.db.update_stage_status(task_id, 'search_stage_status', 'in_progress')
            crawler.db.update_stage_status(task_id, 'qa_stage_status', 'not_started')
            result = crawler.resume_task(task_id)
        
        print(f"âœ… ä»»åŠ¡ {task_id} æ¢å¤å®Œæˆ: {result}")
        return True
        
    except Exception as e:
        print(f"âŒ ä»»åŠ¡ {task_id} æ¢å¤å¤±è´¥: {e}")
        
        # å°è¯•é‡ç½®é©±åŠ¨åé‡è¯•
        print("ğŸ”„ é‡ç½®æµè§ˆå™¨é©±åŠ¨åé‡è¯•...")
        try:
            crawler.reset_driver()
            time.sleep(10)
            result = crawler.resume_task(task_id)
            print(f"âœ… é‡è¯•æˆåŠŸ: {result}")
            return True
            
        except Exception as retry_error:
            print(f"âŒ é‡è¯•ä¹Ÿå¤±è´¥: {retry_error}")
            return False

def resume_task_by_id(crawler, task_id):
    """æ ¹æ®ä»»åŠ¡IDæ¢å¤ä»»åŠ¡"""
    print(f"ğŸ”„ æ¢å¤ä»»åŠ¡ID: {task_id}")
    
    try:
        # è·å–ä»»åŠ¡ä¿¡æ¯
        task_info = crawler.db.get_task_by_id(task_id)
        if not task_info:
            print(f"âŒ æœªæ‰¾åˆ°ä»»åŠ¡ID: {task_id}")
            return False
        
        print(f"ğŸ“‹ ä»»åŠ¡ä¿¡æ¯: å…³é”®è¯='{task_info.keywords}', æœç´¢é˜¶æ®µ='{task_info.search_stage_status}', é—®ç­”é˜¶æ®µ='{task_info.qa_stage_status}'")
        
        # ä½¿ç”¨æ–°çš„å•ä»»åŠ¡æ¢å¤å‡½æ•°
        return resume_single_task(crawler, task_id)
        
    except Exception as e:
        print(f"âŒ æ¢å¤ä»»åŠ¡å¤±è´¥: {e}")
        return False

def resume_all_tasks(crawler):
    """æ¢å¤æ‰€æœ‰ä¸­æ–­çš„ä»»åŠ¡ï¼ˆä½¿ç”¨æ–°çš„ä¸¤é˜¶æ®µæ¢å¤ç­–ç•¥ï¼‰"""
    print("ğŸ”„ å¼€å§‹æ¢å¤æ‰€æœ‰ä¸­æ–­ä»»åŠ¡...")
    
    # å…ˆåˆ—å‡ºæ‰€æœ‰ä¸­æ–­ä»»åŠ¡
    interrupted_tasks = list_unfinished_tasks(crawler)
    
    if not interrupted_tasks:
        return True
    
    print(f"\nğŸš€ å¼€å§‹æ¢å¤ {len(interrupted_tasks)} ä¸ªä»»åŠ¡...")
    
    success_count = 0
    for i, task in enumerate(interrupted_tasks, 1):
        print(f"\n[{i}/{len(interrupted_tasks)}] æ¢å¤ä»»åŠ¡: {task.task_id}")
        
        if resume_single_task(crawler, task.task_id):
            success_count += 1
            print(f"âœ… ä»»åŠ¡ {task.task_id} æ¢å¤æˆåŠŸ")
        else:
            print(f"âŒ ä»»åŠ¡ {task.task_id} æ¢å¤å¤±è´¥")
    
    print(f"\nğŸ“ˆ æ¢å¤å®Œæˆ: {success_count}/{len(interrupted_tasks)} ä¸ªä»»åŠ¡æˆåŠŸ")
    return success_count == len(interrupted_tasks)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="çŸ¥ä¹çˆ¬è™«ä»»åŠ¡æ¢å¤è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python3 resume_tasks.py --list                    # æŸ¥çœ‹æœªå®Œæˆä»»åŠ¡
  python3 resume_tasks.py --all                     # æ¢å¤æ‰€æœ‰ä»»åŠ¡
  python3 resume_tasks.py --keyword "åšå£«å›å›½"        # æ¢å¤ç‰¹å®šå…³é”®è¯
  python3 resume_tasks.py --task-id "task_123"      # æ¢å¤ç‰¹å®šä»»åŠ¡ID
        """
    )
    
    parser.add_argument('--list', action='store_true', help='åˆ—å‡ºæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡')
    parser.add_argument('--all', action='store_true', help='æ¢å¤æ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡')
    parser.add_argument('--keyword', type=str, help='æ¢å¤æŒ‡å®šå…³é”®è¯çš„ä»»åŠ¡')
    parser.add_argument('--task-id', type=str, help='æ¢å¤æŒ‡å®šIDçš„ä»»åŠ¡')
    parser.add_argument('--start-date', type=str, default='2024-01-01', help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, default='2024-12-31', help='ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--headless', action='store_true', default=True, help='æ— å¤´æ¨¡å¼è¿è¡Œ (é»˜è®¤å¼€å¯)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥å‚æ•°
    if not any([args.list, args.all, args.keyword, args.task_id]):
        parser.print_help()
        return
    
    print_banner()
    
    # åˆå§‹åŒ–çˆ¬è™«
    print("ğŸš€ åˆå§‹åŒ–çˆ¬è™«...")
    crawler = None
    
    try:
        crawler = PostgresZhihuCrawler(headless=args.headless)
        print("âœ… çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        
        # æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
        if not check_system_status(crawler):
            print("âŒ ç³»ç»ŸçŠ¶æ€æ£€æŸ¥å¤±è´¥ï¼Œè¯·è§£å†³é—®é¢˜åé‡è¯•")
            return
        
        # æ‰§è¡Œç›¸åº”æ“ä½œ
        if args.list:
            list_unfinished_tasks(crawler)
            
        elif args.all:
            resume_all_tasks(crawler)
            
        elif args.keyword:
            resume_task_by_keyword(crawler, args.keyword, args.start_date, args.end_date)
            
        elif args.task_id:
            resume_task_by_id(crawler, args.task_id)
        
        print("\nğŸ‰ è„šæœ¬æ‰§è¡Œå®Œæˆ")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        
    except Exception as e:
        print(f"\nâŒ è„šæœ¬æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if crawler:
            try:
                crawler.close()
                print("âœ… çˆ¬è™«å·²å…³é—­")
            except:
                pass

if __name__ == "__main__":
    main()