# crawl_specific_question.py æ‰¹é‡ä½¿ç”¨æŒ‡å—

## ğŸ“‹ è„šæœ¬åŠŸèƒ½

`crawl_specific_question.py` æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºé‡‡é›†æŒ‡å®šçŸ¥ä¹é—®é¢˜çš„å®Œæ•´ç­”æ¡ˆæ•°æ®çš„è„šæœ¬ï¼Œæ”¯æŒï¼š

- å•ä¸ªé—®é¢˜å®Œæ•´ç­”æ¡ˆé‡‡é›†
- APIå“åº”è‡ªåŠ¨ä¿å­˜åˆ°æ–‡ä»¶
- æ•°æ®ä¿å­˜åˆ°PostgreSQLæ•°æ®åº“
- ä»»åŠ¡è¿›åº¦ç®¡ç†å’Œæ–­ç‚¹ç»­ä¼ 
- åçˆ¬ç­–ç•¥å’Œé”™è¯¯å¤„ç†

## ğŸ”§ å‡†å¤‡å‚æ•°

### å¿…å¤‡å‚æ•°

1. **question_url** (å¿…éœ€)
   - çŸ¥ä¹é—®é¢˜é¡µé¢çš„URL
   - æ”¯æŒå¤šç§URLæ ¼å¼ï¼š
     - `https://www.zhihu.com/question/378706911/answer/1080446596`
     - `https://www.zhihu.com/question/378706911`
     - `https://www.zhihu.com/question/378706911?sort=created`

2. **task_name** (å¯é€‰)
   - ä»»åŠ¡åç§°ï¼Œç”¨äºæ ‡è¯†å’Œåˆ†ç»„
   - é»˜è®¤å€¼: `"specific_question_crawl"`
   - å»ºè®®æ ¼å¼: `"question_{question_id}_crawl"`

### å¯é€‰å‚æ•°

3. **max_answers** (å¯é€‰)
   - æœ€å¤§é‡‡é›†ç­”æ¡ˆæ•°é‡é™åˆ¶
   - ç±»å‹: `int`
   - é»˜è®¤å€¼: `None` (é‡‡é›†å…¨éƒ¨ç­”æ¡ˆ)
   - ç¤ºä¾‹: `max_answers=1000`

## ğŸ“ ä½¿ç”¨æ–¹æ³•

### æ–¹å¼1: ç›´æ¥åœ¨è„šæœ¬ä¸­ä¿®æ”¹å‚æ•°

```python
# åœ¨mainå‡½æ•°ä¸­ä¿®æ”¹å‚æ•°
def main():
    """ä¸»å‡½æ•°"""
    # ä¿®æ”¹è¿™é‡Œçš„å‚æ•°
    question_url = "https://www.zhihu.com/question/YOUR_QUESTION_ID/answer/ANY_ANSWER_ID"
    task_name = "your_custom_task_name"

    try:
        # åˆå§‹åŒ–çˆ¬è™«
        crawler = SpecificQuestionCrawler(question_url, task_name)

        # å¼€å§‹çˆ¬å– (å¯è®¾ç½®æœ€å¤§ç­”æ¡ˆæ•°)
        result = crawler.crawl_all_answers(max_answers=1000)  # æˆ– None é‡‡é›†å…¨éƒ¨

        # ... å…¶ä»–ä»£ç ä¿æŒä¸å˜
```

### æ–¹å¼2: åˆ›å»ºæ‰¹é‡å¤„ç†è„šæœ¬

```python
#!/usr/bin/env python3
"""
æ‰¹é‡é‡‡é›†å¤šä¸ªé—®é¢˜çš„ç¤ºä¾‹è„šæœ¬
"""

from crawl_specific_question import SpecificQuestionCrawler
import logging
logging.basicConfig(level=logging.INFO)

def batch_crawl_questions():
    """æ‰¹é‡é‡‡é›†å¤šä¸ªé—®é¢˜"""

    # å®šä¹‰è¦é‡‡é›†çš„é—®é¢˜åˆ—è¡¨
    questions = [
        {
            "url": "https://www.zhihu.com/question/378706911/answer/1080446596",
            "task_name": "question_378706911_full",
            "max_answers": None  # é‡‡é›†å…¨éƒ¨ç­”æ¡ˆ
        },
        {
            "url": "https://www.zhihu.com/question/457478394/answer/1910416671937659055",
            "task_name": "question_457478394_sample",
            "max_answers": 100  # é™åˆ¶é‡‡é›†100ä¸ªç­”æ¡ˆ
        },
        {
            "url": "https://www.zhihu.com/question/37197524",
            "task_name": "question_37197524_test",
            "max_answers": 50   # æµ‹è¯•ç”¨ï¼Œé‡‡é›†50ä¸ªç­”æ¡ˆ
        }
    ]

    results = []

    for i, question in enumerate(questions, 1):
        print(f"\n{'='*60}")
        print(f"å¼€å§‹å¤„ç†ç¬¬ {i}/{len(questions)} ä¸ªé—®é¢˜")
        print(f"é—®é¢˜URL: {question['url']}")
        print(f"ä»»åŠ¡åç§°: {question['task_name']}")
        print(f"æœ€å¤§ç­”æ¡ˆæ•°: {question['max_answers']}")
        print(f"{'='*60}")

        try:
            # åˆå§‹åŒ–çˆ¬è™«
            crawler = SpecificQuestionCrawler(
                question_url=question["url"],
                task_name=question["task_name"]
            )

            # å¼€å§‹çˆ¬å–
            result = crawler.crawl_all_answers(max_answers=question["max_answers"])

            # ä¿å­˜æ‘˜è¦
            summary_file = crawler.save_crawl_summary(result)

            # è®°å½•ç»“æœ
            results.append({
                "question": question,
                "result": result,
                "summary_file": summary_file
            })

            print(f"âœ… é—®é¢˜ {question['url']} å¤„ç†å®Œæˆ")
            print(f"ğŸ“Š é‡‡é›†ç­”æ¡ˆ: {result.get('total_answers', 0)}")
            print(f"â±ï¸ è€—æ—¶: {result.get('duration_seconds', 0)} ç§’")

        except Exception as e:
            print(f"âŒ å¤„ç†é—®é¢˜ {question['url']} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            results.append({
                "question": question,
                "error": str(e)
            })

        # é—®é¢˜é—´å»¶æ—¶ï¼Œé¿å…è¿‡äºé¢‘ç¹
        if i < len(questions):
            print("â³ ç­‰å¾…30ç§’åå¤„ç†ä¸‹ä¸€ä¸ªé—®é¢˜...")
            import time
            time.sleep(30)

    return results

if __name__ == "__main__":
    results = batch_crawl_questions()

    # è¾“å‡ºæ±‡æ€»ç»“æœ
    print(f"\n{'='*80}")
    print("æ‰¹é‡å¤„ç†å®Œæˆï¼")
    print(f"{'='*80}")

    success_count = 0
    total_answers = 0

    for result in results:
        if "result" in result:
            success_count += 1
            total_answers += result["result"].get("total_answers", 0)

    print(f"æˆåŠŸå¤„ç†é—®é¢˜: {success_count}/{len(results)}")
    print(f"æ€»å…±é‡‡é›†ç­”æ¡ˆ: {total_answers}")
    print(f"{'='*80}")
```

### æ–¹å¼3: ä»æ•°æ®åº“è¯»å–URLæ‰¹é‡å¤„ç†

```python
#!/usr/bin/env python3
"""
ä»æ•°æ®åº“è¯»å–URLè¿›è¡Œæ‰¹é‡å¤„ç†çš„ç¤ºä¾‹è„šæœ¬
"""

from crawl_specific_question import SpecificQuestionCrawler
from postgres_models import PostgreSQLManager
import logging
logging.basicConfig(level=logging.INFO)

def crawl_from_database():
    """ä»æ•°æ®åº“è¯»å–é—®é¢˜URLè¿›è¡Œæ‰¹é‡é‡‡é›†"""

    # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
    db = PostgreSQLManager()

    try:
        with db.get_connection() as conn:
            cursor = conn.cursor()

            # æŸ¥è¯¢éœ€è¦é‡‡é›†çš„é—®é¢˜ (ä¾‹å¦‚æœªå¤„ç†æˆ–éœ€è¦æ›´æ–°çš„é—®é¢˜)
            cursor.execute("""
                SELECT question_id, title, url, answer_count
                FROM questions
                WHERE processed = FALSE
                AND answer_count > 100  -- åªå¤„ç†ç­”æ¡ˆæ•°é‡è¾ƒå¤šçš„
                ORDER BY answer_count DESC
                LIMIT 5  -- é™åˆ¶å¤„ç†æ•°é‡
            """)

            questions = cursor.fetchall()

            print(f"ä»æ•°æ®åº“æ‰¾åˆ° {len(questions)} ä¸ªé—®é¢˜éœ€è¦å¤„ç†")

            for i, (question_id, title, url, expected_answers) in enumerate(questions, 1):
                print(f"\n{'='*60}")
                print(f"å¤„ç†ç¬¬ {i}/{len(questions)} ä¸ªé—®é¢˜")
                print(f"é—®é¢˜ID: {question_id}")
                print(f"æ ‡é¢˜: {title[:50]}...")
                print(f"é¢„æœŸç­”æ¡ˆæ•°: {expected_answers}")
                print(f"{'='*60}")

                try:
                    # åˆ›å»ºä»»åŠ¡åç§°
                    task_name = f"db_question_{question_id}"

                    # åˆå§‹åŒ–çˆ¬è™«
                    crawler = SpecificQuestionCrawler(url, task_name)

                    # å¼€å§‹é‡‡é›† (å¯æ ¹æ®é¢„æœŸç­”æ¡ˆæ•°è®¾ç½®é™åˆ¶)
                    max_answers = min(expected_answers, 1000) if expected_answers > 1000 else None
                    result = crawler.crawl_all_answers(max_answers=max_answers)

                    # ä¿å­˜æ‘˜è¦
                    summary_file = crawler.save_crawl_summary(result)

                    print(f"âœ… é—®é¢˜ {question_id} å¤„ç†å®Œæˆ")
                    print(f"ğŸ“Š é‡‡é›†ç­”æ¡ˆ: {result.get('total_answers', 0)}")
                    print(f"ğŸ“ˆ å®Œæˆç‡: {result.get('completion_rate', 0):.2f}%")

                except Exception as e:
                    print(f"âŒ å¤„ç†é—®é¢˜ {question_id} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

                # å»¶æ—¶
                if i < len(questions):
                    import time
                    time.sleep(30)

    except Exception as e:
        print(f"æ•°æ®åº“æ“ä½œé”™è¯¯: {e}")

if __name__ == "__main__":
    crawl_from_database()
```

## ğŸš€ è¿è¡Œè„šæœ¬

### åŸºæœ¬è¿è¡Œ

```bash
# ç›´æ¥è¿è¡Œè„šæœ¬ (ä½¿ç”¨è„šæœ¬ä¸­ç¡¬ç¼–ç çš„å‚æ•°)
python3 crawl_specific_question.py
```

### å¸¦å‚æ•°è¿è¡Œ

```bash
# å¦‚æœè„šæœ¬æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼Œå¯ä»¥è¿™æ ·è¿è¡Œ
python3 crawl_specific_question.py --url "https://www.zhihu.com/question/378706911" --task "my_task"
```

### æ‰¹é‡å¤„ç†è¿è¡Œ

```bash
# è¿è¡Œæ‰¹é‡å¤„ç†è„šæœ¬
python3 batch_crawl_example.py

# è¿è¡Œæ•°æ®åº“æ‰¹é‡å¤„ç†è„šæœ¬
python3 db_batch_crawl_example.py
```

## ğŸ“Š å‚æ•°é…ç½®å»ºè®®

### 1. æ ¹æ®é—®é¢˜è§„æ¨¡é€‰æ‹©å‚æ•°

```python
# å°é—®é¢˜ (< 100ä¸ªç­”æ¡ˆ)
crawler = SpecificQuestionCrawler(url, task_name)
result = crawler.crawl_all_answers(max_answers=None)

# ä¸­ç­‰è§„æ¨¡é—®é¢˜ (100-1000ä¸ªç­”æ¡ˆ)
result = crawler.crawl_all_answers(max_answers=1000)

# å¤§é—®é¢˜ (> 1000ä¸ªç­”æ¡ˆ)
result = crawler.crawl_all_answers(max_answers=5000)
```

### 2. ä»»åŠ¡å‘½åå»ºè®®

```python
# æŒ‰é—®é¢˜IDå‘½å
task_name = f"question_{question_id}_full_crawl"

# æŒ‰ä¸»é¢˜å‘½å
task_name = "ç•™å­¦ç”Ÿå›å›½é—®é¢˜è°ƒç ”"

# æŒ‰æ—¶é—´å‘½å
task_name = f"crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
```

### 3. URLæ ¼å¼å¤„ç†

```python
# æ¨èçš„URLæ ¼å¼
valid_urls = [
    "https://www.zhihu.com/question/378706911/answer/1080446596",  # å®Œæ•´æ ¼å¼
    "https://www.zhihu.com/question/378706911",                    # ç®€æ´æ ¼å¼
    "https://www.zhihu.com/question/378706911?sort=created"        # å¸¦å‚æ•°æ ¼å¼
]
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. URLæ ¼å¼è¦æ±‚
- å¿…é¡»åŒ…å« `question/` å’Œé—®é¢˜ID
- é—®é¢˜IDå¿…é¡»æ˜¯çº¯æ•°å­—
- æ”¯æŒanswerå‚æ•°ä½†ä¸æ˜¯å¿…éœ€

### 2. æ•°æ®åº“å‡†å¤‡
- ç¡®ä¿PostgreSQLæ•°æ®åº“å·²åˆå§‹åŒ–
- ç¡®ä¿æœ‰ç›¸åº”çš„cookiesæ–‡ä»¶
- ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸

### 3. èµ„æºæ¶ˆè€—
- å¤§é—®é¢˜å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
- ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å­˜å‚¨APIå“åº”
- å»ºè®®åœ¨ç½‘ç»œç¨³å®šçš„ç¯å¢ƒä¸‹è¿è¡Œ

### 4. åçˆ¬ç­–ç•¥
- è„šæœ¬å†…ç½®äº†åçˆ¬å¤„ç†
- å¦‚é‡403é”™è¯¯ä¼šè‡ªåŠ¨å°è¯•éªŒè¯è§£å†³
- å»ºè®®ä¸è¦å¹¶å‘è¿è¡Œå¤šä¸ªå®ä¾‹

## ğŸ“‹ æ‰¹é‡é‡‡é›†æ£€æŸ¥æ¸…å•

- [ ] å‡†å¤‡å¥½è¦é‡‡é›†çš„é—®é¢˜URLåˆ—è¡¨
- [ ] ç¡®å®šæ¯ä¸ªé—®é¢˜çš„task_name
- [ ] ç¡®è®¤æ˜¯å¦éœ€è¦é™åˆ¶max_answers
- [ ] æ£€æŸ¥PostgreSQLæ•°æ®åº“è¿æ¥
- [ ] ç¡®è®¤cookiesæ–‡ä»¶å­˜åœ¨ä¸”æœ‰æ•ˆ
- [ ] å‡†å¤‡å¥½è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´
- [ ] ç¡®è®¤ç½‘ç»œè¿æ¥ç¨³å®š
- [ ] é€‰æ‹©é€‚å½“çš„è¿è¡Œæ—¶é—´ï¼ˆé¿å…é«˜å³°æœŸï¼‰

æŒ‰ç…§è¿™ä¸ªæŒ‡å—ï¼Œæ‚¨å°±å¯ä»¥æˆåŠŸä½¿ç”¨ `crawl_specific_question.py` è¿›è¡Œæ‰¹é‡é—®é¢˜é‡‡é›†äº†ï¼
