#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½çŸ¥ä¹çˆ¬è™«

æ•´åˆåŠ¨æ€å‚æ•°è·å–å’ŒAPIæ‰¹é‡è¯·æ±‚çš„æ™ºèƒ½çˆ¬è™«ç³»ç»Ÿ
"""

import time
import asyncio
import aiohttp
import json
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from loguru import logger
import random
from urllib.parse import urlencode

from dynamic_params_extractor import DynamicParamsExtractor
from params_pool_manager import ParamsPoolManager, ParamsRecord
from zhihu_lazyload_crawler import ZhihuLazyLoadCrawler


@dataclass
class CrawlResult:
    """çˆ¬å–ç»“æœ"""
    question_id: str
    success: bool
    data: Optional[Dict] = None
    error: Optional[str] = None
    params_used: Optional[int] = None
    response_time: float = 0.0
    

class SmartCrawler:
    """æ™ºèƒ½çŸ¥ä¹çˆ¬è™«"""
    
    def __init__(self, 
                 params_db_path: str = "params_pool.db",
                 max_pool_size: int = 100,
                 max_concurrent: int = 5,
                 user_data_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½çˆ¬è™«
        
        Args:
            params_db_path: å‚æ•°æ± æ•°æ®åº“è·¯å¾„
            max_pool_size: å‚æ•°æ± æœ€å¤§å®¹é‡
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            user_data_dir: Chromeç”¨æˆ·æ•°æ®ç›®å½•
        """
        self.params_manager = ParamsPoolManager(params_db_path, max_pool_size)
        self.params_extractor = None
        self.legacy_crawler = ZhihuLazyLoadCrawler()
        self.traditional_crawler = ZhihuLazyLoadCrawler()
        self.max_concurrent = max_concurrent
        self.user_data_dir = user_data_dir
        
        # è¯·æ±‚é…ç½®
        self.base_url = "https://www.zhihu.com/api/v4/questions"
        self.timeout = aiohttp.ClientTimeout(total=30)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'params_extracted': 0,
            'fallback_used': 0
        }
        
    async def crawl_question_feeds(self, 
                                   question_id: str, 
                                   limit: int = 20, 
                                   offset: int = 0,
                                   max_retries: int = 3) -> CrawlResult:
        """
        çˆ¬å–é—®é¢˜çš„feedsæ•°æ®
        
        Args:
            question_id: é—®é¢˜ID
            limit: æ¯é¡µæ•°é‡
            offset: åç§»é‡
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            çˆ¬å–ç»“æœ
        """
        start_time = time.time()
        
        for attempt in range(max_retries + 1):
            try:
                # è·å–å‚æ•°
                params_record = await self._get_valid_params(question_id)
                
                if not params_record:
                    logger.warning(f"âš ï¸ æ— å¯ç”¨å‚æ•°ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•çˆ¬å–é—®é¢˜ {question_id}")
                    return await self._fallback_crawl(question_id, start_time)
                    
                # ä½¿ç”¨APIè¯·æ±‚
                result = await self._api_request(question_id, params_record, limit, offset)
                
                if result.success:
                    self.params_manager.mark_params_used(params_record.id, True)
                    self.stats['successful_requests'] += 1
                    return result
                else:
                    self.params_manager.mark_params_used(params_record.id, False)
                    logger.warning(f"âš ï¸ APIè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries + 1}): {result.error}")
                    
            except Exception as e:
                logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™ (å°è¯• {attempt + 1}/{max_retries + 1}): {e}")
                
            # é‡è¯•å‰ç­‰å¾…
            if attempt < max_retries:
                await asyncio.sleep(random.uniform(1, 3))
                
        # æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
        logger.warning(f"âš ï¸ APIæ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•çˆ¬å–é—®é¢˜ {question_id}")
        return await self._fallback_crawl(question_id, start_time)
        
    async def _get_valid_params(self, question_id: str) -> Optional[ParamsRecord]:
        """
        è·å–æœ‰æ•ˆçš„åçˆ¬è™«å‚æ•°
        
        Args:
            question_id: é—®é¢˜ID
            
        Returns:
            æœ‰æ•ˆçš„å‚æ•°è®°å½•
        """
        # é¦–å…ˆå°è¯•ä»æ± ä¸­è·å–
        params_record = self.params_manager.get_best_params()
        
        if params_record and not params_record.is_expired:
            return params_record
            
        # æ± ä¸­æ— å¯ç”¨å‚æ•°ï¼ŒåŠ¨æ€æå–
        logger.info(f"ğŸ”„ ä¸ºé—®é¢˜ {question_id} åŠ¨æ€æå–å‚æ•°")
        
        try:
            if not self.params_extractor:
                self.params_extractor = DynamicParamsExtractor(
                    headless=True, 
                    user_data_dir=self.user_data_dir
                )
                
            params = self.params_extractor.extract_params_from_question(question_id)
            
            if params and self.params_extractor.validate_params(params):
                # æ·»åŠ åˆ°å‚æ•°æ± 
                params['question_id'] = question_id
                if self.params_manager.add_params(params):
                    self.stats['params_extracted'] += 1
                    return self.params_manager.get_best_params()
                    
        except Exception as e:
            logger.error(f"âŒ åŠ¨æ€æå–å‚æ•°å¤±è´¥: {e}")
            
        return None
        
    async def _api_request(self, 
                          question_id: str, 
                          params_record: ParamsRecord, 
                          limit: int, 
                          offset: int) -> CrawlResult:
        """
        æ‰§è¡ŒAPIè¯·æ±‚
        
        Args:
            question_id: é—®é¢˜ID
            params_record: å‚æ•°è®°å½•
            limit: æ¯é¡µæ•°é‡
            offset: åç§»é‡
            
        Returns:
            çˆ¬å–ç»“æœ
        """
        start_time = time.time()
        
        # æ„å»ºURLå’Œå‚æ•°
        url = f"{self.base_url}/{question_id}/feeds"
        
        query_params = {
            'limit': limit,
            'offset': offset,
            'order': 'default',
            'include': 'data[*].is_normal,admin_closed_comment,reward_info,is_collapsed,annotation_action,annotation_detail,collapse_reason,is_sticky,collapsed_by,suggest_edit,comment_count,can_comment,content,editable_content,attachment,voteup_count,reshipment_settings,comment_permission,created_time,updated_time,review_info,relevant_info,question,excerpt,is_labeled,paid_info,paid_info_content,relationship.is_authorized,is_author,voting,is_thanked,is_nothelp,is_recognized;data[*].mark_infos[*].url;data[*].author.follower_count,vip_info,badge[*].topics;data[*].settings.table_of_contents.enabled'
        }
        
        full_url = f"{url}?{urlencode(query_params)}"
        
        # æ„å»ºè¯·æ±‚å¤´
        headers = {
            'accept': '*/*',
            'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'cache-control': 'no-cache',
            'pragma': 'no-cache',
            'sec-ch-ua': '"Google Chrome";v="119", "Chromium";v="119", "Not?A_Brand";v="24"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin',
            **params_record.to_headers()
        }
        
        # æ·»åŠ cookie
        if params_record.session_id:
            headers['cookie'] = f'z_c0={params_record.session_id}'
            
        try:
            async with aiohttp.ClientSession(timeout=self.timeout) as session:
                async with session.get(full_url, headers=headers) as response:
                    response_time = time.time() - start_time
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        # éªŒè¯å“åº”æ•°æ®
                        if self._validate_response(data):
                            logger.info(f"âœ… APIè¯·æ±‚æˆåŠŸ: {question_id}, è€—æ—¶: {response_time:.2f}s")
                            return CrawlResult(
                                question_id=question_id,
                                success=True,
                                data=data,
                                params_used=params_record.id,
                                response_time=response_time
                            )
                        else:
                            error_msg = "å“åº”æ•°æ®æ ¼å¼æ— æ•ˆ"
                            logger.warning(f"âš ï¸ {error_msg}: {question_id}")
                            return CrawlResult(
                                question_id=question_id,
                                success=False,
                                error=error_msg,
                                params_used=params_record.id,
                                response_time=response_time
                            )
                    else:
                        error_msg = f"HTTP {response.status}: {await response.text()}"
                        logger.warning(f"âš ï¸ APIè¯·æ±‚å¤±è´¥: {error_msg}")
                        return CrawlResult(
                            question_id=question_id,
                            success=False,
                            error=error_msg,
                            params_used=params_record.id,
                            response_time=response_time
                        )
                        
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = f"è¯·æ±‚å¼‚å¸¸: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            return CrawlResult(
                question_id=question_id,
                success=False,
                error=error_msg,
                params_used=params_record.id,
                response_time=response_time
            )
            
    def _validate_response(self, data: Dict) -> bool:
        """
        éªŒè¯APIå“åº”æ•°æ®
        
        Args:
            data: å“åº”æ•°æ®
            
        Returns:
            æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        """
        if not isinstance(data, dict):
            return False
            
        # æ£€æŸ¥åŸºæœ¬ç»“æ„
        if 'data' not in data:
            return False
            
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯
        if 'error' in data:
            return False
            
        return True
        
    async def _fallback_crawl(self, question_id: str, start_time: float) -> CrawlResult:
        """
        ä¼ ç»Ÿæ–¹æ³•çˆ¬å–ï¼ˆé™çº§ç­–ç•¥ï¼‰
        
        Args:
            question_id: é—®é¢˜ID
            start_time: å¼€å§‹æ—¶é—´
            
        Returns:
            çˆ¬å–ç»“æœ
        """
        try:
            # ä½¿ç”¨ä¼ ç»Ÿçˆ¬è™«æ–¹æ³•
            # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å
            feeds_data = await asyncio.get_event_loop().run_in_executor(
                None, 
                lambda: self.traditional_crawler.crawl_feeds_with_browser_headers(question_id), 
            )
            
            response_time = time.time() - start_time
            self.stats['fallback_used'] += 1
            
            if feeds_data:
                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                answers = self.traditional_crawler.extract_answers_from_feeds(feeds_data)
                data = {
                    'data': answers,
                    'paging': {'is_end': True, 'totals': len(answers)}
                }
                logger.info(f"âœ… ä¼ ç»Ÿæ–¹æ³•çˆ¬å–æˆåŠŸ: {question_id}, è€—æ—¶: {response_time:.2f}s")
                return CrawlResult(
                    question_id=question_id,
                    success=True,
                    data=data,
                    response_time=response_time
                )
            else:
                error_msg = "ä¼ ç»Ÿæ–¹æ³•æœªè·å–åˆ°æ•°æ®"
                logger.warning(f"âš ï¸ {error_msg}: {question_id}")
                return CrawlResult(
                    question_id=question_id,
                    success=False,
                    error=error_msg,
                    response_time=response_time
                )
                
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = f"ä¼ ç»Ÿæ–¹æ³•å¼‚å¸¸: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            return CrawlResult(
                question_id=question_id,
                success=False,
                error=error_msg,
                response_time=response_time
            )
            
    async def batch_crawl(self, 
                         question_ids: List[str], 
                         limit: int = 20,
                         progress_callback: Optional[callable] = None) -> List[CrawlResult]:
        """
        æ‰¹é‡çˆ¬å–é—®é¢˜
        
        Args:
            question_ids: é—®é¢˜IDåˆ—è¡¨
            limit: æ¯é¡µæ•°é‡
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            çˆ¬å–ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å– {len(question_ids)} ä¸ªé—®é¢˜")
        
        # æ¸…ç†è¿‡æœŸå‚æ•°
        self.params_manager.cleanup_expired_params()
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def crawl_with_semaphore(question_id: str, index: int) -> CrawlResult:
            async with semaphore:
                result = await self.crawl_question_feeds(question_id, limit)
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats['total_requests'] += 1
                if not result.success:
                    self.stats['failed_requests'] += 1
                    
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(index + 1, len(question_ids), result)
                    
                return result
                
        # æ‰§è¡Œæ‰¹é‡çˆ¬å–
        tasks = [
            crawl_with_semaphore(question_id, i) 
            for i, question_id in enumerate(question_ids)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        final_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"âŒ ä»»åŠ¡å¼‚å¸¸: {question_ids[i]} - {result}")
                final_results.append(CrawlResult(
                    question_id=question_ids[i],
                    success=False,
                    error=str(result)
                ))
            else:
                final_results.append(result)
                
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        successful_count = sum(1 for r in final_results if r.success)
        logger.info(f"ğŸ‰ æ‰¹é‡çˆ¬å–å®Œæˆ: {successful_count}/{len(question_ids)} æˆåŠŸ")
        
        return final_results
        
    def get_stats(self) -> Dict:
        """
        è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        pool_stats = self.params_manager.get_pool_stats()
        
        return {
            **self.stats,
            'success_rate': self.stats['successful_requests'] / max(self.stats['total_requests'], 1),
            'pool_stats': pool_stats
        }
        
    def close(self):
        """å…³é—­çˆ¬è™«ï¼Œé‡Šæ”¾èµ„æº"""
        if self.params_extractor:
            self.params_extractor.close()
            
    async def __aenter__(self):
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        self.close()