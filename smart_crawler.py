#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½çŸ¥ä¹çˆ¬è™«

æ™ºèƒ½çŸ¥ä¹çˆ¬è™«ç³»ç»Ÿ
"""

import time
import asyncio
import aiohttp
import json
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from loguru import logger
import random
from urllib.parse import urlencode

from dynamic_params_extractor import DynamicParamsExtractor
from params_pool_manager import ParamsPoolManager, ParamsRecord
from zhihu_lazyload_crawler import ZhihuLazyLoadCrawler


@dataclass
class CrawlResult:
    """çˆ¬å–ç»“æœ"""
    question_id: str
    success: bool
    data: Optional[Dict] = None
    error: Optional[str] = None
    params_used: Optional[int] = None
    response_time: float = 0.0
    

class SmartCrawler:
    """æ™ºèƒ½çŸ¥ä¹çˆ¬è™«"""
    
    def __init__(self, 
                 max_concurrent: int = 5,
                 user_data_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½çˆ¬è™«ï¼ˆä»…ä½¿ç”¨æµè§ˆå™¨æ¨¡æ‹Ÿæ–¹å¼ï¼‰
        
        Args:
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            user_data_dir: Chromeç”¨æˆ·æ•°æ®ç›®å½•
        """
        # åˆå§‹åŒ–çˆ¬è™«ï¼Œå¼ºåˆ¶ä¸ä½¿ç”¨æ— å¤´æµè§ˆå™¨
        self.legacy_crawler = ZhihuLazyLoadCrawler()
        # åˆå§‹åŒ–æµè§ˆå™¨çˆ¬è™«
        from zhihu_lazyload_crawler import BrowserFeedsCrawler
        self.browser_crawler = BrowserFeedsCrawler(headless=False)
        self.max_concurrent = max_concurrent
        self.user_data_dir = user_data_dir
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0
        }
        
    async def crawl_question_feeds(self, 
                                   question_id: str, 
                                   limit: int = 20, 
                                   offset: int = 0,
                                   max_retries: int = 3) -> CrawlResult:
        """
        çˆ¬å–é—®é¢˜çš„feedsæ•°æ®ï¼ˆä»…ä½¿ç”¨seleniumæ–¹å¼ï¼‰
        
        Args:
            question_id: é—®é¢˜ID
            limit: æ¯é¡µæ•°é‡ï¼ˆä¼ é€’ç»™seleniumçˆ¬è™«ï¼‰
            offset: åç§»é‡ï¼ˆä¼ é€’ç»™seleniumçˆ¬è™«ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            çˆ¬å–ç»“æœ
        """
        start_time = time.time()
        
        for attempt in range(max_retries + 1):
            try:
                # ç›´æ¥ä½¿ç”¨seleniumæ–¹å¼çˆ¬å–
                logger.info(f"ğŸ” ä½¿ç”¨seleniumæ–¹å¼çˆ¬å–é—®é¢˜ {question_id} (å°è¯• {attempt + 1}/{max_retries + 1})")
                return await self._selenium_crawl(question_id, start_time, limit)
                    
            except Exception as e:
                logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™ (å°è¯• {attempt + 1}/{max_retries + 1}): {e}")
                
            # é‡è¯•å‰ç­‰å¾…
            if attempt < max_retries:
                await asyncio.sleep(random.uniform(1, 3))
                
        # æ‰€æœ‰é‡è¯•å¤±è´¥
        response_time = time.time() - start_time
        error_msg = f"æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œæ— æ³•çˆ¬å–é—®é¢˜ {question_id}"
        logger.error(f"âŒ {error_msg}")
        return CrawlResult(
            question_id=question_id,
            success=False,
            error=error_msg,
            response_time=response_time
        )
        

        
    async def _selenium_crawl(self, question_id: str, start_time: float, limit: int = 20) -> CrawlResult:
        """
        ä½¿ç”¨æµè§ˆå™¨æ¨¡æ‹Ÿæ–¹å¼çˆ¬å–é—®é¢˜
        
        Args:
            question_id: é—®é¢˜ID
            start_time: å¼€å§‹æ—¶é—´
            limit: é™åˆ¶è·å–çš„å›ç­”æ•°é‡
            
        Returns:
            çˆ¬å–ç»“æœ
        """
        try:
            # ä½¿ç”¨æµè§ˆå™¨çˆ¬è™«æ–¹æ³•
            # è®¾ç½®è¾ƒé•¿çš„æš‚åœæ—¶é—´ï¼Œç¡®ä¿å†…å®¹åŠ è½½å®Œæ•´
            feeds_data = await asyncio.get_event_loop().run_in_executor(
                None, 
                lambda: self.browser_crawler.crawl_feeds_via_browser(question_id, max_scrolls=limit//2, pause=3.0), 
            )
            
            response_time = time.time() - start_time
            self.stats['successful_requests'] += 1
            
            if feeds_data:
                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                answers = self.legacy_crawler.extract_answers_from_feeds(feeds_data)
                data = {
                    'data': answers,
                    'paging': {'is_end': True, 'totals': len(answers)}
                }
                logger.info(f"âœ… Seleniumçˆ¬å–æˆåŠŸ: {question_id}, è·å–åˆ° {len(answers)} ä¸ªå›ç­”, è€—æ—¶: {response_time:.2f}s")
                return CrawlResult(
                    question_id=question_id,
                    success=True,
                    data=data,
                    response_time=response_time
                )
            else:
                error_msg = "Seleniumçˆ¬å–æœªè·å–åˆ°æ•°æ®"
                logger.warning(f"âš ï¸ {error_msg}: {question_id}")
                return CrawlResult(
                    question_id=question_id,
                    success=False,
                    error=error_msg,
                    response_time=response_time
                )
                
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = f"Seleniumçˆ¬å–å¼‚å¸¸: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            return CrawlResult(
                question_id=question_id,
                success=False,
                error=error_msg,
                response_time=response_time
            )
            
    async def batch_crawl(self, 
                         question_ids: List[str], 
                         limit: int = 20,
                         progress_callback: Optional[callable] = None) -> List[CrawlResult]:
        """
        æ‰¹é‡çˆ¬å–é—®é¢˜ï¼ˆä»…ä½¿ç”¨seleniumæ–¹å¼ï¼‰
        
        Args:
            question_ids: é—®é¢˜IDåˆ—è¡¨
            limit: æ¯é¡µæ•°é‡ï¼ˆä¼ é€’ç»™seleniumçˆ¬è™«ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            çˆ¬å–ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å– {len(question_ids)} ä¸ªé—®é¢˜ï¼ˆä½¿ç”¨seleniumæ–¹å¼ï¼‰")
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def crawl_with_semaphore(question_id: str, index: int) -> CrawlResult:
            async with semaphore:
                result = await self.crawl_question_feeds(question_id, limit)
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats['total_requests'] += 1
                if not result.success:
                    self.stats['failed_requests'] += 1
                    
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(index + 1, len(question_ids), result)
                    
                return result
                
        # æ‰§è¡Œæ‰¹é‡çˆ¬å–
        tasks = [
            crawl_with_semaphore(question_id, i) 
            for i, question_id in enumerate(question_ids)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        final_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"âŒ ä»»åŠ¡å¼‚å¸¸: {question_ids[i]} - {result}")
                final_results.append(CrawlResult(
                    question_id=question_ids[i],
                    success=False,
                    error=str(result)
                ))
            else:
                final_results.append(result)
                
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        successful_count = sum(1 for r in final_results if r.success)
        logger.info(f"ğŸ‰ æ‰¹é‡çˆ¬å–å®Œæˆ: {successful_count}/{len(question_ids)} æˆåŠŸ")
        
        return final_results
        
    def get_stats(self):
        """
        è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        # åœ¨ä»…Seleniumæ¨¡å¼ä¸‹ä¸éœ€è¦è·å–å‚æ•°æ± ç»Ÿè®¡ä¿¡æ¯
        
        return {
            **self.stats,
            'success_rate': self.stats['successful_requests'] / max(self.stats['total_requests'], 1),
            'pool_stats': {}
        }
        
    def close(self):
        """å…³é—­çˆ¬è™«ï¼Œé‡Šæ”¾èµ„æº"""
        # åœ¨ä»…Seleniumæ¨¡å¼ä¸‹ä¸éœ€è¦å…³é—­params_extractor
        pass
            
    async def __aenter__(self):
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        self.close()