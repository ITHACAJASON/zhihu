#!/usr/bin/env python3
"""
ä¸“é—¨ç”¨äºé‡‡é›†æŒ‡å®šçŸ¥ä¹é—®é¢˜çš„å®Œæ•´ç­”æ¡ˆæ•°æ®
é—®é¢˜é“¾æ¥ï¼šhttps://www.zhihu.com/question/378706911/answer/1080446596
ç›®æ ‡ï¼šé‡‡é›†å®Œæ•´çš„4470ä¸ªç­”æ¡ˆæ•°æ®
"""

import os
import json
import time
import hashlib
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from loguru import logger

from zhihu_api_crawler import ZhihuAPIAnswerCrawler
from postgres_models import PostgreSQLManager, TaskInfo, Question, Answer


class SpecificQuestionCrawler:
    """ä¸“é—¨ç”¨äºé‡‡é›†æŒ‡å®šé—®é¢˜çš„çˆ¬è™«ç±»"""

    def __init__(self, question_url: str, task_name: str = None):
        self.question_url = question_url
        self.task_name = task_name or "specific_question_crawl"

        # åˆå§‹åŒ–ç»„ä»¶
        self.api_crawler = ZhihuAPIAnswerCrawler()
        self.db_manager = PostgreSQLManager()

        # æå–é—®é¢˜ID
        self.question_id = self._extract_question_id()
        if not self.question_id:
            raise ValueError(f"æ— æ³•ä»URLæå–é—®é¢˜ID: {question_url}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("output") / f"question_{self.question_id}"
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºä»»åŠ¡ID
        self.task_id = self._create_or_get_task()

        logger.info(f"ä¸“é¡¹é—®é¢˜çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"é—®é¢˜ID: {self.question_id}")
        logger.info(f"ä»»åŠ¡ID: {self.task_id}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")

    def _extract_question_id(self) -> Optional[str]:
        """æå–é—®é¢˜ID"""
        return self.api_crawler.extract_question_id_from_url(self.question_url)

    def _create_or_get_task(self) -> str:
        """åˆ›å»ºæˆ–è·å–ä»»åŠ¡"""
        # æŸ¥æ‰¾ç°æœ‰ä»»åŠ¡
        existing_tasks = self.db_manager.get_tasks_by_keyword(self.task_name)
        if existing_tasks:
            task = existing_tasks[0]
            logger.info(f"æ‰¾åˆ°ç°æœ‰ä»»åŠ¡: {task.task_id}")
            return task.task_id

        # åˆ›å»ºæ–°ä»»åŠ¡
        task_id = self.db_manager.create_task(
            keywords=self.task_name,
            start_date="2024-01-01",  # æ‰©å¤§æ—¶é—´èŒƒå›´
            end_date="2025-12-31"
        )
        logger.info(f"åˆ›å»ºæ–°ä»»åŠ¡: {task_id}")
        return task_id

    def _save_api_response(self, response_data: Dict, page_num: int,
                         cursor: str = None, offset: int = 0) -> str:
        """ä¿å­˜APIå“åº”æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            if cursor:
                filename = f"api_response_page_{page_num}_cursor_{cursor[:10]}_{timestamp}.json"
            else:
                filename = f"api_response_page_{page_num}_offset_{offset}_{timestamp}.json"

            filepath = self.output_dir / filename

            # æ·»åŠ å…ƒæ•°æ®
            response_with_meta = {
                "metadata": {
                    "question_id": self.question_id,
                    "question_url": self.question_url,
                    "task_id": self.task_id,
                    "page_num": page_num,
                    "cursor": cursor,
                    "offset": offset,
                    "timestamp": datetime.now().isoformat(),
                    "response_hash": hashlib.md5(json.dumps(response_data, sort_keys=True).encode()).hexdigest()
                },
                "response": response_data
            }

            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(response_with_meta, f, ensure_ascii=False, indent=2)

            logger.info(f"âœ… APIå“åº”å·²ä¿å­˜: {filepath}")
            return str(filepath)

        except Exception as e:
            logger.error(f"ä¿å­˜APIå“åº”å¤±è´¥: {e}")
            return ""

    def _fetch_answers_with_save(self, cursor: str = None, offset: int = 0,
                               limit: int = 20, page_num: int = 0) -> Tuple[Optional[Dict], str]:
        """è·å–ç­”æ¡ˆå¹¶ä¿å­˜å“åº”æ•°æ®"""
        try:
            # ä½¿ç”¨åŸæœ‰çš„APIè·å–æ–¹æ³•
            response_data = self.api_crawler.fetch_answers_page(
                self.question_id, cursor, offset, limit,
                save_response_callback=self._save_api_response,
                page_num=page_num
            )

            if response_data:
                # å“åº”æ•°æ®å·²ç»åœ¨å›è°ƒå‡½æ•°ä¸­ä¿å­˜äº†
                return response_data, f"page_{page_num}_saved"
            else:
                logger.warning("è·å–ç­”æ¡ˆæ•°æ®å¤±è´¥")
                return None, ""

        except Exception as e:
            logger.error(f"è·å–ç­”æ¡ˆæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None, ""

    def _parse_answers_from_api(self, answers_data: List[Dict]) -> List[Answer]:
        """ä»APIå“åº”ä¸­è§£æç­”æ¡ˆ"""
        answers = []
        for answer_data in answers_data:
            answer = self.api_crawler.parse_answer_data(answer_data, self.question_id, self.task_id)
            if answer:
                answers.append(answer)
        return answers

    def crawl_all_answers(self, max_answers: int = None) -> Dict:
        """çˆ¬å–æ‰€æœ‰ç­”æ¡ˆå¹¶ä¿å­˜æ•°æ®"""
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–é—®é¢˜ {self.question_id} çš„æ‰€æœ‰ç­”æ¡ˆ")
        logger.info(f"ç›®æ ‡ç­”æ¡ˆæ•°é‡: {max_answers or 'å…¨éƒ¨'}")

        start_time = time.time()
        all_answers = []
        saved_files = []
        cursor = None
        offset = 0
        limit = 20
        page_count = 0
        total_api_calls = 0

        try:
            while True:
                page_count += 1
                total_api_calls += 1

                logger.info(f"ğŸ“„ è·å–ç¬¬ {page_count} é¡µç­”æ¡ˆ (cursor={cursor}, offset={offset})")

                # è·å–æ•°æ®å¹¶ä¿å­˜å“åº”
                page_data, saved_file = self._fetch_answers_with_save(cursor, offset, limit, page_count)
                if saved_file:
                    saved_files.append(saved_file)

                if not page_data:
                    logger.error(f"è·å–ç¬¬ {page_count} é¡µæ•°æ®å¤±è´¥")
                    break

                # è§£æåˆ†é¡µä¿¡æ¯
                paging = page_data.get('paging', {})
                is_end = paging.get('is_end', True)
                next_url = paging.get('next', '')

                # è·å–ç­”æ¡ˆæ•°æ®
                answers_data = page_data.get('data', [])
                logger.info(f"ğŸ“¦ ç¬¬ {page_count} é¡µè·å–åˆ° {len(answers_data)} ä¸ªç­”æ¡ˆ")

                # è§£æç­”æ¡ˆæ•°æ®
                page_answers = self._parse_answers_from_api(answers_data)
                logger.info(f"ğŸ“ æœ¬é¡µè§£æå‡º {len(page_answers)} ä¸ªæœ‰æ•ˆç­”æ¡ˆ")

                # æ·»åŠ åˆ°æ€»ç­”æ¡ˆåˆ—è¡¨
                all_answers.extend(page_answers)

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§ç­”æ¡ˆæ•°é™åˆ¶
                if max_answers and len(all_answers) >= max_answers:
                    logger.info(f"âœ… å·²è¾¾åˆ°æœ€å¤§ç­”æ¡ˆæ•°é™åˆ¶: {max_answers}")
                    break

                # æ£€æŸ¥æ˜¯å¦å·²ç»åˆ°æœ€åä¸€é¡µ
                if is_end:
                    logger.info(f"ğŸ¯ å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break

                # è§£æä¸‹ä¸€é¡µå‚æ•°
                if next_url:
                    next_params = self.api_crawler._parse_next_url_params(next_url)
                    if 'cursor' in next_params:
                        cursor = next_params['cursor']
                        logger.info(f"ğŸ”„ æ›´æ–°cursor: {cursor}")
                    elif 'offset' in next_params:
                        offset = int(next_params['offset'])
                        cursor = None
                        logger.info(f"ğŸ”„ æ›´æ–°offset: {offset}")
                    else:
                        offset += limit
                        cursor = None
                        logger.info(f"ğŸ”„ é€’å¢offset: {offset}")
                else:
                    offset += limit
                    cursor = None
                    logger.info(f"ğŸ”„ é€’å¢offset: {offset}")

                # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(3)

                # å®‰å…¨æ£€æŸ¥ï¼šé¿å…æ— é™å¾ªç¯
                if page_count > 500:  # æœ€å¤š500é¡µ
                    logger.warning(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ï¼Œåœæ­¢çˆ¬å–")
                    break

            # å¡«å……ç­”æ¡ˆçš„contentå­—æ®µ
            if all_answers:
                all_answers = self._fill_answers_content(all_answers)

            # ä¿å­˜ç­”æ¡ˆåˆ°æ•°æ®åº“
            saved_to_db = False
            if all_answers:
                saved_to_db = self._save_answers_to_db(all_answers)

            end_time = time.time()
            duration = end_time - start_time

            result = {
                'question_id': self.question_id,
                'question_url': self.question_url,
                'task_id': self.task_id,
                'total_answers': len(all_answers),
                'total_pages': page_count,
                'total_api_calls': total_api_calls,
                'saved_files_count': len(saved_files),
                'saved_files': saved_files,
                'saved_to_db': saved_to_db,
                'duration_seconds': round(duration, 2),
                'crawl_time': datetime.now().isoformat(),
                'answers': all_answers
            }

            logger.info(f"ğŸ‰ é—®é¢˜ {self.question_id} ç­”æ¡ˆçˆ¬å–å®Œæˆ")
            logger.info(f"ğŸ“Š æ€»å…±è·å–åˆ° {len(all_answers)} ä¸ªç­”æ¡ˆ")
            logger.info(f"ğŸ“„ å…±è¯·æ±‚äº† {page_count} é¡µæ•°æ®")
            logger.info(f"ğŸ’¾ ä¿å­˜äº† {len(saved_files)} ä¸ªAPIå“åº”æ–‡ä»¶")
            logger.info(f"â±ï¸ è€—æ—¶ {duration:.2f} ç§’")

            return result

        except Exception as e:
            logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {
                'question_id': self.question_id,
                'error': str(e),
                'total_answers': len(all_answers),
                'saved_files_count': len(saved_files),
                'saved_files': saved_files
            }

    def _fill_answers_content(self, answers: List[Answer]) -> List[Answer]:
        """ä¸ºç­”æ¡ˆåˆ—è¡¨å¡«å……contentå­—æ®µ"""
        logger.info(f"å¼€å§‹ä¸º {len(answers)} ä¸ªç­”æ¡ˆå¡«å……content")

        filled_count = 0
        for i, answer in enumerate(answers):
            if not answer.content and answer.answer_id:
                logger.debug(f"è·å–ç­”æ¡ˆ {answer.answer_id} çš„content ({i+1}/{len(answers)})")
                content = self.api_crawler.fetch_single_answer_content(answer.answer_id)

                if content:
                    answer.content = content
                    filled_count += 1

                    # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(0.5)
                else:
                    logger.warning(f"æ— æ³•è·å–ç­”æ¡ˆ {answer.answer_id} çš„content")

        logger.info(f"æˆåŠŸå¡«å…… {filled_count}/{len(answers)} ä¸ªç­”æ¡ˆçš„content")
        return answers

    def _save_answers_to_db(self, answers: List[Answer]) -> bool:
        """ä¿å­˜ç­”æ¡ˆæ•°æ®åˆ°æ•°æ®åº“"""
        try:
            saved_count = 0
            for answer in answers:
                # ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡
                content_hash = hashlib.md5(answer.content.encode('utf-8')).hexdigest()
                answer.content_hash = content_hash

                if self.db_manager.save_answer(answer):
                    saved_count += 1
                else:
                    logger.warning(f"ä¿å­˜ç­”æ¡ˆå¤±è´¥: {answer.answer_id}")

            logger.info(f"æˆåŠŸä¿å­˜ {saved_count}/{len(answers)} ä¸ªç­”æ¡ˆåˆ°æ•°æ®åº“")
            return saved_count == len(answers)

        except Exception as e:
            logger.error(f"ä¿å­˜ç­”æ¡ˆåˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return False

    def save_crawl_summary(self, result: Dict) -> str:
        """ä¿å­˜çˆ¬å–æ‘˜è¦ä¿¡æ¯"""
        try:
            summary_file = self.output_dir / "crawl_summary.json"

            summary = {
                "crawl_info": {
                    "question_id": self.question_id,
                    "question_url": self.question_url,
                    "task_id": self.task_id,
                    "task_name": self.task_name,
                    "crawl_time": datetime.now().isoformat(),
                    "target_answers": 4470
                },
                "results": result,
                "statistics": {
                    "total_answers_collected": result.get('total_answers', 0),
                    "total_pages": result.get('total_pages', 0),
                    "total_api_calls": result.get('total_api_calls', 0),
                    "saved_files_count": result.get('saved_files_count', 0),
                    "saved_to_db": result.get('saved_to_db', False),
                    "duration_seconds": result.get('duration_seconds', 0),
                    "completion_rate": round(result.get('total_answers', 0) / 4470 * 100, 2) if result.get('total_answers', 0) > 0 else 0
                }
            }

            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)

            logger.info(f"âœ… çˆ¬å–æ‘˜è¦å·²ä¿å­˜: {summary_file}")
            return str(summary_file)

        except Exception as e:
            logger.error(f"ä¿å­˜çˆ¬å–æ‘˜è¦å¤±è´¥: {e}")
            return ""


def main():
    """ä¸»å‡½æ•°"""
    # æŒ‡å®šçš„é—®é¢˜é“¾æ¥
    question_url = "https://www.zhihu.com/question/378706911/answer/1080446596"

    try:
        # åˆå§‹åŒ–çˆ¬è™«
        crawler = SpecificQuestionCrawler(question_url, "question_378706911_full_crawl")

        # å¼€å§‹çˆ¬å–
        logger.info("å¼€å§‹æ‰§è¡Œä¸“é¡¹é—®é¢˜ç­”æ¡ˆé‡‡é›†ä»»åŠ¡...")
        result = crawler.crawl_all_answers(max_answers=4470)

        # ä¿å­˜æ‘˜è¦
        summary_file = crawler.save_crawl_summary(result)

        # è¾“å‡ºç»“æœ
        print("\n" + "="*50)
        print("ä¸“é¡¹é—®é¢˜ç­”æ¡ˆé‡‡é›†ä»»åŠ¡å®Œæˆï¼")
        print("="*50)
        print(f"é—®é¢˜ID: {result.get('question_id', 'N/A')}")
        print(f"ä»»åŠ¡ID: {result.get('task_id', 'N/A')}")
        print(f"é‡‡é›†ç­”æ¡ˆæ•°é‡: {result.get('total_answers', 0)}")
        print(f"ç›®æ ‡ç­”æ¡ˆæ•°é‡: 4470")
        print(f"å®Œæˆç‡: {result.get('total_answers', 0)/4470*100:.2f}%")
        print(f"è¯·æ±‚é¡µæ•°: {result.get('total_pages', 0)}")
        print(f"APIè°ƒç”¨æ¬¡æ•°: {result.get('total_api_calls', 0)}")
        print(f"ä¿å­˜æ–‡ä»¶æ•°é‡: {result.get('saved_files_count', 0)}")
        print(f"æ•°æ®åº“ä¿å­˜çŠ¶æ€: {'æˆåŠŸ' if result.get('saved_to_db', False) else 'å¤±è´¥'}")
        print(f"è€—æ—¶: {result.get('duration_seconds', 0)} ç§’")
        print(f"è¾“å‡ºç›®å½•: {crawler.output_dir}")
        if summary_file:
            print(f"æ‘˜è¦æ–‡ä»¶: {summary_file}")
        print("="*50)

        return result

    except Exception as e:
        logger.error(f"æ‰§è¡Œä¸“é¡¹é‡‡é›†ä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None


if __name__ == "__main__":
    main()
