# çŸ¥ä¹é—®ç­”çˆ¬è™«

ä¸€ä¸ªåŸºäº Selenium çš„çŸ¥ä¹é—®ç­”çˆ¬è™«ï¼Œèƒ½å¤Ÿä» PostgreSQL æ•°æ®åº“è¯»å–é—®é¢˜ URLï¼Œè‡ªåŠ¨é‡‡é›†é—®é¢˜é¡µé¢çš„æ‰€æœ‰å›ç­”ã€‚

## åŠŸèƒ½ç‰¹ç‚¹

- ğŸš€ **æ™ºèƒ½çˆ¬å–**: ä»æ•°æ®åº“è¯»å–é—®é¢˜ URL å’Œç›®æ ‡å›ç­”æ•°ï¼Œè‡ªåŠ¨åˆ¤æ–­çˆ¬å–å®Œæˆæ¡ä»¶
- ğŸ” **ç”¨æˆ·ç™»å½•**: æ”¯æŒæ‰‹åŠ¨ç™»å½•çŸ¥ä¹è´¦å·ï¼Œé¿å…ç™»å½•éªŒè¯é—®é¢˜
- ğŸ›¡ï¸ **ååçˆ¬**: ä½¿ç”¨å¤šç§åæ£€æµ‹æŠ€æœ¯ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
- ğŸ’¾ **å†…å­˜ä¼˜åŒ–**: æ¯é‡‡é›† 200 ä¸ªå›ç­”è‡ªåŠ¨æ¸…ç† DOMï¼Œé¿å…å†…å­˜æº¢å‡º
- ğŸ“Š **è¿›åº¦è·Ÿè¸ª**: å®æ—¶æ˜¾ç¤ºçˆ¬å–è¿›åº¦å’Œå®Œæˆåº¦ç»Ÿè®¡
- ğŸ”„ **æ–­ç‚¹ç»­çˆ¬**: æ”¯æŒä¸­æ–­åç»§ç»­çˆ¬å–ï¼Œé¿å…é‡å¤é‡‡é›†

## ç³»ç»Ÿè¦æ±‚

- Python 3.7+
- PostgreSQL æ•°æ®åº“
- Chrome æµè§ˆå™¨
- macOS/Linux/Windows

## å®‰è£…ä¾èµ–

```bash
# å®‰è£… Python ä¾èµ–
pip3 install -r requirements.txt

# ç¡®ä¿ Chrome æµè§ˆå™¨å·²å®‰è£…
# ChromeDriver ä¼šè‡ªåŠ¨ä¸‹è½½ç®¡ç†
```

## æ•°æ®åº“å‡†å¤‡

### 1. åˆ›å»ºæ•°æ®åº“

```sql
CREATE DATABASE zhihu_crawl;
```

### 2. åˆ›å»º questions è¡¨

```sql
CREATE TABLE questions (
    id SERIAL PRIMARY KEY,
    url TEXT NOT NULL,
    answer_count INTEGER NOT NULL,
    crawl_status VARCHAR(20) DEFAULT 'pending',
    crawled_count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 3. æ’å…¥æµ‹è¯•æ•°æ®

```sql
INSERT INTO questions (url, answer_count) VALUES 
('https://www.zhihu.com/question/123456789', 100),
('https://www.zhihu.com/question/987654321', 50);
```

## é…ç½®è®¾ç½®

### ç¯å¢ƒå˜é‡é…ç½®ï¼ˆå¯é€‰ï¼‰

```bash
export DB_HOST=localhost
export DB_PORT=5432
export DB_NAME=zhihu_crawl
export DB_USER=postgres
export DB_PASSWORD=your_password
```

### ä¿®æ”¹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `config.py` æ–‡ä»¶ï¼Œæ ¹æ®éœ€è¦è°ƒæ•´ä»¥ä¸‹é…ç½®ï¼š

```python
# æ•°æ®åº“é…ç½®
DATABASE_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'zhihu_crawl',
    'user': 'postgres',
    'password': 'your_password'
}

# çˆ¬è™«é…ç½®
CRAWLER_CONFIG = {
    'headless': False,  # è®¾ç½®ä¸º True å¯ç”¨æ— å¤´æ¨¡å¼
    'answers_per_cleanup': 200,  # DOM æ¸…ç†é¢‘ç‡
    'scroll_delay': (1, 3),  # æ»šåŠ¨å»¶æ—¶
    'page_load_delay': (2, 4),  # é¡µé¢åŠ è½½å»¶æ—¶
}
```

## ä½¿ç”¨æ–¹æ³•

### 1. ç¯å¢ƒåˆå§‹åŒ–

#### å®‰è£…ä¾èµ–
```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /path/to/zhihu

# å®‰è£… Python ä¾èµ–
pip3 install -r requirements.txt
```

#### æ•°æ®åº“åˆå§‹åŒ–
```bash
# è¿æ¥ PostgreSQL æ•°æ®åº“
psql -U postgres

# åˆ›å»ºæ•°æ®åº“
CREATE DATABASE zhihu_crawl;

# åˆ‡æ¢åˆ°æ–°æ•°æ®åº“
\c zhihu_crawl;

# åˆ›å»º questions è¡¨
CREATE TABLE questions (
    id SERIAL PRIMARY KEY,
    url TEXT NOT NULL,
    answer_count INTEGER NOT NULL,
    crawl_status VARCHAR(20) DEFAULT 'pending',
    crawled_count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

# åˆ›å»º answers è¡¨
CREATE TABLE answers (
    id SERIAL PRIMARY KEY,
    question_id TEXT NOT NULL,
    answer_id TEXT UNIQUE NOT NULL,
    author TEXT,
    content TEXT,
    vote_count INTEGER DEFAULT 0,
    create_time TIMESTAMP,
    task_id TEXT,
    url TEXT,
    crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

# é€€å‡º psql
\q
```

#### é…ç½®æ–‡ä»¶è®¾ç½®
```bash
# ç¼–è¾‘é…ç½®æ–‡ä»¶
vim config.py

# æˆ–ä½¿ç”¨å…¶ä»–ç¼–è¾‘å™¨
nano config.py
```

### 2. æ•°æ®å‡†å¤‡

#### æ·»åŠ å¾…çˆ¬å–é—®é¢˜
```bash
# æ–¹æ³•1: ç›´æ¥ä½¿ç”¨ SQL æ’å…¥
psql -U postgres -d zhihu_crawl -c "
INSERT INTO questions (url, answer_count) VALUES 
('https://www.zhihu.com/question/123456789', 100),
('https://www.zhihu.com/question/987654321', 50);"

# æ–¹æ³•2: ä» CSV æ–‡ä»¶æ‰¹é‡å¯¼å…¥
# å‡†å¤‡ questions.csv æ–‡ä»¶ï¼Œæ ¼å¼ï¼šurl,answer_count
# https://www.zhihu.com/question/123456789,100
# https://www.zhihu.com/question/987654321,50

psql -U postgres -d zhihu_crawl -c "
\COPY questions(url, answer_count) FROM 'questions.csv' DELIMITER ',' CSV HEADER;"
```

#### æŸ¥çœ‹å¾…çˆ¬å–é—®é¢˜
```bash
# æŸ¥çœ‹æ‰€æœ‰é—®é¢˜
psql -U postgres -d zhihu_crawl -c "SELECT * FROM questions;"

# æŸ¥çœ‹å¾…çˆ¬å–é—®é¢˜ç»Ÿè®¡
psql -U postgres -d zhihu_crawl -c "
SELECT 
    crawl_status,
    COUNT(*) as count,
    SUM(answer_count) as total_answers,
    SUM(crawled_count) as crawled_answers
FROM questions 
GROUP BY crawl_status;"
```

### 3. å…¨é‡é‡‡é›†

#### å¯åŠ¨å®Œæ•´çˆ¬è™«æµç¨‹
```bash
# å¯åŠ¨çˆ¬è™«ï¼ˆäº¤äº’æ¨¡å¼ï¼‰
python3 main.py

# ç¨‹åºå¯åŠ¨åçš„æ“ä½œæ­¥éª¤ï¼š
# 1. ç­‰å¾… Chrome æµè§ˆå™¨è‡ªåŠ¨æ‰“å¼€
# 2. æ‰‹åŠ¨ç™»å½•çŸ¥ä¹è´¦å·ï¼ˆåŒ…æ‹¬éªŒè¯ç ï¼‰
# 3. ç™»å½•æˆåŠŸååœ¨æ§åˆ¶å°è¾“å…¥ 'done' ç»§ç»­
# 4. ç¨‹åºè‡ªåŠ¨å¼€å§‹çˆ¬å–æ‰€æœ‰å¾…å¤„ç†é—®é¢˜
```

#### åå°è¿è¡Œæ¨¡å¼
```bash
# ä½¿ç”¨ nohup åå°è¿è¡Œ
nohup python3 main.py > crawler.log 2>&1 &

# æŸ¥çœ‹è¿è¡ŒçŠ¶æ€
tail -f crawler.log

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep main.py

# åœæ­¢åå°è¿›ç¨‹
kill -TERM <è¿›ç¨‹ID>
```

### 4. æ–­ç‚¹ç»­ä¼ 

#### æŸ¥çœ‹çˆ¬å–è¿›åº¦
```bash
# æŸ¥çœ‹æ•´ä½“è¿›åº¦
psql -U postgres -d zhihu_crawl -c "
SELECT 
    url,
    answer_count as target_count,
    crawled_count,
    ROUND(crawled_count::float / answer_count * 100, 2) as progress_percent,
    crawl_status
FROM questions 
ORDER BY id;"

# æŸ¥çœ‹æœªå®Œæˆçš„é—®é¢˜
psql -U postgres -d zhihu_crawl -c "
SELECT * FROM questions 
WHERE crawl_status != 'completed' 
OR crawled_count < answer_count;"
```

#### é‡æ–°å¯åŠ¨çˆ¬è™«
```bash
# ç›´æ¥é‡æ–°å¯åŠ¨ï¼Œç¨‹åºä¼šè‡ªåŠ¨ä»ä¸­æ–­å¤„ç»§ç»­
python3 main.py

# ç¨‹åºä¼šè‡ªåŠ¨ï¼š
# 1. æ£€æŸ¥æ•°æ®åº“ä¸­çš„çˆ¬å–çŠ¶æ€
# 2. è·³è¿‡å·²å®Œæˆçš„é—®é¢˜
# 3. ä»æœªå®Œæˆçš„é—®é¢˜ç»§ç»­çˆ¬å–
```

#### é‡ç½®ç‰¹å®šé—®é¢˜çŠ¶æ€
```bash
# é‡ç½®æŸä¸ªé—®é¢˜çš„çˆ¬å–çŠ¶æ€
psql -U postgres -d zhihu_crawl -c "
UPDATE questions 
SET crawl_status = 'pending', crawled_count = 0 
WHERE url = 'https://www.zhihu.com/question/123456789';"

# é‡ç½®æ‰€æœ‰é—®é¢˜çŠ¶æ€
psql -U postgres -d zhihu_crawl -c "
UPDATE questions 
SET crawl_status = 'pending', crawled_count = 0;"
```

### 5. æ•°æ®å¯¼å‡º

#### å¯¼å‡ºå›ç­”æ•°æ®
```bash
# å¯¼å‡ºæ‰€æœ‰å›ç­”ä¸º CSV
psql -U postgres -d zhihu_crawl -c "
\COPY (SELECT * FROM answers) TO 'answers_export.csv' DELIMITER ',' CSV HEADER;"

# å¯¼å‡ºç‰¹å®šé—®é¢˜çš„å›ç­”
psql -U postgres -d zhihu_crawl -c "
\COPY (
    SELECT * FROM answers 
    WHERE question_id = '123456789'
) TO 'question_123456789_answers.csv' DELIMITER ',' CSV HEADER;"

# å¯¼å‡ºå›ç­”ç»Ÿè®¡ä¿¡æ¯
psql -U postgres -d zhihu_crawl -c "
\COPY (
    SELECT 
        question_id,
        COUNT(*) as answer_count,
        AVG(vote_count) as avg_votes,
        MAX(vote_count) as max_votes,
        MIN(create_time) as earliest_answer,
        MAX(create_time) as latest_answer
    FROM answers 
    GROUP BY question_id
) TO 'questions_summary.csv' DELIMITER ',' CSV HEADER;"
```

#### å¯¼å‡ºä¸º JSON æ ¼å¼
```bash
# å¯¼å‡ºä¸º JSONï¼ˆéœ€è¦ PostgreSQL 9.2+ï¼‰
psql -U postgres -d zhihu_crawl -c "
\COPY (
    SELECT row_to_json(answers) FROM answers
) TO 'answers_export.json';"

# å¯¼å‡ºç»“æ„åŒ– JSON
psql -U postgres -d zhihu_crawl -c "
\COPY (
    SELECT json_build_object(
        'question_id', question_id,
        'answers', json_agg(
            json_build_object(
                'answer_id', answer_id,
                'author', author,
                'content', content,
                'vote_count', vote_count,
                'create_time', create_time
            )
        )
    )
    FROM answers 
    GROUP BY question_id
) TO 'questions_with_answers.json';"
```

#### æ•°æ®å¤‡ä»½
```bash
# å¤‡ä»½æ•´ä¸ªæ•°æ®åº“
pg_dump -U postgres zhihu_crawl > zhihu_crawl_backup.sql

# ä»…å¤‡ä»½æ•°æ®ï¼ˆä¸åŒ…æ‹¬è¡¨ç»“æ„ï¼‰
pg_dump -U postgres --data-only zhihu_crawl > zhihu_crawl_data.sql

# æ¢å¤æ•°æ®åº“
psql -U postgres -d zhihu_crawl < zhihu_crawl_backup.sql
```

### 6. ç›‘æ§å’Œç®¡ç†

#### å®æ—¶ç›‘æ§çˆ¬å–è¿›åº¦
```bash
# ç›‘æ§è„šæœ¬ï¼ˆæ¯10ç§’åˆ·æ–°ä¸€æ¬¡ï¼‰
watch -n 10 "psql -U postgres -d zhihu_crawl -c '
SELECT 
    COUNT(*) as total_questions,
    SUM(CASE WHEN crawl_status = \"completed\" THEN 1 ELSE 0 END) as completed,
    SUM(answer_count) as total_target_answers,
    SUM(crawled_count) as total_crawled_answers,
    ROUND(SUM(crawled_count)::float / SUM(answer_count) * 100, 2) as overall_progress
FROM questions;'"
```

#### æŸ¥çœ‹æ—¥å¿—
```bash
# æŸ¥çœ‹å®æ—¶æ—¥å¿—
tail -f zhihu_crawler.log

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
grep -i error zhihu_crawler.log

# æŸ¥çœ‹ç‰¹å®šæ—¶é—´æ®µçš„æ—¥å¿—
grep "2024-01-01" zhihu_crawler.log
```

#### æ€§èƒ½ä¼˜åŒ–
```bash
# æŸ¥çœ‹æ•°æ®åº“æ€§èƒ½
psql -U postgres -d zhihu_crawl -c "
SELECT 
    schemaname,
    tablename,
    attname,
    n_distinct,
    correlation
FROM pg_stats 
WHERE tablename IN ('questions', 'answers');"

# åˆ›å»ºç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
psql -U postgres -d zhihu_crawl -c "
CREATE INDEX IF NOT EXISTS idx_answers_question_id ON answers(question_id);
CREATE INDEX IF NOT EXISTS idx_answers_create_time ON answers(create_time);
CREATE INDEX IF NOT EXISTS idx_questions_crawl_status ON questions(crawl_status);"
```

### 7. å¿«é€Ÿå‘½ä»¤å‚è€ƒ

#### å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥è¡¨

| æ“ä½œç±»å‹ | å‘½ä»¤ | è¯´æ˜ |
|---------|------|------|
| **ç¯å¢ƒåˆå§‹åŒ–** | `pip3 install -r requirements.txt` | å®‰è£…ä¾èµ– |
| | `psql -U postgres` | è¿æ¥æ•°æ®åº“ |
| | `CREATE DATABASE zhihu_crawl;` | åˆ›å»ºæ•°æ®åº“ |
| **æ•°æ®å‡†å¤‡** | `psql -U postgres -d zhihu_crawl -c "INSERT INTO questions..."` | æ·»åŠ é—®é¢˜ |
| | `psql -U postgres -d zhihu_crawl -c "SELECT * FROM questions;"` | æŸ¥çœ‹é—®é¢˜ |
| **çˆ¬è™«è¿è¡Œ** | `python3 main.py` | å¯åŠ¨çˆ¬è™« |
| | `nohup python3 main.py > crawler.log 2>&1 &` | åå°è¿è¡Œ |
| | `tail -f crawler.log` | æŸ¥çœ‹æ—¥å¿— |
| **è¿›åº¦ç›‘æ§** | `psql -U postgres -d zhihu_crawl -c "SELECT crawl_status, COUNT(*) FROM questions GROUP BY crawl_status;"` | æŸ¥çœ‹çŠ¶æ€ç»Ÿè®¡ |
| | `watch -n 10 "psql -U postgres -d zhihu_crawl -c 'SELECT COUNT(*) FROM answers;'"` | å®æ—¶ç›‘æ§ |
| **æ•°æ®å¯¼å‡º** | `psql -U postgres -d zhihu_crawl -c "\COPY (SELECT * FROM answers) TO 'answers.csv' CSV HEADER;"` | å¯¼å‡ºCSV |
| | `pg_dump -U postgres zhihu_crawl > backup.sql` | æ•°æ®åº“å¤‡ä»½ |
| **æ•…éšœå¤„ç†** | `ps aux | grep main.py` | æŸ¥çœ‹è¿›ç¨‹ |
| | `kill -TERM <PID>` | åœæ­¢è¿›ç¨‹ |
| | `UPDATE questions SET crawl_status = 'pending';` | é‡ç½®çŠ¶æ€ |

#### ä¸€é”®è„šæœ¬ç¤ºä¾‹

**å®Œæ•´åˆå§‹åŒ–è„šæœ¬** (`init.sh`):
```bash
#!/bin/bash
echo "=== çŸ¥ä¹çˆ¬è™«åˆå§‹åŒ– ==="

# 1. å®‰è£…ä¾èµ–
echo "å®‰è£… Python ä¾èµ–..."
pip3 install -r requirements.txt

# 2. åˆ›å»ºæ•°æ®åº“å’Œè¡¨
echo "åˆå§‹åŒ–æ•°æ®åº“..."
psql -U postgres -c "CREATE DATABASE zhihu_crawl;"
psql -U postgres -d zhihu_crawl -c "
CREATE TABLE questions (
    id SERIAL PRIMARY KEY,
    url TEXT NOT NULL,
    answer_count INTEGER NOT NULL,
    crawl_status VARCHAR(20) DEFAULT 'pending',
    crawled_count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE answers (
    id SERIAL PRIMARY KEY,
    question_id TEXT NOT NULL,
    answer_id TEXT UNIQUE NOT NULL,
    author TEXT,
    content TEXT,
    vote_count INTEGER DEFAULT 0,
    create_time TIMESTAMP,
    task_id TEXT,
    url TEXT,
    crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_answers_question_id ON answers(question_id);
CREATE INDEX idx_answers_create_time ON answers(create_time);
CREATE INDEX idx_questions_crawl_status ON questions(crawl_status);
"

echo "åˆå§‹åŒ–å®Œæˆï¼"
```

**æ‰¹é‡æ·»åŠ é—®é¢˜è„šæœ¬** (`add_questions.sh`):
```bash
#!/bin/bash
# ä½¿ç”¨æ–¹æ³•: ./add_questions.sh questions.txt
# questions.txt æ ¼å¼: æ¯è¡Œä¸€ä¸ªURLï¼Œç”¨ç©ºæ ¼åˆ†éš”URLå’Œç›®æ ‡å›ç­”æ•°
# ä¾‹å¦‚: https://www.zhihu.com/question/123456789 100

if [ $# -eq 0 ]; then
    echo "ä½¿ç”¨æ–¹æ³•: $0 <questions_file>"
    echo "æ–‡ä»¶æ ¼å¼: URL ç›®æ ‡å›ç­”æ•°"
    exit 1
fi

while read -r url count; do
    if [ -n "$url" ] && [ -n "$count" ]; then
        psql -U postgres -d zhihu_crawl -c "
        INSERT INTO questions (url, answer_count) VALUES ('$url', $count);"
        echo "å·²æ·»åŠ : $url (ç›®æ ‡: $count ä¸ªå›ç­”)"
    fi
done < "$1"

echo "æ‰¹é‡æ·»åŠ å®Œæˆï¼"
```

**ç›‘æ§è„šæœ¬** (`monitor.sh`):
```bash
#!/bin/bash
# å®æ—¶ç›‘æ§çˆ¬å–è¿›åº¦

while true; do
    clear
    echo "=== çŸ¥ä¹çˆ¬è™«ç›‘æ§é¢æ¿ ==="
    echo "æ›´æ–°æ—¶é—´: $(date)"
    echo ""
    
    # æ€»ä½“ç»Ÿè®¡
    psql -U postgres -d zhihu_crawl -t -c "
    SELECT 
        'æ€»é—®é¢˜æ•°: ' || COUNT(*) || ' ä¸ª' as total,
        'å·²å®Œæˆ: ' || SUM(CASE WHEN crawl_status = 'completed' THEN 1 ELSE 0 END) || ' ä¸ª' as completed,
        'è¿›è¡Œä¸­: ' || SUM(CASE WHEN crawl_status = 'in_progress' THEN 1 ELSE 0 END) || ' ä¸ª' as in_progress,
        'å¾…å¤„ç†: ' || SUM(CASE WHEN crawl_status = 'pending' THEN 1 ELSE 0 END) || ' ä¸ª' as pending
    FROM questions;
    "
    
    echo ""
    echo "=== å›ç­”é‡‡é›†ç»Ÿè®¡ ==="
    psql -U postgres -d zhihu_crawl -t -c "
    SELECT 
        'ç›®æ ‡å›ç­”æ€»æ•°: ' || SUM(answer_count) || ' ä¸ª' as target_total,
        'å·²é‡‡é›†å›ç­”: ' || SUM(crawled_count) || ' ä¸ª' as crawled_total,
        'å®Œæˆåº¦: ' || ROUND(SUM(crawled_count)::float / SUM(answer_count) * 100, 2) || '%' as progress
    FROM questions;
    "
    
    echo ""
    echo "=== æœ€è¿‘é‡‡é›†çš„å›ç­” ==="
    psql -U postgres -d zhihu_crawl -c "
    SELECT 
        question_id,
        author,
        vote_count,
        crawl_time
    FROM answers 
    ORDER BY crawl_time DESC 
    LIMIT 5;
    "
    
    sleep 10
done
```

**æ•°æ®å¯¼å‡ºè„šæœ¬** (`export_data.sh`):
```bash
#!/bin/bash
# ä¸€é”®å¯¼å‡ºæ‰€æœ‰æ•°æ®

EXPORT_DIR="exports_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$EXPORT_DIR"

echo "=== å¼€å§‹å¯¼å‡ºæ•°æ®åˆ° $EXPORT_DIR ==="

# å¯¼å‡ºé—®é¢˜åˆ—è¡¨
psql -U postgres -d zhihu_crawl -c "
\COPY (SELECT * FROM questions) TO '$EXPORT_DIR/questions.csv' DELIMITER ',' CSV HEADER;"
echo "âœ“ é—®é¢˜åˆ—è¡¨å·²å¯¼å‡º"

# å¯¼å‡ºæ‰€æœ‰å›ç­”
psql -U postgres -d zhihu_crawl -c "
\COPY (SELECT * FROM answers) TO '$EXPORT_DIR/answers.csv' DELIMITER ',' CSV HEADER;"
echo "âœ“ å›ç­”æ•°æ®å·²å¯¼å‡º"

# å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯
psql -U postgres -d zhihu_crawl -c "
\COPY (
    SELECT 
        question_id,
        COUNT(*) as answer_count,
        AVG(vote_count) as avg_votes,
        MAX(vote_count) as max_votes,
        MIN(create_time) as earliest_answer,
        MAX(create_time) as latest_answer
    FROM answers 
    GROUP BY question_id
) TO '$EXPORT_DIR/statistics.csv' DELIMITER ',' CSV HEADER;"
echo "âœ“ ç»Ÿè®¡ä¿¡æ¯å·²å¯¼å‡º"

# æ•°æ®åº“å¤‡ä»½
pg_dump -U postgres zhihu_crawl > "$EXPORT_DIR/database_backup.sql"
echo "âœ“ æ•°æ®åº“å¤‡ä»½å·²åˆ›å»º"

echo "=== å¯¼å‡ºå®Œæˆï¼æ–‡ä»¶ä¿å­˜åœ¨ $EXPORT_DIR ç›®å½• ==="
ls -la "$EXPORT_DIR"
```

ä½¿ç”¨è¿™äº›è„šæœ¬:
```bash
# ç»™è„šæœ¬æ·»åŠ æ‰§è¡Œæƒé™
chmod +x init.sh add_questions.sh monitor.sh export_data.sh

# è¿è¡Œåˆå§‹åŒ–
./init.sh

# æ‰¹é‡æ·»åŠ é—®é¢˜
./add_questions.sh my_questions.txt

# å¯åŠ¨ç›‘æ§
./monitor.sh

# å¯¼å‡ºæ•°æ®
./export_data.sh
```

## é¡¹ç›®ç»“æ„

```
zhihu/
â”œâ”€â”€ main.py              # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ zhihu_crawler.py     # çˆ¬è™«æ ¸å¿ƒæ¨¡å—
â”œâ”€â”€ database.py          # æ•°æ®åº“æ“ä½œæ¨¡å—
â”œâ”€â”€ config.py            # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt     # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ README.md           # è¯´æ˜æ–‡æ¡£
â””â”€â”€ zhihu_crawler.log   # æ—¥å¿—æ–‡ä»¶ï¼ˆè¿è¡Œæ—¶ç”Ÿæˆï¼‰
```

## æ•°æ®è¡¨ç»“æ„

### questions è¡¨

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| id | SERIAL | ä¸»é”® |
| url | TEXT | é—®é¢˜ URL |
| answer_count | INTEGER | ç›®æ ‡å›ç­”æ•° |
| crawl_status | VARCHAR(20) | çˆ¬å–çŠ¶æ€ |
| crawled_count | INTEGER | å·²çˆ¬å–æ•°é‡ |
| created_at | TIMESTAMP | åˆ›å»ºæ—¶é—´ |

### answers è¡¨ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| id | SERIAL | ä¸»é”® |
| question_url | TEXT | é—®é¢˜ URL |
| answer_id | TEXT | å›ç­” ID |
| author | TEXT | ä½œè€… |
| content | TEXT | å›ç­”å†…å®¹ |
| vote_count | INTEGER | ç‚¹èµæ•° |
| created_time | TIMESTAMP | å›ç­”æ—¶é—´ |
| crawl_time | TIMESTAMP | çˆ¬å–æ—¶é—´ |

## ååçˆ¬æœºåˆ¶

æœ¬çˆ¬è™«é‡‡ç”¨å¤šç§æŠ€æœ¯è§„é¿çŸ¥ä¹çš„åçˆ¬æ£€æµ‹ï¼š

1. **æµè§ˆå™¨ä¼ªè£…**: ä½¿ç”¨çœŸå®çš„ Chrome æµè§ˆå™¨å’Œç”¨æˆ·ä»£ç†
2. **è¡Œä¸ºæ¨¡æ‹Ÿ**: éšæœºå»¶æ—¶ã€æ»šåŠ¨åŠ è½½ã€äººå·¥ç™»å½•
3. **DOM æ¸…ç†**: å®šæœŸæ¸…ç†é¡µé¢å…ƒç´ ï¼Œå‡å°‘å†…å­˜å ç”¨
4. **è¯·æ±‚æ§åˆ¶**: åˆç†çš„è¯·æ±‚é¢‘ç‡å’Œé‡è¯•æœºåˆ¶

## æ³¨æ„äº‹é¡¹

âš ï¸ **é‡è¦æé†’**

1. **éµå®ˆæ³•å¾‹æ³•è§„**: è¯·ç¡®ä¿çˆ¬å–è¡Œä¸ºç¬¦åˆç›¸å…³æ³•å¾‹æ³•è§„
2. **å°Šé‡ç½‘ç«™è§„åˆ™**: éµå®ˆçŸ¥ä¹çš„ robots.txt å’ŒæœåŠ¡æ¡æ¬¾
3. **åˆç†ä½¿ç”¨**: æ§åˆ¶çˆ¬å–é¢‘ç‡ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›
4. **æ•°æ®å®‰å…¨**: å¦¥å–„ä¿ç®¡çˆ¬å–çš„æ•°æ®ï¼Œä¸è¦æ³„éœ²ç”¨æˆ·éšç§

## å¸¸è§é—®é¢˜

### Q: ç™»å½•æ—¶é‡åˆ°éªŒè¯ç æ€ä¹ˆåŠï¼Ÿ
A: æ‰‹åŠ¨å®ŒæˆéªŒè¯ç éªŒè¯ï¼Œç¨‹åºä¼šç­‰å¾…æ‚¨å®Œæˆæ‰€æœ‰ç™»å½•æ­¥éª¤ã€‚

### Q: çˆ¬å–è¿‡ç¨‹ä¸­ç¨‹åºå´©æºƒæ€ä¹ˆåŠï¼Ÿ
A: é‡æ–°å¯åŠ¨ç¨‹åºï¼Œä¼šè‡ªåŠ¨ä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­çˆ¬å–ã€‚

### Q: å¦‚ä½•è°ƒæ•´çˆ¬å–é€Ÿåº¦ï¼Ÿ
A: ä¿®æ”¹ `config.py` ä¸­çš„å»¶æ—¶å‚æ•°ï¼Œå¢å¤§å»¶æ—¶å¯ä»¥é™ä½è¢«æ£€æµ‹çš„é£é™©ã€‚

### Q: æ•°æ®åº“è¿æ¥å¤±è´¥æ€ä¹ˆåŠï¼Ÿ
A: æ£€æŸ¥æ•°æ®åº“é…ç½®å’Œç½‘ç»œè¿æ¥ï¼Œç¡®ä¿ PostgreSQL æœåŠ¡æ­£å¸¸è¿è¡Œã€‚

## æ•…éšœæ’é™¤

### 1. Chrome æµè§ˆå™¨é—®é¢˜

```bash
# æ£€æŸ¥ Chrome ç‰ˆæœ¬
google-chrome --version

# æ‰‹åŠ¨æ›´æ–° ChromeDriver
pip3 install --upgrade webdriver-manager
```

### 2. æ•°æ®åº“è¿æ¥é—®é¢˜

```bash
# æµ‹è¯•æ•°æ®åº“è¿æ¥
psql -h localhost -U postgres -d zhihu_crawl
```

### 3. ä¾èµ–é—®é¢˜

```bash
# é‡æ–°å®‰è£…ä¾èµ–
pip3 install --upgrade -r requirements.txt
```

## æ—¥å¿—æ–‡ä»¶

ç¨‹åºè¿è¡Œæ—¶ä¼šç”Ÿæˆ `zhihu_crawler.log` æ—¥å¿—æ–‡ä»¶ï¼ŒåŒ…å«è¯¦ç»†çš„è¿è¡Œä¿¡æ¯ï¼š

- æ•°æ®åº“è¿æ¥çŠ¶æ€
- çˆ¬å–è¿›åº¦å’Œç»“æœ
- é”™è¯¯ä¿¡æ¯å’Œå¼‚å¸¸
- æ€§èƒ½ç»Ÿè®¡

## è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºå•†ä¸šç”¨é€”ã€‚ä½¿ç”¨æ—¶è¯·éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„å’Œç½‘ç«™æœåŠ¡æ¡æ¬¾ã€‚

## æ›´æ–°æ—¥å¿—

- v1.0.0: åˆå§‹ç‰ˆæœ¬ï¼Œæ”¯æŒåŸºæœ¬çš„é—®ç­”çˆ¬å–åŠŸèƒ½
- æ”¯æŒç”¨æˆ·ç™»å½•å’Œååçˆ¬æœºåˆ¶
- æ”¯æŒ DOM æ¸…ç†å’Œå†…å­˜ä¼˜åŒ–
- æ”¯æŒæ–­ç‚¹ç»­çˆ¬å’Œè¿›åº¦è·Ÿè¸ª