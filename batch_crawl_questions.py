#!/usr/bin/env python3
"""
æ‰¹é‡é‡‡é›†æ•°æ®åº“ä¸­æœªå¤„ç†é—®é¢˜çš„ç­”æ¡ˆæ•°æ®
åŸºäºæˆåŠŸçš„crawl_specific_question.pyæ–¹æ³•
"""

import os
import json
import time
import hashlib
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from loguru import logger

from zhihu_api_crawler import ZhihuAPIAnswerCrawler
from postgres_models import PostgreSQLManager, TaskInfo, Question, Answer


class BatchQuestionCrawler:
    """æ‰¹é‡é‡‡é›†æ•°æ®åº“ä¸­é—®é¢˜çš„çˆ¬è™«ç±»"""

    def __init__(self):
        self.api_crawler = ZhihuAPIAnswerCrawler()
        self.db_manager = PostgreSQLManager()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("output") / "batch_crawl"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("æ‰¹é‡é—®é¢˜çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")

    def get_unprocessed_questions(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æœªå¤„ç†çš„é—®é¢˜"""
        try:
            with self.db_manager.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT question_id, title, url, answer_count, task_id
                    FROM questions 
                    WHERE processed = FALSE
                    ORDER BY answer_count DESC
                """)
                
                questions = []
                for row in cursor.fetchall():
                    questions.append({
                        'question_id': row[0],
                        'title': row[1] or 'æœªçŸ¥æ ‡é¢˜',
                        'url': row[2],
                        'answer_count': row[3],
                        'task_id': row[4]
                    })
                
                return questions
                
        except Exception as e:
            logger.error(f"è·å–æœªå¤„ç†é—®é¢˜å¤±è´¥: {e}")
            return []

    def extract_question_id_from_url(self, question_url: str) -> Optional[str]:
        """ä»é—®é¢˜URLä¸­æå–é—®é¢˜ID"""
        return self.api_crawler.extract_question_id_from_url(question_url)

    def _save_api_response(self, response_data: Dict, question_id: str, page_num: int,
                         cursor: str = None, offset: int = 0) -> str:
        """ä¿å­˜APIå“åº”æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            # åˆ›å»ºé—®é¢˜ä¸“ç”¨ç›®å½•
            question_dir = self.output_dir / f"question_{question_id}"
            question_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            if cursor:
                filename = f"api_response_page_{page_num}_cursor_{cursor[:10]}_{timestamp}.json"
            else:
                filename = f"api_response_page_{page_num}_offset_{offset}_{timestamp}.json"

            filepath = question_dir / filename

            # æ·»åŠ å…ƒæ•°æ®
            response_with_meta = {
                "metadata": {
                    "question_id": question_id,
                    "page_num": page_num,
                    "cursor": cursor,
                    "offset": offset,
                    "timestamp": datetime.now().isoformat(),
                    "response_hash": hashlib.md5(json.dumps(response_data, sort_keys=True).encode()).hexdigest()
                },
                "response": response_data
            }

            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(response_with_meta, f, ensure_ascii=False, indent=2)

            logger.info(f"âœ… APIå“åº”å·²ä¿å­˜: {filepath}")
            return str(filepath)

        except Exception as e:
            logger.error(f"ä¿å­˜APIå“åº”å¤±è´¥: {e}")
            return ""

    def _parse_answers_from_api(self, answers_data: List[Dict], question_id: str, task_id: str) -> List[Answer]:
        """ä»APIå“åº”ä¸­è§£æç­”æ¡ˆ"""
        answers = []
        for answer_data in answers_data:
            answer = self.api_crawler.parse_answer_data(answer_data, question_id, task_id)
            if answer:
                answers.append(answer)
        return answers

    def crawl_question_answers(self, question_info: Dict, max_answers: int = None) -> Dict:
        """çˆ¬å–å•ä¸ªé—®é¢˜çš„æ‰€æœ‰ç­”æ¡ˆå¹¶ä¿å­˜æ•°æ®"""
        question_id = question_info['question_id']
        question_url = question_info['url']
        task_id = question_info['task_id']
        expected_answers = question_info['answer_count']
        
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–é—®é¢˜ {question_id} çš„æ‰€æœ‰ç­”æ¡ˆ")
        logger.info(f"é—®é¢˜æ ‡é¢˜: {question_info['title']}")
        logger.info(f"é¢„æœŸç­”æ¡ˆæ•°é‡: {expected_answers}")
        logger.info(f"ç›®æ ‡ç­”æ¡ˆæ•°é‡: {max_answers or 'å…¨éƒ¨'}")

        start_time = time.time()
        all_answers = []
        saved_files = []
        cursor = None
        offset = 0
        limit = 20
        page_count = 0
        total_api_calls = 0

        try:
            while True:
                page_count += 1
                total_api_calls += 1

                logger.info(f"ğŸ“„ è·å–ç¬¬ {page_count} é¡µç­”æ¡ˆ (cursor={cursor}, offset={offset})")

                # è·å–æ•°æ®å¹¶ä¿å­˜å“åº”
                response_data = self.api_crawler.fetch_answers_page(
                    question_id, cursor, offset, limit,
                    save_response_callback=lambda data, page, c, o: self._save_api_response(data, question_id, page, c, o),
                    page_num=page_count
                )
                
                if response_data:
                    saved_files.append(f"page_{page_count}_saved")

                if not response_data:
                    logger.error(f"è·å–ç¬¬ {page_count} é¡µæ•°æ®å¤±è´¥")
                    break

                # è§£æåˆ†é¡µä¿¡æ¯
                paging = response_data.get('paging', {})
                is_end = paging.get('is_end', True)
                next_url = paging.get('next', '')

                # è·å–ç­”æ¡ˆæ•°æ®
                answers_data = response_data.get('data', [])
                logger.info(f"ğŸ“¦ ç¬¬ {page_count} é¡µè·å–åˆ° {len(answers_data)} ä¸ªç­”æ¡ˆ")

                # è§£æç­”æ¡ˆæ•°æ®
                page_answers = self._parse_answers_from_api(answers_data, question_id, task_id)
                logger.info(f"ğŸ“ æœ¬é¡µè§£æå‡º {len(page_answers)} ä¸ªæœ‰æ•ˆç­”æ¡ˆ")

                # æ·»åŠ åˆ°æ€»ç­”æ¡ˆåˆ—è¡¨
                all_answers.extend(page_answers)

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§ç­”æ¡ˆæ•°é™åˆ¶
                if max_answers and len(all_answers) >= max_answers:
                    logger.info(f"âœ… å·²è¾¾åˆ°æœ€å¤§ç­”æ¡ˆæ•°é™åˆ¶: {max_answers}")
                    break

                # æ£€æŸ¥æ˜¯å¦å·²ç»åˆ°æœ€åä¸€é¡µ
                if is_end:
                    logger.info(f"ğŸ¯ å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break

                # è§£æä¸‹ä¸€é¡µå‚æ•°
                if next_url:
                    next_params = self.api_crawler._parse_next_url_params(next_url)
                    if 'cursor' in next_params:
                        cursor = next_params['cursor']
                        logger.info(f"ğŸ”„ æ›´æ–°cursor: {cursor}")
                    elif 'offset' in next_params:
                        offset = int(next_params['offset'])
                        cursor = None
                        logger.info(f"ğŸ”„ æ›´æ–°offset: {offset}")
                    else:
                        offset += limit
                        cursor = None
                        logger.info(f"ğŸ”„ é€’å¢offset: {offset}")
                else:
                    offset += limit
                    cursor = None
                    logger.info(f"ğŸ”„ é€’å¢offset: {offset}")

                # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(3)

                # å®‰å…¨æ£€æŸ¥ï¼šé¿å…æ— é™å¾ªç¯
                if page_count > 500:  # æœ€å¤š500é¡µ
                    logger.warning(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ï¼Œåœæ­¢çˆ¬å–")
                    break

            # ä¿å­˜ç­”æ¡ˆåˆ°æ•°æ®åº“
            saved_to_db = False
            if all_answers:
                saved_to_db = self._save_answers_to_db(all_answers)

            # æ ‡è®°é—®é¢˜ä¸ºå·²å¤„ç†
            if all_answers:
                self._mark_question_processed(question_id, task_id)

            end_time = time.time()
            duration = end_time - start_time

            result = {
                'question_id': question_id,
                'question_url': question_url,
                'task_id': task_id,
                'title': question_info['title'],
                'expected_answers': expected_answers,
                'total_answers': len(all_answers),
                'total_pages': page_count,
                'total_api_calls': total_api_calls,
                'saved_files_count': len(saved_files),
                'saved_files': saved_files,
                'saved_to_db': saved_to_db,
                'duration_seconds': round(duration, 2),
                'crawl_time': datetime.now().isoformat(),
                'completion_rate': round(len(all_answers) / expected_answers * 100, 2) if expected_answers > 0 else 0
            }

            logger.info(f"ğŸ‰ é—®é¢˜ {question_id} ç­”æ¡ˆçˆ¬å–å®Œæˆ")
            logger.info(f"ğŸ“Š æ€»å…±è·å–åˆ° {len(all_answers)} ä¸ªç­”æ¡ˆ")
            logger.info(f"ğŸ“„ å…±è¯·æ±‚äº† {page_count} é¡µæ•°æ®")
            logger.info(f"ğŸ’¾ ä¿å­˜äº† {len(saved_files)} ä¸ªAPIå“åº”æ–‡ä»¶")
            logger.info(f"â±ï¸ è€—æ—¶ {duration:.2f} ç§’")
            logger.info(f"ğŸ“ˆ å®Œæˆç‡: {result['completion_rate']}%")

            return result

        except Exception as e:
            logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {
                'question_id': question_id,
                'error': str(e),
                'total_answers': len(all_answers),
                'saved_files_count': len(saved_files),
                'saved_files': saved_files
            }

    def _save_answers_to_db(self, answers: List[Answer]) -> bool:
        """ä¿å­˜ç­”æ¡ˆæ•°æ®åˆ°æ•°æ®åº“"""
        try:
            saved_count = 0
            for answer in answers:
                # ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡
                content_hash = hashlib.md5(answer.content.encode('utf-8')).hexdigest()
                answer.content_hash = content_hash

                if self.db_manager.save_answer(answer):
                    saved_count += 1
                else:
                    logger.warning(f"ä¿å­˜ç­”æ¡ˆå¤±è´¥: {answer.answer_id}")

            logger.info(f"æˆåŠŸä¿å­˜ {saved_count}/{len(answers)} ä¸ªç­”æ¡ˆåˆ°æ•°æ®åº“")
            return saved_count == len(answers)

        except Exception as e:
            logger.error(f"ä¿å­˜ç­”æ¡ˆåˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return False

    def _mark_question_processed(self, question_id: str, task_id: str):
        """æ ‡è®°é—®é¢˜ä¸ºå·²å¤„ç†"""
        try:
            with self.db_manager.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE questions 
                    SET processed = TRUE, updated_at = CURRENT_TIMESTAMP
                    WHERE question_id = %s AND task_id = %s
                """, (question_id, task_id))
                conn.commit()
                logger.info(f"âœ… é—®é¢˜ {question_id} æ ‡è®°ä¸ºå·²å¤„ç†")
                
        except Exception as e:
            logger.error(f"æ ‡è®°é—®é¢˜å¤„ç†çŠ¶æ€å¤±è´¥: {e}")

    def crawl_all_unprocessed_questions(self, max_answers_per_question: int = None) -> Dict:
        """æ‰¹é‡çˆ¬å–æ‰€æœ‰æœªå¤„ç†çš„é—®é¢˜"""
        logger.info("ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å–æ‰€æœ‰æœªå¤„ç†çš„é—®é¢˜")
        
        # è·å–æœªå¤„ç†çš„é—®é¢˜
        questions = self.get_unprocessed_questions()
        if not questions:
            logger.info("æ²¡æœ‰æœªå¤„ç†çš„é—®é¢˜")
            return {'total_questions': 0, 'results': []}

        logger.info(f"æ‰¾åˆ° {len(questions)} ä¸ªæœªå¤„ç†çš„é—®é¢˜")
        
        # æ’é™¤å·²ç»å¤„ç†è¿‡çš„é—®é¢˜378706911
        questions = [q for q in questions if q['question_id'] != '378706911']
        logger.info(f"æ’é™¤å·²å¤„ç†é—®é¢˜åï¼Œå‰©ä½™ {len(questions)} ä¸ªé—®é¢˜")

        start_time = time.time()
        results = []
        total_answers = 0
        total_files = 0

        for i, question_info in enumerate(questions, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"å¤„ç†ç¬¬ {i}/{len(questions)} ä¸ªé—®é¢˜")
            logger.info(f"é—®é¢˜ID: {question_info['question_id']}")
            logger.info(f"é¢„æœŸç­”æ¡ˆæ•°: {question_info['answer_count']}")
            logger.info(f"{'='*60}")

            # çˆ¬å–é—®é¢˜ç­”æ¡ˆ
            result = self.crawl_question_answers(question_info, max_answers_per_question)
            results.append(result)

            # ç»Ÿè®¡
            if 'total_answers' in result:
                total_answers += result['total_answers']
            if 'saved_files_count' in result:
                total_files += result['saved_files_count']

            # é—®é¢˜é—´å»¶æ—¶
            if i < len(questions):
                logger.info(f"â³ ç­‰å¾…30ç§’åå¤„ç†ä¸‹ä¸€ä¸ªé—®é¢˜...")
                time.sleep(30)

        end_time = time.time()
        total_duration = end_time - start_time

        summary = {
            'total_questions': len(questions),
            'total_answers_collected': total_answers,
            'total_files_saved': total_files,
            'total_duration_seconds': round(total_duration, 2),
            'average_time_per_question': round(total_duration / len(questions), 2) if questions else 0,
            'results': results,
            'crawl_time': datetime.now().isoformat()
        }

        # ä¿å­˜æ‰¹é‡çˆ¬å–æ‘˜è¦
        self._save_batch_summary(summary)

        logger.info(f"\nğŸ‰ æ‰¹é‡çˆ¬å–ä»»åŠ¡å®Œæˆ!")
        logger.info(f"ğŸ“Š å¤„ç†é—®é¢˜æ•°é‡: {len(questions)}")
        logger.info(f"ğŸ“Š æ€»å…±é‡‡é›†ç­”æ¡ˆ: {total_answers}")
        logger.info(f"ğŸ“Š ä¿å­˜æ–‡ä»¶æ•°é‡: {total_files}")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_duration:.2f} ç§’")

        return summary

    def _save_batch_summary(self, summary: Dict) -> str:
        """ä¿å­˜æ‰¹é‡çˆ¬å–æ‘˜è¦"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            summary_file = self.output_dir / f"batch_crawl_summary_{timestamp}.json"

            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2, default=str)

            logger.info(f"âœ… æ‰¹é‡çˆ¬å–æ‘˜è¦å·²ä¿å­˜: {summary_file}")
            return str(summary_file)

        except Exception as e:
            logger.error(f"ä¿å­˜æ‰¹é‡çˆ¬å–æ‘˜è¦å¤±è´¥: {e}")
            return ""


def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆå§‹åŒ–æ‰¹é‡çˆ¬è™«
        crawler = BatchQuestionCrawler()

        # å¼€å§‹æ‰¹é‡çˆ¬å–
        logger.info("å¼€å§‹æ‰§è¡Œæ‰¹é‡é—®é¢˜ç­”æ¡ˆé‡‡é›†ä»»åŠ¡...")
        summary = crawler.crawl_all_unprocessed_questions()

        # è¾“å‡ºæœ€ç»ˆç»“æœ
        print("\n" + "="*80)
        print("æ‰¹é‡é—®é¢˜ç­”æ¡ˆé‡‡é›†ä»»åŠ¡å®Œæˆï¼")
        print("="*80)
        print(f"å¤„ç†é—®é¢˜æ•°é‡: {summary['total_questions']}")
        print(f"æ€»å…±é‡‡é›†ç­”æ¡ˆ: {summary['total_answers_collected']}")
        print(f"ä¿å­˜æ–‡ä»¶æ•°é‡: {summary['total_files_saved']}")
        print(f"æ€»è€—æ—¶: {summary['total_duration_seconds']} ç§’")
        print(f"å¹³å‡æ¯ä¸ªé—®é¢˜è€—æ—¶: {summary['average_time_per_question']} ç§’")
        print(f"è¾“å‡ºç›®å½•: {crawler.output_dir}")
        print("="*80)

        # æ˜¾ç¤ºå„é—®é¢˜è¯¦ç»†ç»“æœ
        if summary['results']:
            print("\nå„é—®é¢˜å¤„ç†ç»“æœ:")
            for i, result in enumerate(summary['results'], 1):
                status = "æˆåŠŸ" if result.get('saved_to_db', False) else "å¤±è´¥"
                completion = result.get('completion_rate', 0)
                print(f"{i}. é—®é¢˜{result['question_id']}: {result.get('total_answers', 0)}ä¸ªç­”æ¡ˆ, "
                      f"å®Œæˆç‡{completion}%, çŠ¶æ€: {status}")

        return summary

    except Exception as e:
        logger.error(f"æ‰§è¡Œæ‰¹é‡é‡‡é›†ä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None


if __name__ == "__main__":
    main()
