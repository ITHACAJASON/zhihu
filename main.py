#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½çŸ¥ä¹çˆ¬è™«ç³»ç»Ÿä¸»å…¥å£

æ•´åˆåŠ¨æ€å‚æ•°è·å–+APIæ‰¹é‡è¯·æ±‚çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ
"""

import asyncio
import click
import json
import time
from pathlib import Path
from loguru import logger
import sys
from typing import List, Optional

from smart_crawler import SmartCrawler
from monitor_recovery import MonitorRecovery
from params_pool_manager import ParamsPoolManager
from dynamic_params_extractor import DynamicParamsExtractor


# é…ç½®æ—¥å¿—
logger.remove()
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan> - <level>{message}</level>",
    level="INFO"
)


@click.group()
@click.option('--debug', is_flag=True, help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
def cli(debug):
    """æ™ºèƒ½çŸ¥ä¹çˆ¬è™«ç³»ç»Ÿ - åŠ¨æ€å‚æ•°è·å–+APIæ‰¹é‡è¯·æ±‚"""
    if debug:
        logger.remove()
        logger.add(
            sys.stdout,
            format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
            level="DEBUG"
        )


@cli.command()
@click.argument('question_ids', nargs=-1, required=True)
@click.option('--limit', default=20, help='æ¯é¡µå›ç­”æ•°é‡')
@click.option('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
@click.option('--concurrent', default=5, help='æœ€å¤§å¹¶å‘æ•°')
@click.option('--db-path', default='params_pool.db', help='å‚æ•°æ± æ•°æ®åº“è·¯å¾„')
@click.option('--user-data-dir', help='Chromeç”¨æˆ·æ•°æ®ç›®å½•')
@click.option('--monitor', is_flag=True, help='å¯ç”¨ç›‘æ§')
def crawl(question_ids: tuple, limit: int, output: Optional[str], 
          concurrent: int, db_path: str, user_data_dir: Optional[str], monitor: bool):
    """çˆ¬å–æŒ‡å®šé—®é¢˜çš„å›ç­”æ•°æ®
    
    QUESTION_IDS: ä¸€ä¸ªæˆ–å¤šä¸ªé—®é¢˜ID
    
    ç¤ºä¾‹:
        python main.py crawl 19550225 20831813 --limit 10 --output results.json
    """
    asyncio.run(_crawl_async(
        list(question_ids), limit, output, concurrent, 
        db_path, user_data_dir, monitor
    ))


async def _crawl_async(question_ids: List[str], limit: int, output: Optional[str],
                      concurrent: int, db_path: str, user_data_dir: Optional[str], 
                      monitor: bool):
    """å¼‚æ­¥çˆ¬å–å‡½æ•°"""
    logger.info(f"ğŸš€ å¼€å§‹çˆ¬å– {len(question_ids)} ä¸ªé—®é¢˜")
    
    # åˆ›å»ºæ™ºèƒ½çˆ¬è™«
    async with SmartCrawler(
        params_db_path=db_path,
        max_concurrent=concurrent,
        user_data_dir=user_data_dir
    ) as crawler:
        
        # å¯åŠ¨ç›‘æ§ï¼ˆå¦‚æœéœ€è¦ï¼‰
        monitor_system = None
        if monitor:
            params_manager = ParamsPoolManager(db_path)
            monitor_system = MonitorRecovery(params_manager)
            monitor_system.start_monitoring()
            logger.info("ğŸ“Š ç›‘æ§ç³»ç»Ÿå·²å¯åŠ¨")
            
        try:
            # è¿›åº¦å›è°ƒ
            def progress_callback(current, total, result):
                status = "âœ…" if result.success else "âŒ"
                logger.info(f"ğŸ“‹ è¿›åº¦: {current}/{total} {status} {result.question_id} ({result.response_time:.2f}s)")
                
            # æ‰¹é‡çˆ¬å–
            results = await crawler.batch_crawl(
                question_ids,
                limit=limit,
                progress_callback=progress_callback
            )
            
            # ç»Ÿè®¡ç»“æœ
            successful_count = sum(1 for r in results if r.success)
            total_count = len(results)
            
            logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆ: {successful_count}/{total_count} æˆåŠŸ")
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            stats = crawler.get_stats()
            logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {json.dumps(stats, indent=2, ensure_ascii=False)}")
            
            # ä¿å­˜ç»“æœ
            if output:
                await _save_results(results, output)
                logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output}")
            else:
                # è¾“å‡ºåˆ°æ§åˆ¶å°
                _print_results(results)
                
        finally:
            # åœæ­¢ç›‘æ§
            if monitor_system:
                monitor_system.stop_monitoring()
                logger.info("ğŸ“Š ç›‘æ§ç³»ç»Ÿå·²åœæ­¢")


async def _save_results(results, output_path: str):
    """ä¿å­˜çˆ¬å–ç»“æœ"""
    output_data = {
        'timestamp': time.time(),
        'total_questions': len(results),
        'successful_questions': sum(1 for r in results if r.success),
        'results': [
            {
                'question_id': r.question_id,
                'success': r.success,
                'error': r.error,
                'response_time': r.response_time,
                'data': r.data
            }
            for r in results
        ]
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)


def _print_results(results):
    """æ‰“å°ç»“æœåˆ°æ§åˆ¶å°"""
    for result in results:
        if result.success and result.data:
            logger.info(f"\nğŸ“„ é—®é¢˜ {result.question_id}:")
            data = result.data.get('data', [])
            logger.info(f"  å›ç­”æ•°é‡: {len(data)}")
            
            for i, answer in enumerate(data[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªå›ç­”
                author = answer.get('author', {}).get('name', 'æœªçŸ¥ç”¨æˆ·')
                voteup_count = answer.get('voteup_count', 0)
                content = answer.get('content', '')[:100] + '...' if len(answer.get('content', '')) > 100 else answer.get('content', '')
                
                logger.info(f"  {i}. {author} (ğŸ‘{voteup_count}): {content}")
        else:
            logger.error(f"âŒ é—®é¢˜ {result.question_id}: {result.error}")


@cli.command()
@click.argument('question_ids', nargs=-1, required=True)
@click.option('--db-path', default='params_pool.db', help='å‚æ•°æ± æ•°æ®åº“è·¯å¾„')
@click.option('--user-data-dir', help='Chromeç”¨æˆ·æ•°æ®ç›®å½•')
@click.option('--headless/--no-headless', default=True, help='æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼')
def extract_params(question_ids: tuple, db_path: str, user_data_dir: Optional[str], headless: bool):
    """æå–åçˆ¬è™«å‚æ•°åˆ°å‚æ•°æ± 
    
    QUESTION_IDS: ä¸€ä¸ªæˆ–å¤šä¸ªé—®é¢˜ID
    
    ç¤ºä¾‹:
        python main.py extract-params 19550225 20831813 --no-headless
    """
    logger.info(f"ğŸ” å¼€å§‹æå– {len(question_ids)} ä¸ªé—®é¢˜çš„å‚æ•°")
    
    # åˆ›å»ºå‚æ•°æ± ç®¡ç†å™¨
    params_manager = ParamsPoolManager(db_path)
    
    # åˆ›å»ºå‚æ•°æå–å™¨
    with DynamicParamsExtractor(headless=headless, user_data_dir=user_data_dir) as extractor:
        
        successful_count = 0
        
        for i, question_id in enumerate(question_ids, 1):
            logger.info(f"ğŸ“‹ å¤„ç†ç¬¬ {i}/{len(question_ids)} ä¸ªé—®é¢˜: {question_id}")
            
            try:
                params = extractor.extract_params_from_question(question_id)
                
                if params and extractor.validate_params(params):
                    params['question_id'] = question_id
                    
                    if params_manager.add_params(params):
                        successful_count += 1
                        logger.info(f"âœ… å‚æ•°æå–æˆåŠŸ: {question_id}")
                    else:
                        logger.warning(f"âš ï¸ å‚æ•°æ·»åŠ å¤±è´¥: {question_id}")
                else:
                    logger.warning(f"âš ï¸ å‚æ•°æå–å¤±è´¥: {question_id}")
                    
            except Exception as e:
                logger.error(f"âŒ å¤„ç†é—®é¢˜ {question_id} æ—¶å‡ºé”™: {e}")
                
            # æ·»åŠ å»¶æ—¶
            if i < len(question_ids):
                time.sleep(2)
                
    logger.info(f"ğŸ‰ å‚æ•°æå–å®Œæˆ: {successful_count}/{len(question_ids)} æˆåŠŸ")
    
    # æ˜¾ç¤ºå‚æ•°æ± çŠ¶æ€
    stats = params_manager.get_pool_stats()
    logger.info(f"ğŸ“Š å‚æ•°æ± çŠ¶æ€: {json.dumps(stats, indent=2, ensure_ascii=False)}")


@cli.command()
@click.option('--db-path', default='params_pool.db', help='å‚æ•°æ± æ•°æ®åº“è·¯å¾„')
def pool_status(db_path: str):
    """æŸ¥çœ‹å‚æ•°æ± çŠ¶æ€"""
    params_manager = ParamsPoolManager(db_path)
    stats = params_manager.get_pool_stats()
    
    logger.info("ğŸ“Š å‚æ•°æ± çŠ¶æ€:")
    logger.info(f"  æ€»å‚æ•°æ•°: {stats['total_count']}")
    logger.info(f"  æ´»è·ƒå‚æ•°æ•°: {stats['active_count']}")
    logger.info(f"  æ–°é²œå‚æ•°æ•°: {stats['fresh_count']}")
    logger.info(f"  å¹³å‡æˆåŠŸç‡: {stats['avg_success_rate']:.2%}")
    logger.info(f"  æœ€æ—§å‚æ•°å¹´é¾„: {stats['oldest_age_minutes']:.1f} åˆ†é’Ÿ")
    logger.info(f"  æœ€æ–°å‚æ•°å¹´é¾„: {stats['newest_age_minutes']:.1f} åˆ†é’Ÿ")


@cli.command()
@click.option('--db-path', default='params_pool.db', help='å‚æ•°æ± æ•°æ®åº“è·¯å¾„')
@click.option('--interval', default=60, help='ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰')
@click.option('--duration', default=3600, help='ç›‘æ§æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰')
@click.option('--export', help='å¯¼å‡ºæŒ‡æ ‡æ–‡ä»¶è·¯å¾„')
def monitor(db_path: str, interval: int, duration: int, export: Optional[str]):
    """å¯åŠ¨ç›‘æ§ç³»ç»Ÿ"""
    logger.info(f"ğŸ“Š å¯åŠ¨ç›‘æ§ç³»ç»Ÿï¼Œé—´éš”: {interval}ç§’ï¼ŒæŒç»­: {duration}ç§’")
    
    params_manager = ParamsPoolManager(db_path)
    monitor_system = MonitorRecovery(
        params_manager=params_manager,
        monitor_interval=interval,
        recovery_enabled=True
    )
    
    try:
        monitor_system.start_monitoring()
        
        # è¿è¡ŒæŒ‡å®šæ—¶é—´
        time.sleep(duration)
        
    except KeyboardInterrupt:
        logger.info("âš ï¸ ç›‘æ§è¢«ç”¨æˆ·ä¸­æ–­")
    finally:
        monitor_system.stop_monitoring()
        
        # å¯¼å‡ºæŒ‡æ ‡
        if export:
            if monitor_system.export_metrics(export):
                logger.info(f"ğŸ“„ ç›‘æ§æŒ‡æ ‡å·²å¯¼å‡ºåˆ°: {export}")
            else:
                logger.error("âŒ å¯¼å‡ºç›‘æ§æŒ‡æ ‡å¤±è´¥")
                
        # æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Š
        health_report = monitor_system.get_health_report()
        logger.info(f"ğŸ“Š æœ€ç»ˆå¥åº·åº¦æŠ¥å‘Š: {json.dumps(health_report, indent=2, ensure_ascii=False)}")


@cli.command()
@click.option('--db-path', default='params_pool.db', help='å‚æ•°æ± æ•°æ®åº“è·¯å¾„')
def cleanup(db_path: str):
    """æ¸…ç†è¿‡æœŸå‚æ•°"""
    params_manager = ParamsPoolManager(db_path)
    
    # æ¸…ç†å‰çŠ¶æ€
    before_stats = params_manager.get_pool_stats()
    logger.info(f"æ¸…ç†å‰å‚æ•°æ•°: {before_stats['total_count']}")
    
    # æ‰§è¡Œæ¸…ç†
    cleaned_count = params_manager.cleanup_expired_params()
    
    # æ¸…ç†åçŠ¶æ€
    after_stats = params_manager.get_pool_stats()
    logger.info(f"æ¸…ç†åå‚æ•°æ•°: {after_stats['total_count']}")
    logger.info(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸå‚æ•°")


@cli.command()
def test():
    """è¿è¡Œç³»ç»Ÿæµ‹è¯•"""
    asyncio.run(_test_async())


async def _test_async():
    """å¼‚æ­¥æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸ§ª å¼€å§‹è¿è¡Œç³»ç»Ÿæµ‹è¯•...")
    
    try:
        # å¯¼å…¥æµ‹è¯•æ¨¡å—
        from test_smart_crawler import SmartCrawlerTester
        
        tester = SmartCrawlerTester()
        success = await tester.run_integration_test()
        
        if success:
            logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        else:
            logger.error("ğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
            sys.exit(1)
            
    except ImportError as e:
        logger.error(f"âŒ æ— æ³•å¯¼å…¥æµ‹è¯•æ¨¡å—: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        sys.exit(1)


@cli.command()
@click.option('--file', '-f', help='ä»æ–‡ä»¶è¯»å–é—®é¢˜IDåˆ—è¡¨')
@click.option('--limit', default=20, help='æ¯é¡µå›ç­”æ•°é‡')
@click.option('--output', '-o', required=True, help='è¾“å‡ºç›®å½•')
@click.option('--concurrent', default=5, help='æœ€å¤§å¹¶å‘æ•°')
@click.option('--db-path', default='params_pool.db', help='å‚æ•°æ± æ•°æ®åº“è·¯å¾„')
@click.option('--user-data-dir', help='Chromeç”¨æˆ·æ•°æ®ç›®å½•')
def batch(file: Optional[str], limit: int, output: str, 
          concurrent: int, db_path: str, user_data_dir: Optional[str]):
    """æ‰¹é‡çˆ¬å–æ¨¡å¼
    
    ä»æ–‡ä»¶è¯»å–é—®é¢˜IDåˆ—è¡¨è¿›è¡Œæ‰¹é‡çˆ¬å–
    
    ç¤ºä¾‹:
        python main.py batch -f questions.txt -o results/ --concurrent 10
    """
    if not file:
        logger.error("âŒ è¯·æŒ‡å®šé—®é¢˜IDæ–‡ä»¶ (--file)")
        sys.exit(1)
        
    # è¯»å–é—®é¢˜IDåˆ—è¡¨
    try:
        with open(file, 'r', encoding='utf-8') as f:
            question_ids = [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        sys.exit(1)
        
    if not question_ids:
        logger.error("âŒ æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„é—®é¢˜ID")
        sys.exit(1)
        
    logger.info(f"ğŸ“‹ ä»æ–‡ä»¶ {file} è¯»å–åˆ° {len(question_ids)} ä¸ªé—®é¢˜ID")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    timestamp = int(time.time())
    output_file = output_dir / f"batch_results_{timestamp}.json"
    
    # æ‰§è¡Œæ‰¹é‡çˆ¬å–
    asyncio.run(_crawl_async(
        question_ids, limit, str(output_file), concurrent, 
        db_path, user_data_dir, True  # å¯ç”¨ç›‘æ§
    ))


if __name__ == '__main__':
    cli()