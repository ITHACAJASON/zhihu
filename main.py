#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½çŸ¥ä¹çˆ¬è™«ç³»ç»Ÿä¸»å…¥å£

æ™ºèƒ½çŸ¥ä¹çˆ¬è™«ç³»ç»Ÿ
"""

import asyncio
import click
import json
import time
from pathlib import Path
from loguru import logger
import sys
from typing import List, Optional

from smart_crawler import SmartCrawler
from monitor_recovery import MonitorRecovery
from params_pool_manager import ParamsPoolManager
from dynamic_params_extractor import DynamicParamsExtractor


# é…ç½®æ—¥å¿—
logger.remove()
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan> - <level>{message}</level>",
    level="INFO"
)


@click.group()
@click.option('--debug', is_flag=True, help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
def cli(debug):
    """æ™ºèƒ½çŸ¥ä¹çˆ¬è™«ç³»ç»Ÿ"""
    if debug:
        logger.remove()
        logger.add(
            sys.stdout,
            format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
            level="DEBUG"
        )


@cli.command()
@click.argument('question_ids', nargs=-1, required=True)
@click.option('--limit', default=20, help='æ¯é¡µå›ç­”æ•°é‡')
@click.option('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
@click.option('--concurrent', default=5, help='æœ€å¤§å¹¶å‘æ•°')
@click.option('--db-path', default='params_pool.db', help='å‚æ•°æ± æ•°æ®åº“è·¯å¾„')
@click.option('--user-data-dir', help='Chromeç”¨æˆ·æ•°æ®ç›®å½•')
@click.option('--monitor', is_flag=True, help='å¯ç”¨ç›‘æ§')
def crawl(question_ids: tuple, limit: int, output: Optional[str], 
          concurrent: int, db_path: str, user_data_dir: Optional[str], monitor: bool):
    """çˆ¬å–æŒ‡å®šé—®é¢˜çš„å›ç­”æ•°æ®
    
    QUESTION_IDS: ä¸€ä¸ªæˆ–å¤šä¸ªé—®é¢˜ID
    
    ç¤ºä¾‹:
        python main.py crawl 19550225 20831813 --limit 10 --output results.json
    """
    asyncio.run(_crawl_async(
        list(question_ids), limit, output, concurrent, 
        db_path, user_data_dir, monitor
    ))


async def _crawl_async(question_ids: List[str], limit: int, output: Optional[str],
                      concurrent: int, db_path: str, user_data_dir: Optional[str], 
                      monitor: bool):
    """å¼‚æ­¥çˆ¬å–å‡½æ•°"""
    logger.info(f"ğŸš€ å¼€å§‹çˆ¬å– {len(question_ids)} ä¸ªé—®é¢˜")
    
    # åˆ›å»ºæ™ºèƒ½çˆ¬è™«ï¼ˆä»…Seleniumæ¨¡å¼ï¼‰
    async with SmartCrawler(
        max_concurrent=concurrent,
        user_data_dir=user_data_dir
    ) as crawler:
        
        # åœ¨ä»…Seleniumæ¨¡å¼ä¸‹ä¸å†ä½¿ç”¨ç›‘æ§ç³»ç»Ÿ
        monitor_system = None
        if monitor:
            logger.info("ğŸ“Š æ³¨æ„ï¼šåœ¨ä»…Seleniumæ¨¡å¼ä¸‹ä¸å¯ç”¨å‚æ•°æ± ç›‘æ§")
            
        try:
            # è¿›åº¦å›è°ƒ
            def progress_callback(current, total, result):
                status = "âœ…" if result.success else "âŒ"
                logger.info(f"ğŸ“‹ è¿›åº¦: {current}/{total} {status} {result.question_id} ({result.response_time:.2f}s)")
                
            # æ‰¹é‡çˆ¬å–
            results = await crawler.batch_crawl(
                question_ids,
                limit=limit,
                progress_callback=progress_callback
            )
            
            # ç»Ÿè®¡ç»“æœ
            successful_count = sum(1 for r in results if r.success)
            total_count = len(results)
            
            logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆ: {successful_count}/{total_count} æˆåŠŸ")
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            stats = crawler.get_stats()
            logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {json.dumps(stats, indent=2, ensure_ascii=False)}")
            
            # ä¿å­˜ç»“æœ
            if output:
                await _save_results(results, output)
                logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output}")
            else:
                # è¾“å‡ºåˆ°æ§åˆ¶å°
                _print_results(results)
                
        finally:
            # åœ¨ä»…Seleniumæ¨¡å¼ä¸‹ä¸éœ€è¦åœæ­¢ç›‘æ§ç³»ç»Ÿ
            pass


async def _save_results(results, output_path: str):
    """ä¿å­˜çˆ¬å–ç»“æœ"""
    output_data = {
        'timestamp': time.time(),
        'total_questions': len(results),
        'successful_questions': sum(1 for r in results if r.success),
        'results': [
            {
                'question_id': r.question_id,
                'success': r.success,
                'error': r.error,
                'response_time': r.response_time,
                'data': r.data
            }
            for r in results
        ]
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)


def _print_results(results):
    """æ‰“å°ç»“æœåˆ°æ§åˆ¶å°"""
    for result in results:
        if result.success and result.data:
            logger.info(f"\nğŸ“„ é—®é¢˜ {result.question_id}:")
            data = result.data.get('data', [])
            logger.info(f"  å›ç­”æ•°é‡: {len(data)}")
            
            for i, answer in enumerate(data[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªå›ç­”
                author = answer.get('author', {}).get('name', 'æœªçŸ¥ç”¨æˆ·')
                voteup_count = answer.get('voteup_count', 0)
                content = answer.get('content', '')[:100] + '...' if len(answer.get('content', '')) > 100 else answer.get('content', '')
                
                logger.info(f"  {i}. {author} (ğŸ‘{voteup_count}): {content}")
        else:
            logger.error(f"âŒ é—®é¢˜ {result.question_id}: {result.error}")


@cli.command()
@click.argument('question_ids', nargs=-1, required=True)
@click.option('--db-path', default='params_pool.db', help='å‚æ•°æ± æ•°æ®åº“è·¯å¾„')
@click.option('--user-data-dir', help='Chromeç”¨æˆ·æ•°æ®ç›®å½•')
@click.option('--headless/--no-headless', default=False, help='æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼')
def extract_params(question_ids: tuple, db_path: str, user_data_dir: Optional[str], headless: bool):
    """æå–åçˆ¬è™«å‚æ•°åˆ°å‚æ•°æ± 
    
    QUESTION_IDS: ä¸€ä¸ªæˆ–å¤šä¸ªé—®é¢˜ID
    
    ç¤ºä¾‹:
        python main.py extract-params 19550225 20831813 --no-headless
    """
    logger.info(f"ğŸ” å¼€å§‹æå– {len(question_ids)} ä¸ªé—®é¢˜çš„å‚æ•°")
    
    # åˆ›å»ºå‚æ•°æ± ç®¡ç†å™¨
    params_manager = ParamsPoolManager(db_path)
    
    # åˆ›å»ºå‚æ•°æå–å™¨
    with DynamicParamsExtractor(headless=headless, user_data_dir=user_data_dir) as extractor:
        
        successful_count = 0
        
        for i, question_id in enumerate(question_ids, 1):
            logger.info(f"ğŸ“‹ å¤„ç†ç¬¬ {i}/{len(question_ids)} ä¸ªé—®é¢˜: {question_id}")
            
            try:
                params = extractor.extract_params_from_question(question_id)
                
                if params and extractor.validate_params(params):
                    params['question_id'] = question_id
                    
                    if params_manager.add_params(params):
                        successful_count += 1
                        logger.info(f"âœ… å‚æ•°æå–æˆåŠŸ: {question_id}")
                    else:
                        logger.warning(f"âš ï¸ å‚æ•°æ·»åŠ å¤±è´¥: {question_id}")
                else:
                    logger.warning(f"âš ï¸ å‚æ•°æå–å¤±è´¥: {question_id}")
                    
            except Exception as e:
                logger.error(f"âŒ å¤„ç†é—®é¢˜ {question_id} æ—¶å‡ºé”™: {e}")
                
            # æ·»åŠ å»¶æ—¶
            if i < len(question_ids):
                time.sleep(2)
                
    logger.info(f"ğŸ‰ å‚æ•°æå–å®Œæˆ: {successful_count}/{len(question_ids)} æˆåŠŸ")
    
    # æ˜¾ç¤ºå‚æ•°æ± çŠ¶æ€
    stats = params_manager.get_pool_stats()
    logger.info(f"ğŸ“Š å‚æ•°æ± çŠ¶æ€: {json.dumps(stats, indent=2, ensure_ascii=False)}")


@cli.command()
@click.option('--db-path', default='params_pool.db', help='å‚æ•°æ± æ•°æ®åº“è·¯å¾„')
def pool_status(db_path: str):
    """æŸ¥çœ‹å‚æ•°æ± çŠ¶æ€"""
    params_manager = ParamsPoolManager(db_path)
    stats = params_manager.get_pool_stats()
    
    logger.info("ğŸ“Š å‚æ•°æ± çŠ¶æ€:")
    logger.info(f"  æ€»å‚æ•°æ•°: {stats['total_count']}")
    logger.info(f"  æ´»è·ƒå‚æ•°æ•°: {stats['active_count']}")
    logger.info(f"  æ–°é²œå‚æ•°æ•°: {stats['fresh_count']}")
    logger.info(f"  å¹³å‡æˆåŠŸç‡: {stats['avg_success_rate']:.2%}")
    logger.info(f"  æœ€æ—§å‚æ•°å¹´é¾„: {stats['oldest_age_minutes']:.1f} åˆ†é’Ÿ")
    logger.info(f"  æœ€æ–°å‚æ•°å¹´é¾„: {stats['newest_age_minutes']:.1f} åˆ†é’Ÿ")


@cli.command()
@click.option('--db-path', default='params_pool.db', help='å‚æ•°æ± æ•°æ®åº“è·¯å¾„')
@click.option('--interval', default=60, help='ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰')
@click.option('--duration', default=3600, help='ç›‘æ§æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰')
@click.option('--export', help='å¯¼å‡ºæŒ‡æ ‡æ–‡ä»¶è·¯å¾„')
def monitor(db_path: str, interval: int, duration: int, export: Optional[str]):
    """å¯åŠ¨ç›‘æ§ç³»ç»Ÿ"""
    logger.info(f"ğŸ“Š å¯åŠ¨ç›‘æ§ç³»ç»Ÿï¼Œé—´éš”: {interval}ç§’ï¼ŒæŒç»­: {duration}ç§’")
    
    params_manager = ParamsPoolManager(db_path)
    monitor_system = MonitorRecovery(
        params_manager=params_manager,
        monitor_interval=interval,
        recovery_enabled=True
    )
    
    try:
        monitor_system.start_monitoring()
        
        # è¿è¡ŒæŒ‡å®šæ—¶é—´
        time.sleep(duration)
        
    except KeyboardInterrupt:
        logger.info("âš ï¸ ç›‘æ§è¢«ç”¨æˆ·ä¸­æ–­")
    finally:
        monitor_system.stop_monitoring()
        
        # å¯¼å‡ºæŒ‡æ ‡
        if export:
            if monitor_system.export_metrics(export):
                logger.info(f"ğŸ“„ ç›‘æ§æŒ‡æ ‡å·²å¯¼å‡ºåˆ°: {export}")
            else:
                logger.error("âŒ å¯¼å‡ºç›‘æ§æŒ‡æ ‡å¤±è´¥")
                
        # æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Š
        health_report = monitor_system.get_health_report()
        logger.info(f"ğŸ“Š æœ€ç»ˆå¥åº·åº¦æŠ¥å‘Š: {json.dumps(health_report, indent=2, ensure_ascii=False)}")


@cli.command()
@click.option('--db-path', default='params_pool.db', help='å‚æ•°æ± æ•°æ®åº“è·¯å¾„')
def cleanup(db_path: str):
    """æ¸…ç†è¿‡æœŸå‚æ•°"""
    params_manager = ParamsPoolManager(db_path)
    
    # æ¸…ç†å‰çŠ¶æ€
    before_stats = params_manager.get_pool_stats()
    logger.info(f"æ¸…ç†å‰å‚æ•°æ•°: {before_stats['total_count']}")
    
    # æ‰§è¡Œæ¸…ç†
    cleaned_count = params_manager.cleanup_expired_params()
    
    # æ¸…ç†åçŠ¶æ€
    after_stats = params_manager.get_pool_stats()
    logger.info(f"æ¸…ç†åå‚æ•°æ•°: {after_stats['total_count']}")
    logger.info(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸå‚æ•°")


@cli.command()
def test():
    """è¿è¡Œç³»ç»Ÿæµ‹è¯•"""
    asyncio.run(_test_async())


async def _test_async():
    """å¼‚æ­¥æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸ§ª å¼€å§‹è¿è¡Œç³»ç»Ÿæµ‹è¯•...")
    
    try:
        # å¯¼å…¥æµ‹è¯•æ¨¡å—
        from test_smart_crawler import SmartCrawlerTester
        
        tester = SmartCrawlerTester()
        success = await tester.run_integration_test()
        
        if success:
            logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        else:
            logger.error("ğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
            sys.exit(1)
            
    except ImportError as e:
        logger.error(f"âŒ æ— æ³•å¯¼å…¥æµ‹è¯•æ¨¡å—: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        sys.exit(1)


@cli.command()
@click.option('--file', '-f', help='ä»æ–‡ä»¶è¯»å–é—®é¢˜IDåˆ—è¡¨')
@click.option('--limit', default=20, help='æ¯é¡µå›ç­”æ•°é‡')
@click.option('--output', '-o', required=True, help='è¾“å‡ºç›®å½•')
@click.option('--concurrent', default=5, help='æœ€å¤§å¹¶å‘æ•°')
@click.option('--db-path', default='params_pool.db', help='å‚æ•°æ± æ•°æ®åº“è·¯å¾„')
@click.option('--user-data-dir', help='Chromeç”¨æˆ·æ•°æ®ç›®å½•')
def batch(file: Optional[str], limit: int, output: str, 
          concurrent: int, db_path: str, user_data_dir: Optional[str]):
    """æ‰¹é‡çˆ¬å–æ¨¡å¼
    
    ä»æ–‡ä»¶è¯»å–é—®é¢˜IDåˆ—è¡¨è¿›è¡Œæ‰¹é‡çˆ¬å–
    
    ç¤ºä¾‹:
        python main.py batch -f questions.txt -o results/ --concurrent 10
    """
    if not file:
        logger.error("âŒ è¯·æŒ‡å®šé—®é¢˜IDæ–‡ä»¶ (--file)")
        sys.exit(1)
        
    # è¯»å–é—®é¢˜IDåˆ—è¡¨
    try:
        with open(file, 'r', encoding='utf-8') as f:
            question_ids = [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        sys.exit(1)
        
    if not question_ids:
        logger.error("âŒ æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„é—®é¢˜ID")
        sys.exit(1)
        
    logger.info(f"ğŸ“‹ ä»æ–‡ä»¶ {file} è¯»å–åˆ° {len(question_ids)} ä¸ªé—®é¢˜ID")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    timestamp = int(time.time())
    output_file = output_dir / f"batch_results_{timestamp}.json"
    
    # æ‰§è¡Œæ‰¹é‡çˆ¬å–
    asyncio.run(_crawl_async(
        question_ids, limit, str(output_file), concurrent, 
        db_path, user_data_dir, True  # å¯ç”¨ç›‘æ§
    ))


@cli.command()
@click.option('--name', required=True, help='æ‰¹é‡ä»»åŠ¡åç§°')
@click.option('--description', default='', help='ä»»åŠ¡æè¿°')
@click.option('--min-answers', type=int, help='æœ€å°å›ç­”æ•°')
@click.option('--max-answers', type=int, help='æœ€å¤§å›ç­”æ•°')
@click.option('--keywords', help='å…³é”®è¯è¿‡æ»¤ï¼ˆé€—å·åˆ†éš”ï¼‰')
@click.option('--task-ids', help='æŒ‡å®šä»»åŠ¡IDï¼ˆé€—å·åˆ†éš”ï¼‰')
@click.option('--concurrent', default=3, help='å¹¶å‘æ•°')
@click.option('--batch-size', default=10, help='æ‰¹å¤„ç†å¤§å°')
@click.option('--request-delay', default=1.0, type=float, help='è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰')
@click.option('--max-retries', default=3, help='æœ€å¤§é‡è¯•æ¬¡æ•°')
@click.option('--auto-pause', is_flag=True, default=True, help='æ£€æµ‹åˆ°åçˆ¬è™«æ—¶è‡ªåŠ¨æš‚åœ')
@click.option('--chrome-user-data-dir', help='Chromeç”¨æˆ·æ•°æ®ç›®å½•')
@click.option('--headless/--no-headless', default=True, help='æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼')
def batch_crawl_by_rules(name, description, min_answers, max_answers, keywords, task_ids,
                        concurrent, batch_size, request_delay, max_retries, auto_pause, chrome_user_data_dir, headless):
    """åŸºäºè§„åˆ™çš„æ‰¹é‡é‡‡é›†
    
    ç¤ºä¾‹:
    python main.py batch-crawl-by-rules --name "é«˜è´¨é‡é—®é¢˜é‡‡é›†" --min-answers 50 --max-answers 500
    python main.py batch-crawl-by-rules --name "ç‰¹å®šå…³é”®è¯é‡‡é›†" --keywords "Python,æœºå™¨å­¦ä¹ "
    python main.py batch-crawl-by-rules --name "ä½å›ç­”æ•°é—®é¢˜é‡‡é›†" --max-answers 4409 --no-headless
    
    æ³¨æ„ï¼šä½¿ç”¨Seleniumæ–¹å¼é‡‡é›†
    """
    import asyncio
    from database_query_manager import QueryFilter
    from batch_crawl_manager import BatchCrawlManager, BatchCrawlConfig
    
    async def _batch_crawl():
        # æ„å»ºæŸ¥è¯¢è¿‡æ»¤å™¨
        query_filter = QueryFilter()
        
        if min_answers is not None:
            query_filter.answer_count_min = min_answers
        if max_answers is not None:
            query_filter.answer_count_max = max_answers
        if keywords:
            query_filter.title_keywords = [k.strip() for k in keywords.split(',')]
        if task_ids:
            query_filter.task_ids = [t.strip() for t in task_ids.split(',')]
        
        # åˆ›å»ºæ‰¹é‡é‡‡é›†é…ç½®
        config = BatchCrawlConfig(
            concurrent_limit=concurrent,
            batch_size=batch_size,
            request_delay=request_delay,
            max_retries=max_retries,
            auto_pause_on_anti_crawl=auto_pause
        )
        
        # åˆ›å»ºæ‰¹é‡é‡‡é›†ç®¡ç†å™¨
        manager = BatchCrawlManager(config)
        
        try:
            # åˆå§‹åŒ–çˆ¬è™« - ä»…ä½¿ç”¨Seleniumæ–¹å¼
            logger.info("åˆå§‹åŒ–æ‰¹é‡é‡‡é›†ç®¡ç†å™¨ (ä»…Seleniumæ¨¡å¼)...")
            success = await manager.initialize_crawler(chrome_user_data_dir, headless)
            if not success:
                logger.error("åˆå§‹åŒ–å¤±è´¥")
                return
            
            # åˆ›å»ºæ‰¹é‡ä»»åŠ¡
            logger.info(f"åˆ›å»ºæ‰¹é‡ä»»åŠ¡: {name}")
            task_id = manager.create_batch_task(name, description, query_filter)
            logger.info(f"ä»»åŠ¡ID: {task_id}")
            
            # å¼€å§‹é‡‡é›†
            logger.info("å¼€å§‹æ‰¹é‡é‡‡é›†...")
            result = await manager.start_batch_crawl(task_id)
            
            # è¾“å‡ºç»“æœ
            logger.info("æ‰¹é‡é‡‡é›†å®Œæˆ!")
            logger.info(f"æ€»è®¡: {result['total']}, æˆåŠŸ: {result['completed']}, å¤±è´¥: {result['failed']}")
            logger.info(f"æˆåŠŸç‡: {result['success_rate']:.2f}%")
            
        except Exception as e:
            logger.error(f"æ‰¹é‡é‡‡é›†å¤±è´¥: {e}")
        finally:
            await manager.cleanup()
    
    asyncio.run(_batch_crawl())


@cli.command()
@click.argument('task_id')
@click.option('--concurrent', default=5, help='å¹¶å‘æ•°')
@click.option('--batch-size', default=10, help='æ‰¹å¤„ç†å¤§å°')
@click.option('--request-delay', default=1.0, type=float, help='è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰')
@click.option('--chrome-user-data-dir', help='Chromeç”¨æˆ·æ•°æ®ç›®å½•')
def resume_batch_crawl(task_id, concurrent, batch_size, request_delay, chrome_user_data_dir):
    """ä»æ–­ç‚¹æ¢å¤æ‰¹é‡é‡‡é›†
    
    ç¤ºä¾‹:
    python main.py resume-batch-crawl TASK_20240101_123456
    """
    import asyncio
    from batch_crawl_manager import BatchCrawlManager, BatchCrawlConfig
    
    async def _resume_crawl():
        # åˆ›å»ºæ‰¹é‡é‡‡é›†é…ç½®
        config = BatchCrawlConfig(
            concurrent_limit=concurrent,
            batch_size=batch_size,
            request_delay=request_delay
        )
        
        # åˆ›å»ºæ‰¹é‡é‡‡é›†ç®¡ç†å™¨
        manager = BatchCrawlManager(config)
        
        try:
            # åˆå§‹åŒ–çˆ¬è™«
            logger.info("åˆå§‹åŒ–æ‰¹é‡é‡‡é›†ç®¡ç†å™¨...")
            success = await manager.initialize_crawler(chrome_user_data_dir)
            if not success:
                logger.error("åˆå§‹åŒ–å¤±è´¥")
                return
            
            # è·å–ä»»åŠ¡çŠ¶æ€
            task_status = manager.get_task_status(task_id)
            if 'error' in task_status:
                logger.error(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
                return
            
            logger.info(f"ä»»åŠ¡çŠ¶æ€: {task_status['task'].status}")
            logger.info(f"è¿›åº¦: {task_status['resume_info']['progress_percentage']:.2f}%")
            
            if not task_status['resume_info']['can_resume']:
                logger.warning("ä»»åŠ¡æ— æ³•æ¢å¤ï¼Œå¯èƒ½å·²ç»å®Œæˆæˆ–æ²¡æœ‰å¾…å¤„ç†çš„URL")
                return
            
            # ä»æ–­ç‚¹æ¢å¤
            logger.info("ä»æ–­ç‚¹æ¢å¤é‡‡é›†...")
            result = await manager.start_batch_crawl(task_id, resume_from_checkpoint=True)
            
            # è¾“å‡ºç»“æœ
            logger.info("æ‰¹é‡é‡‡é›†å®Œæˆ!")
            logger.info(f"æ€»è®¡: {result['total']}, æˆåŠŸ: {result['completed']}, å¤±è´¥: {result['failed']}")
            logger.info(f"æˆåŠŸç‡: {result['success_rate']:.2f}%")
            
        except Exception as e:
            logger.error(f"æ¢å¤é‡‡é›†å¤±è´¥: {e}")
        finally:
            await manager.cleanup()
    
    asyncio.run(_resume_crawl())


@cli.command()
@click.option('--status', help='æŒ‰çŠ¶æ€è¿‡æ»¤ (pending/running/completed/failed)')
def list_batch_tasks(status):
    """åˆ—å‡ºæ‰¹é‡é‡‡é›†ä»»åŠ¡
    
    ç¤ºä¾‹:
    python main.py list-batch-tasks
    python main.py list-batch-tasks --status running
    """
    from batch_crawl_manager import BatchCrawlManager
    
    manager = BatchCrawlManager()
    tasks = manager.list_tasks(status)
    
    if not tasks:
        logger.info("æ²¡æœ‰æ‰¾åˆ°ä»»åŠ¡")
        return
    
    logger.info(f"æ‰¾åˆ° {len(tasks)} ä¸ªä»»åŠ¡:")
    for task in tasks:
        logger.info(f"  {task.task_id}: {task.name} [{task.status}]")
        logger.info(f"    æè¿°: {task.description}")
        logger.info(f"    è¿›åº¦: {task.completed_urls}/{task.total_urls} ({task.completed_urls/max(task.total_urls,1)*100:.1f}%)")
        logger.info(f"    åˆ›å»ºæ—¶é—´: {task.created_at}")
        logger.info("")


@cli.command()
@click.argument('task_id')
def batch_task_status(task_id):
    """æŸ¥çœ‹æ‰¹é‡ä»»åŠ¡è¯¦ç»†çŠ¶æ€
    
    ç¤ºä¾‹:
    python main.py batch-task-status TASK_20240101_123456
    """
    from batch_crawl_manager import BatchCrawlManager
    
    manager = BatchCrawlManager()
    status = manager.get_task_status(task_id)
    
    if 'error' in status:
        logger.error(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
        return
    
    task = status['task']
    resume_info = status['resume_info']
    
    logger.info(f"ä»»åŠ¡ID: {task.task_id}")
    logger.info(f"åç§°: {task.name}")
    logger.info(f"æè¿°: {task.description}")
    logger.info(f"çŠ¶æ€: {task.status}")
    logger.info(f"æ€»URLæ•°: {task.total_urls}")
    logger.info(f"å·²å®Œæˆ: {task.completed_urls}")
    logger.info(f"å¤±è´¥: {task.failed_urls}")
    logger.info(f"è¿›åº¦: {resume_info['progress_percentage']:.2f}%")
    logger.info(f"å¯æ¢å¤: {'æ˜¯' if resume_info['can_resume'] else 'å¦'}")
    logger.info(f"åˆ›å»ºæ—¶é—´: {task.created_at}")
    logger.info(f"æ›´æ–°æ—¶é—´: {task.updated_at}")
    
    if resume_info['status_counts']:
        logger.info("\nçŠ¶æ€ç»Ÿè®¡:")
        for status_name, count in resume_info['status_counts'].items():
            logger.info(f"  {status_name}: {count}")
    
    if resume_info['recent_detections']:
        logger.info("\næœ€è¿‘çš„åçˆ¬è™«æ£€æµ‹:")
        for detection in resume_info['recent_detections'][:3]:
            logger.info(f"  {detection['time']}: {detection['type']} - {detection['details']}")


@cli.command()
@click.argument('task_id')
@click.option('--reason', default='', help='æš‚åœåŸå› ')
def pause_batch_task(task_id, reason):
    """æš‚åœæ‰¹é‡ä»»åŠ¡
    
    ç¤ºä¾‹:
    python main.py pause-batch-task TASK_20240101_123456 --reason "æ£€æµ‹åˆ°åçˆ¬è™«"
    """
    from batch_crawl_manager import BatchCrawlManager
    
    manager = BatchCrawlManager()
    manager.pause_task(task_id, reason)
    logger.info(f"ä»»åŠ¡ {task_id} å·²æš‚åœ")


@cli.command()
@click.argument('task_id')
def resume_batch_task(task_id):
    """æ¢å¤æ‰¹é‡ä»»åŠ¡
    
    ç¤ºä¾‹:
    python main.py resume-batch-task TASK_20240101_123456
    """
    from batch_crawl_manager import BatchCrawlManager
    
    manager = BatchCrawlManager()
    manager.resume_task(task_id)
    logger.info(f"ä»»åŠ¡ {task_id} å·²æ¢å¤")


@cli.command()
@click.argument('task_id')
@click.option('--max-retry-count', type=int, help='æœ€å¤§é‡è¯•æ¬¡æ•°é™åˆ¶')
def retry_failed_urls(task_id, max_retry_count):
    """é‡è¯•å¤±è´¥çš„URL
    
    ç¤ºä¾‹:
    python main.py retry-failed-urls TASK_20240101_123456
    python main.py retry-failed-urls TASK_20240101_123456 --max-retry-count 2
    """
    from batch_crawl_manager import BatchCrawlManager
    
    manager = BatchCrawlManager()
    affected_rows = manager.retry_failed_urls(task_id, max_retry_count)
    logger.info(f"é‡ç½®äº† {affected_rows} ä¸ªå¤±è´¥URLä¸ºå¾…é‡è¯•çŠ¶æ€")


@cli.command()
def batch_statistics():
    """æŸ¥çœ‹æ‰¹é‡é‡‡é›†ç»Ÿè®¡ä¿¡æ¯
    
    ç¤ºä¾‹:
    python main.py batch-statistics
    """
    from batch_crawl_manager import BatchCrawlManager
    
    manager = BatchCrawlManager()
    stats = manager.get_statistics()
    
    logger.info("æ‰¹é‡é‡‡é›†ç»Ÿè®¡ä¿¡æ¯:")
    logger.info(f"\nä»»åŠ¡ç»Ÿè®¡:")
    logger.info(f"  æ€»ä»»åŠ¡æ•°: {stats['tasks']['total']}")
    logger.info(f"  è¿è¡Œä¸­: {stats['tasks']['running']}")
    logger.info(f"  å·²å®Œæˆ: {stats['tasks']['completed']}")
    logger.info(f"  å¤±è´¥: {stats['tasks']['failed']}")
    
    logger.info(f"\nURLç»Ÿè®¡:")
    logger.info(f"  æ€»URLæ•°: {stats['urls']['total']}")
    logger.info(f"  å·²å®Œæˆ: {stats['urls']['completed']}")
    logger.info(f"  å¤±è´¥: {stats['urls']['failed']}")
    logger.info(f"  æˆåŠŸç‡: {stats['urls']['success_rate']:.2f}%")
    
    if stats['current_running_tasks']:
        logger.info(f"\nå½“å‰è¿è¡Œçš„ä»»åŠ¡: {', '.join(stats['current_running_tasks'])}")


if __name__ == '__main__':
    cli()