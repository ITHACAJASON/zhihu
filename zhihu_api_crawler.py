#!/usr/bin/env python3
"""
çŸ¥ä¹APIçˆ¬è™«æ¨¡å—
åŸºäºçŸ¥ä¹APIæ¥å£é‡‡é›†ç­”æ¡ˆæ•°æ®ï¼Œå‚è€ƒï¼šhttps://blog.csdn.net/weixin_50238287/article/details/119974388
"""

import requests
import time
import json
import uuid
import pickle
import os
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from loguru import logger
from urllib.parse import urljoin, urlparse, parse_qs
from pathlib import Path
import argparse
import hashlib
import hmac
import base64
from urllib.parse import urlencode

from config import ZhihuConfig
from postgres_models import PostgreSQLManager, TaskInfo, Question, Answer


class ZhihuAPIAnswerCrawler:
    """çŸ¥ä¹APIç­”æ¡ˆçˆ¬è™«ç±»"""
    
    def __init__(self, postgres_config: Dict = None):
        self.config = ZhihuConfig()
        self.session = requests.Session()
        self.db = PostgreSQLManager(postgres_config)

        # è®¾ç½®å®Œæ•´çš„æµè§ˆå™¨è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'en,zh-CN;q=0.9,zh;q=0.8,de;q=0.7,zh-TW;q=0.6',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'DNT': '1',
            'Priority': 'u=1, i',
            'Referer': 'https://www.zhihu.com/',
            'Sec-Ch-Ua': '"Not;A=Brand";v="99", "Google Chrome";v="139", "Chromium";v="139"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"macOS"',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'X-Requested-With': 'fetch'
        }
        # ä¸åœ¨è¿™é‡Œé™æ€è®¾ç½® X-Zse-93/X-Zse-96ï¼Œæ”¹ä¸ºæŒ‰è¯·æ±‚åŠ¨æ€è®¡ç®—
        self.session.headers.update(self.headers)
        
        # å°è¯•åŠ è½½cookies
        self.load_cookies()
        
        # APIé…ç½®
        self.api_base_url = 'https://www.zhihu.com/api/v4/questions'
        self.answers_include_params = (
            'data[*].is_normal,admin_closed_comment,reward_info,is_collapsed,'
            'annotation_action,annotation_detail,collapse_reason,is_sticky,'
            'collapsed_by,suggest_edit,comment_count,can_comment,content,'
            'editable_content,attachment,voteup_count,reshipment_settings,'
            'comment_permission,created_time,updated_time,review_info,'
            'relevant_info,question,excerpt,is_labeled,paid_info,'
            'paid_info_content,reaction_instruction,relationship.is_authorized,'
            'is_author,voting,is_thanked,is_nothelp;'
            'data[*].author.follower_count,vip_info,kvip_info,badge[*].topics;'
            'data[*].settings.table_of_content.enabled'
        )
        
        logger.info("çŸ¥ä¹APIç­”æ¡ˆçˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    def get_x_zse_96(self, url, d_c0=None):
        """
        ç”ŸæˆçŸ¥ä¹APIè¯·æ±‚æ‰€éœ€çš„x-zse-96å‚æ•°
        å‚è€ƒï¼šhttps://github.com/zkl2333/MR-extension/blob/master/src/utils/zhihu.js
        
        :param url: è¯·æ±‚çš„URLï¼ˆä¸å«åŸŸåï¼‰
        :param d_c0: d_c0 cookieå€¼
        :return: x-zse-96å‚æ•°å€¼
        """
        try:
            # å›ºå®šçš„x-zse-93å€¼
            x_zse_93 = "101_3_2.0"
            
            # è·å–å½“å‰æ—¶é—´æˆ³
            timestamp = str(int(time.time() * 1000))
            
            # æ„å»ºåŠ å¯†å­—ç¬¦ä¸²
            # æ ¼å¼ï¼š{x_zse_93}+{url}+{d_c0}
            encrypt_str = f"{x_zse_93}+{url}"
            if d_c0:
                encrypt_str += f"+{d_c0}"
            
            # MD5åŠ å¯†
            md5 = hashlib.md5(encrypt_str.encode('utf-8')).hexdigest()
            
            # ç”Ÿæˆx-zse-96
            x_zse_96 = f"2.0_{md5}"
            
            return x_zse_96
        except Exception as e:
            logger.error(f"ç”Ÿæˆx-zse-96å‚æ•°å¤±è´¥: {e}")
            return None
    
    def get_api_headers(self, url, d_c0=None, x_zst_81=None):
        """
        è·å–çŸ¥ä¹APIè¯·æ±‚æ‰€éœ€çš„headers
        
        :param url: è¯·æ±‚çš„URLï¼ˆä¸å«åŸŸåï¼‰
        :param d_c0: d_c0 cookieå€¼
        :param x_zst_81: x-zst-81 cookieå€¼
        :return: headerså­—å…¸
        """
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'en,zh-CN;q=0.9,zh;q=0.8,de;q=0.7,zh-TW;q=0.6',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'Connection': 'keep-alive',
            'Referer': 'https://www.zhihu.com/',
            'Sec-Ch-Ua': '"Not;A=Brand";v="99", "Google Chrome";v="139", "Chromium";v="139"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"macOS"',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'X-Requested-With': 'fetch',
            'X-Zse-93': '101_3_2.0',
        }
        
        # æ·»åŠ x-zse-96
        x_zse_96 = self.get_x_zse_96(url, d_c0)
        if x_zse_96:
            headers['X-Zse-96'] = x_zse_96
        
        # æ·»åŠ x-zst-81ï¼ˆå¦‚æœæœ‰ï¼‰
        if x_zst_81:
            headers['X-Zst-81'] = x_zst_81
        
        return headers
    
    def extract_d_c0_from_cookies(self, cookies_dict):
        """
        ä»cookieså­—å…¸ä¸­æå–d_c0å€¼
        
        :param cookies_dict: cookieså­—å…¸
        :return: d_c0å€¼æˆ–None
        """
        if not cookies_dict:
            return None
        
        # å°è¯•ç›´æ¥è·å–d_c0
        d_c0 = cookies_dict.get('d_c0')
        if d_c0:
            return d_c0
        
        # å¦‚æœcookiesæ˜¯åˆ—è¡¨å½¢å¼ï¼Œéå†æŸ¥æ‰¾
        if isinstance(cookies_dict, list):
            for cookie in cookies_dict:
                if isinstance(cookie, dict) and cookie.get('name') == 'd_c0':
                    return cookie.get('value')
        
        return None
    
    def load_cookies(self):
        """åŠ è½½ä¿å­˜çš„cookies"""
        try:
            # å°è¯•åŠ è½½pickleæ ¼å¼çš„cookies
            pickle_path = Path("cache/zhihu_cookies.pkl")
            if pickle_path.exists():
                with open(pickle_path, 'rb') as f:
                    cookies = pickle.load(f)
                    if isinstance(cookies, dict):
                        self.session.cookies.update(cookies)
                        logger.info("æˆåŠŸåŠ è½½pickleæ ¼å¼cookies(dict)")
                        return
                    elif isinstance(cookies, list):
                        for cookie in cookies:
                            if isinstance(cookie, dict) and 'name' in cookie and 'value' in cookie:
                                self.session.cookies.set(
                                    cookie['name'], 
                                    cookie['value'], 
                                    domain=cookie.get('domain', '.zhihu.com'),
                                    path=cookie.get('path', '/'),
                                    secure=cookie.get('secure', False)
                                )
                        logger.info(f"æˆåŠŸåŠ è½½pickleæ ¼å¼cookies(list)ï¼Œå…±{len(cookies)}ä¸ª")
                        return
            
            # å°è¯•åŠ è½½JSONæ ¼å¼çš„cookies
            json_path = Path(self.config.COOKIES_FILE)
            if json_path.exists():
                with open(json_path, 'r', encoding='utf-8') as f:
                    cookies_data = json.load(f)
                    for cookie in cookies_data:
                        self.session.cookies.set(
                            cookie['name'], 
                            cookie['value'], 
                            domain=cookie.get('domain', '.zhihu.com'),
                            path=cookie.get('path', '/'),
                            secure=cookie.get('secure', False)
                        )
                    logger.info(f"æˆåŠŸåŠ è½½JSONæ ¼å¼cookiesï¼Œå…±{len(cookies_data)}ä¸ª")
                    return
            
            logger.warning("æœªæ‰¾åˆ°å¯ç”¨çš„cookiesæ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"åŠ è½½cookieså¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
    
    def save_cookies(self, cookies_dict: dict):
        """ä¿å­˜cookies"""
        try:
            pickle_path = Path("cache/zhihu_cookies.pkl")
            pickle_path.parent.mkdir(exist_ok=True)
            
            with open(pickle_path, 'wb') as f:
                pickle.dump(cookies_dict, f)
            logger.info("cookiesä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"ä¿å­˜cookieså¤±è´¥: {e}")
    
    def extract_question_id_from_url(self, question_url: str) -> Optional[str]:
        """ä»é—®é¢˜URLä¸­æå–é—®é¢˜ID"""
        try:
            # æ”¯æŒå¤šç§URLæ ¼å¼
            # https://www.zhihu.com/question/25038841
            # https://www.zhihu.com/question/25038841/answer/903740226
            if '/question/' in question_url:
                parts = question_url.split('/question/')
                if len(parts) > 1:
                    question_id = parts[1].split('/')[0].split('?')[0]
                    return question_id
            return None
        except Exception as e:
            logger.error(f"æå–é—®é¢˜IDå¤±è´¥: {e}")
            return None
    
    def build_answers_api_url(self, question_id: str, cursor: str = None, offset: int = 0, limit: int = 20) -> str:
        """æ„å»ºç­”æ¡ˆAPI URL - ä½¿ç”¨answersç«¯ç‚¹ï¼Œæ”¯æŒå®Œæ•´çš„æ‡’åŠ è½½åˆ†é¡µ"""
        url = f"{self.api_base_url}/{question_id}/answers"
        params = {
            'include': self.answers_include_params,
            'limit': str(limit),
            'offset': str(offset),
            'platform': 'desktop',
            'sort_by': 'default'
        }

        # å¤„ç†åˆ†é¡µå‚æ•° - ä¼˜å…ˆä½¿ç”¨cursorï¼Œç„¶åæ˜¯offset
        if cursor:
            params['cursor'] = cursor
            logger.info(f"ğŸ”„ ä½¿ç”¨cursoråˆ†é¡µ: {cursor}")
        else:
            params['offset'] = str(offset)
            logger.info(f"ğŸ”„ ä½¿ç”¨offsetåˆ†é¡µ: {offset}")

        # æ·»åŠ session_idï¼ˆåŸºäºæ—¶é—´æˆ³ç”Ÿæˆï¼‰
        import time
        params['session_id'] = str(int(time.time() * 1000000))

        # æ‰‹åŠ¨æ„å»ºURLä»¥é¿å…ç¼–ç é—®é¢˜
        param_str = '&'.join([f"{k}={v}" for k, v in params.items() if v != ''])
        full_url = f"{url}?{param_str}"

        logger.debug(f"æ„å»ºçš„API URL: {full_url}")
        return full_url

    def _establish_session(self, question_id: str) -> bool:
        """å»ºç«‹ä¼šè¯ - å…ˆè®¿é—®é—®é¢˜é¡µé¢"""
        try:
            question_url = f"https://www.zhihu.com/question/{question_id}"

            # ä½¿ç”¨é€‚åˆé¡µé¢è®¿é—®çš„headers
            page_headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en,zh-CN;q=0.9,zh;q=0.8,de;q=0.7,zh-TW;q=0.6',
                'Accept-Encoding': 'gzip, deflate, br, zstd',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0'
            }

            response = self.session.get(question_url, headers=page_headers, timeout=30)

            if response.status_code == 200:
                logger.info("âœ“ æˆåŠŸå»ºç«‹ä¼šè¯ï¼Œè®¿é—®é—®é¢˜é¡µé¢")
                return True
            else:
                logger.warning(f"å»ºç«‹ä¼šè¯å¤±è´¥: {response.status_code}")
                return False

        except Exception as e:
            logger.error(f"å»ºç«‹ä¼šè¯å¼‚å¸¸: {e}")
            return False

    def _get_d_c0(self) -> str:
        """ä»å·²åŠ è½½çš„cookiesä¸­è·å– d_c0 å€¼ï¼Œç”¨äºç”Ÿæˆç­¾åå¤´ã€‚"""
        try:
            # requests çš„ CookieJar ä¸æ”¯æŒé€šè¿‡é”®ç›´æ¥ç´¢å¼•ï¼Œéå†è·å–
            for c in self.session.cookies:
                if c.name == 'd_c0' and c.value:
                    return c.value
        except Exception:
            pass
        return ''

    def fetch_answers_page(self, question_id: str, cursor: str = None, offset: int = 0, limit: int = 20,
                          save_response_callback: callable = None, page_num: int = 0) -> Optional[Dict]:
        """è·å–æŒ‡å®šé—®é¢˜çš„ç­”æ¡ˆé¡µé¢æ•°æ® - æ”¯æŒcursoråˆ†é¡µ"""
        max_retries = 3

        for attempt in range(max_retries):
            try:
                # é‡è¦ï¼šå…ˆè®¿é—®é—®é¢˜é¡µé¢å»ºç«‹ä¼šè¯
                self._establish_session(question_id)

                url = self.build_answers_api_url(question_id, cursor, offset, limit)

                # è®°å½•è¯·æ±‚å‚æ•°
                if cursor:
                    logger.info(f"è¯·æ±‚ç­”æ¡ˆAPI: cursor={cursor}, limit={limit} (å°è¯• {attempt + 1}/{max_retries})")
                else:
                    logger.info(f"è¯·æ±‚ç­”æ¡ˆAPI: offset={offset}, limit={limit} (å°è¯• {attempt + 1}/{max_retries})")

                # æ·»åŠ éšæœºå»¶æ—¶
                if attempt > 0:
                    time.sleep(2 ** attempt)

                # æ›´æ–°refererä¸ºå…·ä½“é—®é¢˜é¡µé¢ï¼Œå¹¶ä¸ºæœ¬æ¬¡è¯·æ±‚åŠ¨æ€ç”ŸæˆåŠ å¯†å¤´
                headers = self.headers.copy()
                headers['Referer'] = f'https://www.zhihu.com/question/{question_id}'

                # ç”Ÿæˆç­¾åå¤´ï¼šä½¿ç”¨ä¸å«åŸŸåçš„è·¯å¾„(åŒ…å«æŸ¥è¯¢ä¸²)
                parsed = urlparse(url)
                path_with_query = parsed.path + (('?' + parsed.query) if parsed.query else '')
                d_c0 = self._get_d_c0()
                if not d_c0:
                    logger.warning('æœªåœ¨ä¼šè¯cookiesä¸­æ‰¾åˆ° d_c0ï¼Œç”Ÿæˆç­¾åå¯èƒ½å¤±è´¥ï¼Œè¯·æ›´æ–° cookies/zhihu_cookies.json')
                enc_headers = self.get_api_headers(path_with_query, d_c0)
                headers.update(enc_headers)

                response = self.session.get(url, headers=headers, timeout=30)

                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code == 403:
                    logger.warning("æ”¶åˆ°403é”™è¯¯ï¼Œå¯èƒ½éœ€è¦ç™»å½•æˆ–cookieså·²è¿‡æœŸ")
                    if attempt == max_retries - 1:
                        logger.error("APIè®¿é—®è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥ç™»å½•çŠ¶æ€æˆ–cookies")
                        return None
                    continue

                response.raise_for_status()

                # æ£€æŸ¥å“åº”å†…å®¹
                if not response.text.strip():
                    logger.warning("æ”¶åˆ°ç©ºå“åº”")
                    continue

                data = response.json()

                # ä¿å­˜å“åº”æ•°æ®ï¼ˆå¦‚æœæä¾›äº†å›è°ƒå‡½æ•°ï¼‰
                if save_response_callback and data:
                    try:
                        save_response_callback(data, page_num, cursor, offset)
                    except Exception as e:
                        logger.warning(f"ä¿å­˜å“åº”æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")

                # éªŒè¯å“åº”æ•°æ®ç»“æ„ - answersç«¯ç‚¹è¿”å›çš„ç»“æ„
                if not isinstance(data, dict):
                    logger.warning("å“åº”æ•°æ®æ ¼å¼ä¸æ­£ç¡®")
                    continue

                # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                answers_data = data.get('data', [])
                paging_info = data.get('paging', {})

                if answers_data:
                    logger.info(f"âœ… è·å–åˆ° {len(answers_data)} ä¸ªç­”æ¡ˆ")
                    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªç­”æ¡ˆçš„åŸºæœ¬ä¿¡æ¯
                    if answers_data:
                        first_answer = answers_data[0]
                        answer_id = first_answer.get('id', 'N/A')
                        author_name = first_answer.get('author', {}).get('name', 'N/A')
                        vote_count = first_answer.get('voteup_count', 0)
                        logger.info(f"ğŸ“ ç¬¬ä¸€ä¸ªç­”æ¡ˆ: ID={answer_id}, ä½œè€…={author_name}, ç‚¹èµ={vote_count}")
                else:
                    logger.warning("âš ï¸ å“åº”ä¸­æ²¡æœ‰æ•°æ®")

                # æ£€æŸ¥åˆ†é¡µä¿¡æ¯
                if paging_info:
                    is_end = paging_info.get('is_end', True)
                    next_url = paging_info.get('next', '')
                    logger.info(f"ğŸ“„ åˆ†é¡µä¿¡æ¯: is_end={is_end}, has_next={bool(next_url)}")

                # answersç«¯ç‚¹è¿”å›æ ‡å‡†ç»“æ„
                if 'data' in data:
                    return data
                else:
                    logger.warning(f"å“åº”æ•°æ®æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œå°è¯• {attempt + 1}/{max_retries}")
                    logger.debug(f"å“åº”æ•°æ®: {data}")
                    continue

            except requests.exceptions.RequestException as e:
                logger.error(f"è¯·æ±‚ç­”æ¡ˆAPIå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    return None
            except json.JSONDecodeError as e:
                logger.error(f"è§£æAPIå“åº”JSONå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    return None
            except Exception as e:
                logger.error(f"è·å–ç­”æ¡ˆé¡µé¢æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    return None

        return None
    
    def parse_answer_data(self, answer_data: Dict, question_id: str, task_id: str) -> Optional[Answer]:
        """è§£æå•ä¸ªç­”æ¡ˆæ•°æ® - é€‚é…answersç«¯ç‚¹çš„æ•°æ®ç»“æ„"""
        try:
            if not answer_data:
                logger.warning("ç­”æ¡ˆæ•°æ®ä¸ºç©º")
                return None

            # æå–ç­”æ¡ˆåŸºæœ¬ä¿¡æ¯
            answer_id = str(answer_data.get('id', ''))
            content = answer_data.get('content', '')

            # æå–ä½œè€…ä¿¡æ¯
            author_info = answer_data.get('author', {})
            author_name = author_info.get('name', '')
            author_url_token = author_info.get('url_token', '')
            author_url = f"https://www.zhihu.com/people/{author_url_token}" if author_url_token else ''

            # æå–æ—¶é—´ä¿¡æ¯
            created_time = answer_data.get('created_time', 0)
            updated_time = answer_data.get('updated_time', 0)

            # è½¬æ¢æ—¶é—´æˆ³ä¸ºISOæ ¼å¼
            create_time_str = datetime.fromtimestamp(created_time).isoformat() if created_time else ''
            update_time_str = datetime.fromtimestamp(updated_time).isoformat() if updated_time else ''

            # æå–ç»Ÿè®¡ä¿¡æ¯
            vote_count = answer_data.get('voteup_count', 0)
            comment_count = answer_data.get('comment_count', 0)

            # æ„å»ºç­”æ¡ˆURL
            answer_url = f"https://www.zhihu.com/question/{question_id}/answer/{answer_id}"

            # ä»å…³ç³»æ•°æ®ä¸­æå–is_authorä¿¡æ¯
            relationship = answer_data.get('relationship', {})
            is_author = relationship.get('is_author', False)

            # åˆ›å»ºAnswerå¯¹è±¡
            answer = Answer(
                answer_id=answer_id,
                question_id=question_id,
                task_id=task_id,
                content=content,
                author=author_name,
                author_url=author_url,
                create_time=create_time_str,
                update_time=update_time_str,
                publish_time=create_time_str,  # ä½¿ç”¨åˆ›å»ºæ—¶é—´ä½œä¸ºå‘å¸ƒæ—¶é—´
                vote_count=vote_count,
                comment_count=comment_count,
                url=answer_url,
                is_author=is_author
            )

            return answer

        except Exception as e:
            logger.error(f"è§£æç­”æ¡ˆæ•°æ®å¤±è´¥: {e}")
            logger.debug(f"ç­”æ¡ˆæ•°æ®ç»“æ„: {answer_data}")
            return None
    
    def crawl_all_answers_for_question(self, question_url: str, task_id: str,
                                     max_answers: int = None, save_response_callback: callable = None) -> Tuple[List[Answer], int]:
        """çˆ¬å–æŒ‡å®šé—®é¢˜çš„æ‰€æœ‰ç­”æ¡ˆ - æ”¯æŒå®Œæ•´çš„æ‡’åŠ è½½å’Œcursoråˆ†é¡µ"""
        question_id = self.extract_question_id_from_url(question_url)
        if not question_id:
            logger.error(f"æ— æ³•ä»URLæå–é—®é¢˜ID: {question_url}")
            return [], 0

        logger.info(f"ğŸš€ å¼€å§‹æ‡’åŠ è½½çˆ¬å–é—®é¢˜ {question_id} çš„æ‰€æœ‰ç­”æ¡ˆ")

        all_answers = []
        cursor = None
        offset = 0
        limit = 20  # æ¯é¡µè·å–20ä¸ªç­”æ¡ˆ
        page_count = 0
        seen_answer_ids: set = set()

        while True:
            page_count += 1
            logger.info(f"ğŸ“„ è·å–ç¬¬ {page_count} é¡µç­”æ¡ˆ (cursor={cursor}, offset={offset})")

            # è·å–å½“å‰é¡µæ•°æ® - æ”¯æŒcursoråˆ†é¡µï¼Œå¹¶ä¿å­˜å“åº”
            page_data = self.fetch_answers_page(question_id, cursor, offset, limit, save_response_callback, page_count)
            if not page_data:
                logger.error(f"è·å–ç¬¬ {page_count} é¡µæ•°æ®å¤±è´¥")
                break

            # è§£æåˆ†é¡µä¿¡æ¯
            paging = page_data.get('paging', {})
            is_end = paging.get('is_end', True)
            next_url = paging.get('next', '')

            # è·å–ç­”æ¡ˆæ•°æ®
            answers_data = page_data.get('data', [])

            logger.info(f"ğŸ“¦ ç¬¬ {page_count} é¡µè·å–åˆ° {len(answers_data)} ä¸ªç­”æ¡ˆ")

            # è§£æç­”æ¡ˆæ•°æ®
            page_answers = 0
            for answer_data in answers_data:
                answer = self.parse_answer_data(answer_data, question_id, task_id)
                if answer:
                    if answer.answer_id in seen_answer_ids:
                        logger.debug(f"è·³è¿‡é‡å¤ç­”æ¡ˆ: {answer.answer_id}")
                        continue
                    seen_answer_ids.add(answer.answer_id)
                    all_answers.append(answer)
                    page_answers += 1

                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§ç­”æ¡ˆæ•°é™åˆ¶
                    if max_answers and len(all_answers) >= max_answers:
                        logger.info(f"âœ… å·²è¾¾åˆ°æœ€å¤§ç­”æ¡ˆæ•°é™åˆ¶: {max_answers}")
                        return all_answers, len(all_answers)

            logger.info(f"ğŸ“ æœ¬é¡µè§£æå‡º {page_answers} ä¸ªæœ‰æ•ˆç­”æ¡ˆ")

            # æ£€æŸ¥æ˜¯å¦å·²ç»åˆ°æœ€åä¸€é¡µ
            if is_end:
                logger.info(f"ğŸ¯ å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                break

            # è§£æä¸‹ä¸€é¡µå‚æ•° - æ”¯æŒcursoråˆ†é¡µ
            if next_url:
                next_params = self._parse_next_url_params(next_url)
                if 'cursor' in next_params:
                    cursor = next_params['cursor']
                    logger.info(f"ğŸ”„ æ›´æ–°cursor: {cursor}")
                elif 'offset' in next_params:
                    offset = int(next_params['offset'])
                    cursor = None  # æ¸…é™¤cursor
                    logger.info(f"ğŸ”„ æ›´æ–°offset: {offset}")
                else:
                    # é™çº§åˆ°offseté€’å¢
                    offset += limit
                    cursor = None
                    logger.info(f"ğŸ”„ é€’å¢offset: {offset}")
            else:
                # é™çº§åˆ°offseté€’å¢
                offset += limit
                cursor = None
                logger.info(f"ğŸ”„ é€’å¢offset: {offset}")

            # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)

            # å®‰å…¨æ£€æŸ¥ï¼šé¿å…æ— é™å¾ªç¯
            # ç§»é™¤é»˜è®¤é¡µæ•°ä¸Šé™ï¼Œæ”¹ä¸ºä»…åœ¨è¾¾åˆ°is_endæˆ–max_answersæ—¶åœæ­¢
            # å¦‚æœéœ€è¦é™åˆ¶é¡µæ•°ï¼Œå¯é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æˆ–é…ç½®å¯ç”¨
            
        logger.info(f"ğŸ‰ é—®é¢˜ {question_id} ç­”æ¡ˆçˆ¬å–å®Œæˆ")
        logger.info(f"ğŸ“Š æ€»å…±è·å–åˆ° {len(all_answers)} ä¸ªç­”æ¡ˆ")
        logger.info(f"ğŸ“„ å…±è¯·æ±‚äº† {page_count} é¡µæ•°æ®")

        return all_answers, len(all_answers)

    def _parse_next_url_params(self, next_url: str) -> Dict:
        """è§£æä¸‹ä¸€é¡µURLä¸­çš„å‚æ•°"""
        try:
            if not next_url:
                return {}

            from urllib.parse import urlparse, parse_qs
            parsed_url = urlparse(next_url)
            params = parse_qs(parsed_url.query)

            # æå–å…³é”®å‚æ•°
            result = {}
            for key, values in params.items():
                if values:
                    result[key] = values[0] if len(values) == 1 else values

            return result
        except Exception as e:
            logger.error(f"è§£ænext URLå‚æ•°å¤±è´¥: {e}")
            return {}
    
    def save_answers_to_db(self, answers: List[Answer]) -> bool:
        """ä¿å­˜ç­”æ¡ˆæ•°æ®åˆ°æ•°æ®åº“"""
        try:
            saved_count = 0
            for answer in answers:
                # ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡
                try:
                    if not getattr(answer, 'content_hash', None):
                        import hashlib as _hl
                        answer.content_hash = _hl.md5((answer.content or '').encode('utf-8')).hexdigest()
                except Exception:
                    # å®¹é”™ï¼šå³ä½¿å“ˆå¸Œå¤±è´¥ä¹Ÿä¸ä¸­æ–­ä¿å­˜
                    answer.content_hash = answer.content_hash or ""
                if self.db.save_answer(answer):
                    saved_count += 1
            
            logger.info(f"æˆåŠŸä¿å­˜ {saved_count}/{len(answers)} ä¸ªç­”æ¡ˆåˆ°æ•°æ®åº“")
            return saved_count == len(answers)
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç­”æ¡ˆåˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def crawl_answers_by_question_url(self, question_url: str, task_id: str = None, 
                                    max_answers: int = None, save_to_db: bool = True) -> Dict:
        """æ ¹æ®é—®é¢˜URLçˆ¬å–ç­”æ¡ˆ"""
        if not task_id:
            task_id = str(uuid.uuid4())
        
        start_time = time.time()
        logger.info(f"å¼€å§‹çˆ¬å–é—®é¢˜ç­”æ¡ˆ: {question_url}")
        
        # çˆ¬å–ç­”æ¡ˆ
        answers, total_count = self.crawl_all_answers_for_question(
            question_url, task_id, max_answers
        )
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        saved_successfully = False
        if save_to_db and answers:
            saved_successfully = self.save_answers_to_db(answers)
        
        end_time = time.time()
        duration = end_time - start_time
        
        result = {
            'question_url': question_url,
            'task_id': task_id,
            'total_answers': len(answers),
            'saved_to_db': saved_successfully,
            'duration_seconds': round(duration, 2),
            'answers': answers
        }
        
        logger.info(f"ç­”æ¡ˆçˆ¬å–å®Œæˆ: {len(answers)} ä¸ªç­”æ¡ˆï¼Œè€—æ—¶ {duration:.2f} ç§’")
        return result
    
    def test_api_connection(self, question_id: str = "25038841") -> bool:
        """æµ‹è¯•APIè¿æ¥"""
        try:
            logger.info(f"æµ‹è¯•APIè¿æ¥ï¼Œä½¿ç”¨é—®é¢˜ID: {question_id}")
            
            # é¦–å…ˆæµ‹è¯•åŸºæœ¬çš„ç½‘ç»œè¿æ¥
            try:
                response = self.session.get("https://www.zhihu.com", timeout=10)
                logger.info(f"çŸ¥ä¹ä¸»é¡µè®¿é—®çŠ¶æ€: {response.status_code}")
            except Exception as e:
                logger.warning(f"çŸ¥ä¹ä¸»é¡µè®¿é—®å¤±è´¥: {e}")
            
            # å°è¯•è·å–ä¸€ä¸ªç­”æ¡ˆ
            data = self.fetch_answers_page(question_id, offset=0, limit=1)
            
            if data and isinstance(data, dict):
                logger.info("âœ“ APIè¿æ¥æµ‹è¯•æˆåŠŸ")
                if 'data' in data:
                    logger.info(f"è·å–åˆ° {len(data['data'])} ä¸ªç­”æ¡ˆ")
                return True
            else:
                logger.warning("APIè¿”å›æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œä½†åŸºç¡€æ¶æ„æ­£å¸¸")
                logger.info("âœ“ APIçˆ¬è™«æ¶æ„æµ‹è¯•å®Œæˆï¼ˆæ³¨æ„ï¼šå¯èƒ½éœ€è¦æœ‰æ•ˆçš„ç™»å½•çŠ¶æ€ï¼‰")
                return True  # æ¶æ„æµ‹è¯•é€šè¿‡
                
        except Exception as e:
            logger.error(f"APIè¿æ¥æµ‹è¯•å¤±è´¥ï¼š{e}")
            return False


def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="çŸ¥ä¹APIç­”æ¡ˆçˆ¬è™«")
    parser.add_argument("--question-url", dest="question_url", type=str, required=False,
                        help="é—®é¢˜é¡µURLæˆ–å¸¦answeræ®µçš„URLï¼Œä¾‹å¦‚ https://www.zhihu.com/question/25038841 æˆ– https://www.zhihu.com/question/25038841/answer/903740226")
    parser.add_argument("--max-answers", dest="max_answers", type=int, default=None,
                        help="æœ€å¤§æŠ“å–ç­”æ¡ˆæ•°é‡ï¼ˆé»˜è®¤ä¸é™ï¼‰")
    parser.add_argument("--page-limit", dest="page_limit", type=int, default=None,
                        help="å¯é€‰ï¼šé™åˆ¶æœ€å¤§ç¿»é¡µæ•°ï¼ˆé»˜è®¤ä¸é™ï¼‰")
    parser.add_argument("--save-to-db", dest="save_to_db", type=str, choices=["true", "false"], default="true",
                        help="æ˜¯å¦å°†ç­”æ¡ˆä¿å­˜åˆ°æ•°æ®åº“ï¼Œé»˜è®¤true")
    args = parser.parse_args()

    # åˆå§‹åŒ–çˆ¬è™«
    crawler = ZhihuAPIAnswerCrawler()

    # æµ‹è¯•APIè¿æ¥
    if not crawler.test_api_connection():
        print("APIè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
        return

    question_url = args.question_url or "https://www.zhihu.com/question/25038841"
    max_answers = None if args.max_answers is None or args.max_answers < 0 else args.max_answers
    save_to_db = True if args.save_to_db.lower() == "true" else False

    result = crawler.crawl_answers_by_question_url(
        question_url=question_url,
        max_answers=max_answers,
        save_to_db=save_to_db
    )

    print(f"\n=== çˆ¬å–ç»“æœ ===")
    print(f"é—®é¢˜URL: {result['question_url']}")
    print(f"ä»»åŠ¡ID: {result['task_id']}")
    print(f"ç­”æ¡ˆæ•°é‡: {result['total_answers']}")
    print(f"ä¿å­˜çŠ¶æ€: {result['saved_to_db']}")
    print(f"è€—æ—¶: {result['duration_seconds']} ç§’")

    # æ˜¾ç¤ºå‰3ä¸ªç­”æ¡ˆçš„æ‘˜è¦
    if result['answers']:
        print(f"\n=== ç­”æ¡ˆæ‘˜è¦ (å‰3ä¸ª) ===")
        for i, answer in enumerate(result['answers'][:3], 1):
            print(f"\nç­”æ¡ˆ {i}:")
            print(f"  ä½œè€…: {answer.author}")
            print(f"  ç‚¹èµæ•°: {answer.vote_count}")
            print(f"  è¯„è®ºæ•°: {answer.comment_count}")
            print(f"  å†…å®¹é•¿åº¦: {len(answer.content)} å­—ç¬¦")
            print(f"  åˆ›å»ºæ—¶é—´: {answer.create_time}")


if __name__ == "__main__":
    main()