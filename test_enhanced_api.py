#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æ”¹è¿›åçš„çŸ¥ä¹APIæ‡’åŠ è½½åŠŸèƒ½
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from zhihu_api_crawler import ZhihuAPIAnswerCrawler
from loguru import logger

def test_enhanced_lazyload_api():
    """æµ‹è¯•æ”¹è¿›åçš„æ‡’åŠ è½½API"""
    logger.info("ğŸš€ æµ‹è¯•æ”¹è¿›åçš„çŸ¥ä¹APIæ‡’åŠ è½½åŠŸèƒ½")

    # åˆå§‹åŒ–APIçˆ¬è™«
    crawler = ZhihuAPIAnswerCrawler()

    # æµ‹è¯•é—®é¢˜ - é€‰æ‹©ä¸€ä¸ªç­”æ¡ˆè¾ƒå¤šçš„é—®é¢˜
    test_question_url = "https://www.zhihu.com/question/19551593"
    test_question_id = "19551593"

    logger.info(f"ğŸ“ æµ‹è¯•é—®é¢˜: {test_question_url}")
    logger.info(f"ğŸ”¢ é—®é¢˜ID: {test_question_id}")

    # æµ‹è¯•å•ä¸ªé¡µé¢è¯·æ±‚
    logger.info("\n" + "="*60)
    logger.info("ğŸ§ª æµ‹è¯•å•ä¸ªé¡µé¢è¯·æ±‚")
    logger.info("="*60)

    page_data = crawler.fetch_answers_page(test_question_id, limit=10)
    if page_data:
        feeds_count = len(page_data.get('data', []))
        paging = page_data.get('paging', {})
        session_id = page_data.get('session', {}).get('id', '')

        logger.info("âœ… é¡µé¢è¯·æ±‚æˆåŠŸ!")
        logger.info(f"ğŸ“¦ Feedsæ•°é‡: {feeds_count}")
        logger.info(f"ğŸ”‘ Session ID: {session_id}")
        logger.info(f"ğŸ“„ åˆ†é¡µä¿¡æ¯: is_end={paging.get('is_end', 'Unknown')}")
        logger.info(f"ğŸ”— ä¸‹ä¸€é¡µURL: {paging.get('next', 'None')[:100]}...")

        # æµ‹è¯•å®Œæ•´æ‡’åŠ è½½
        logger.info("\n" + "="*60)
        logger.info("ğŸš€ æµ‹è¯•å®Œæ•´æ‡’åŠ è½½çˆ¬å–")
        logger.info("="*60)

        answers, total_count = crawler.crawl_all_answers_for_question(
            test_question_url,
            task_id="test_lazyload",
            max_answers=30  # é™åˆ¶æ•°é‡ç”¨äºæµ‹è¯•
        )

        logger.info("ğŸ‰ æ‡’åŠ è½½æµ‹è¯•å®Œæˆ!")
        logger.info(f"ğŸ“Š æ€»å…±è·å–åˆ° {total_count} ä¸ªç­”æ¡ˆ")

        if answers:
            logger.info("ğŸ“‹ ç¤ºä¾‹ç­”æ¡ˆä¿¡æ¯:")
            for i, answer in enumerate(answers[:5], 1):
                logger.info(f"  {i}. {answer.author} - {answer.vote_count}èµ")
                logger.info(f"     å†…å®¹é•¿åº¦: {len(answer.content)} å­—ç¬¦")
        else:
            logger.warning("âš ï¸ æœªè·å–åˆ°ç­”æ¡ˆæ•°æ®")

        return True
    else:
        logger.error("âŒ é¡µé¢è¯·æ±‚å¤±è´¥")
        return False

def compare_with_old_method():
    """ä¸æ—§æ–¹æ³•å¯¹æ¯”æµ‹è¯•"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ”„ ä¸æ—§æ–¹æ³•å¯¹æ¯”æµ‹è¯•")
    logger.info("="*60)

    crawler = ZhihuAPIAnswerCrawler()
    test_question_url = "https://www.zhihu.com/question/19551593"

    # æµ‹è¯•æ–°æ–¹æ³•
    logger.info("ğŸ“ˆ æµ‹è¯•æ–°æ–¹æ³•ï¼ˆæ”¯æŒcursoråˆ†é¡µï¼‰...")
    answers_new, count_new = crawler.crawl_all_answers_for_question(
        test_question_url,
        task_id="test_new",
        max_answers=20
    )

    logger.info(f"âœ… æ–°æ–¹æ³•è·å–åˆ° {count_new} ä¸ªç­”æ¡ˆ")

    return count_new > 0

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ çŸ¥ä¹APIæ‡’åŠ è½½å¢å¼ºåŠŸèƒ½æµ‹è¯•")

    # æµ‹è¯•åŸºç¡€åŠŸèƒ½
    success = test_enhanced_lazyload_api()

    if success:
        # å¯¹æ¯”æµ‹è¯•
        compare_with_old_method()

        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        logger.info("="*60)
        logger.info("âœ… æ”¹è¿›åçš„APIæ”¯æŒ:")
        logger.info("   â€¢ å®Œæ•´çš„æ‡’åŠ è½½æœºåˆ¶")
        logger.info("   â€¢ Cursoråˆ†é¡µæ”¯æŒ")
        logger.info("   â€¢ è¿ç»­è¯·æ±‚fetchæ–‡ä»¶")
        logger.info("   â€¢ æœ‰æ•ˆçš„session IDç®¡ç†")
        logger.info("   â€¢ è‡ªåŠ¨åˆ†é¡µå‚æ•°è§£æ")
    else:
        logger.error("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®")

if __name__ == "__main__":
    main()
