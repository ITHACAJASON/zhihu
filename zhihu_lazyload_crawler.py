#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥ä¹APIæ‡’åŠ è½½çˆ¬è™«
æ”¯æŒå®Œæ•´çš„åˆ†é¡µå’Œè¿ç»­è¯·æ±‚fetchæ–‡ä»¶
"""

import requests
import json
import time
import re
from urllib.parse import urlparse, parse_qs
from typing import List, Dict, Optional, Tuple
from loguru import logger
from pathlib import Path

class ZhihuLazyLoadCrawler:
    """çŸ¥ä¹æ‡’åŠ è½½APIçˆ¬è™«"""

    def __init__(self):
        self.session = requests.Session()
        self.base_headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin'
        }
        self.session.headers.update(self.base_headers)
        self.load_cookies()

        # APIé…ç½®
        self.api_base_url = 'https://www.zhihu.com/api/v4/questions'
        self.feeds_include_params = (
            'data[*].is_normal,admin_closed_comment,reward_info,is_collapsed,'
            'annotation_action,annotation_detail,collapse_reason,is_sticky,'
            'collapsed_by,suggest_edit,comment_count,can_comment,content,'
            'editable_content,attachment,voteup_count,reshipment_settings,'
            'comment_permission,created_time,updated_time,review_info,'
            'relevant_info,question,excerpt,is_labeled,paid_info,'
            'paid_info_content,reaction_instruction,relationship.is_authorized,'
            'is_author,voting,is_thanked,is_nothelp;'
            'data[*].author.follower_count,vip_info,kvip_info,badge[*].topics;'
            'data[*].settings.table_of_content.enabled'
        )

    def load_cookies(self):
        """åŠ è½½cookies"""
        try:
            cookie_file = Path('cookies/zhihu_cookies.json')
            with open(cookie_file, 'r', encoding='utf-8') as f:
                cookies_data = json.load(f)
                for cookie in cookies_data:
                    self.session.cookies.set(
                        cookie['name'],
                        cookie['value'],
                        domain=cookie.get('domain', '.zhihu.com'),
                        path=cookie.get('path', '/')
                    )
                logger.info(f"æˆåŠŸåŠ è½½cookiesï¼Œå…±{len(cookies_data)}ä¸ª")
                return True
        except Exception as e:
            logger.error(f"åŠ è½½cookieså¤±è´¥: {e}")
            return False

    def extract_question_id(self, question_url: str) -> Optional[str]:
        """ä»é—®é¢˜URLä¸­æå–é—®é¢˜ID"""
        try:
            if '/question/' in question_url:
                parts = question_url.split('/question/')
                if len(parts) > 1:
                    question_id = parts[1].split('/')[0].split('?')[0]
                    return question_id
            return None
        except Exception as e:
            logger.error(f"æå–é—®é¢˜IDå¤±è´¥: {e}")
            return None

    def parse_next_url(self, next_url: str) -> Dict:
        """è§£æä¸‹ä¸€é¡µURLä¸­çš„å‚æ•°"""
        try:
            if not next_url:
                return {}

            parsed_url = urlparse(next_url)
            params = parse_qs(parsed_url.query)

            # æå–å…³é”®å‚æ•°
            result = {}
            for key, values in params.items():
                if values:
                    result[key] = values[0] if len(values) == 1 else values

            return result
        except Exception as e:
            logger.error(f"è§£ænext URLå¤±è´¥: {e}")
            return {}

    def build_feeds_url(self, question_id: str, cursor: str = None,
                       offset: int = None, limit: int = 20) -> str:
        """æ„å»ºfeeds API URL"""
        url = f"{self.api_base_url}/{question_id}/feeds"

        params = {
            'include': self.feeds_include_params,
            'limit': str(limit),
            'order': 'default',
            'ws_qiangzhisafe': '0',
            'platform': 'desktop'
        }

        # æ·»åŠ cursoræˆ–offsetï¼ˆä¼˜å…ˆä½¿ç”¨cursorï¼‰
        if cursor:
            params['cursor'] = cursor
        elif offset is not None:
            params['offset'] = str(offset)

        # æ‰‹åŠ¨æ„å»ºURLé¿å…ç¼–ç é—®é¢˜
        param_str = '&'.join([f"{k}={v}" for k, v in params.items()])
        return f"{url}?{param_str}"

    def fetch_feeds_page(self, question_id: str, cursor: str = None,
                        offset: int = None, limit: int = 20) -> Optional[Dict]:
        """è·å–feedsé¡µé¢æ•°æ®"""
        max_retries = 3

        for attempt in range(max_retries):
            try:
                url = self.build_feeds_url(question_id, cursor, offset, limit)
                logger.info(f"è¯·æ±‚feeds API (å°è¯• {attempt + 1}/{max_retries})")
                logger.debug(f"è¯·æ±‚URL: {url}")

                # è®¾ç½®headers
                headers = self.base_headers.copy()
                headers['Referer'] = f'https://www.zhihu.com/question/{question_id}'
                headers['X-Requested-With'] = 'fetch'

                response = self.session.get(url, headers=headers, timeout=30)

                if response.status_code == 200:
                    data = response.json()

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                    feeds_data = data.get('data', [])
                    logger.info(f"âœ… è·å–åˆ° {len(feeds_data)} ä¸ªfeedé¡¹")

                    # æ£€æŸ¥sessionä¿¡æ¯
                    session_info = data.get('session', {})
                    session_id = session_info.get('id', '')
                    if session_id:
                        logger.info(f"ğŸ”‘ Session ID: {session_id}")

                    return data

                elif response.status_code == 403:
                    logger.warning(f"âŒ APIè®¿é—®è¢«æ‹’ç»: {response.status_code}")
                    error_data = response.json()
                    if 'error' in error_data:
                        logger.warning(f"é”™è¯¯ä¿¡æ¯: {error_data['error'].get('message', 'Unknown error')}")
                    return None

                else:
                    logger.warning(f"âš ï¸ APIè¿”å›å¼‚å¸¸çŠ¶æ€: {response.status_code}")
                    if attempt == max_retries - 1:
                        return None

            except requests.exceptions.RequestException as e:
                logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return None
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æå¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return None
            except Exception as e:
                logger.error(f"æœªçŸ¥é”™è¯¯ (å°è¯• {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return None

            # å¤±è´¥åç­‰å¾…åé‡è¯•
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)

        return None

    def crawl_all_feeds_lazyload(self, question_id: str, max_feeds: int = None) -> List[Dict]:
        """ä½¿ç”¨æ‡’åŠ è½½æ–¹å¼çˆ¬å–æ‰€æœ‰feedsæ•°æ®"""
        all_feeds = []
        page_count = 0
        cursor = None
        offset = 0

        logger.info(f"ğŸš€ å¼€å§‹æ‡’åŠ è½½çˆ¬å–é—®é¢˜ {question_id} çš„feeds")
        logger.info(f"ğŸ“Š æœ€å¤§feedsé™åˆ¶: {max_feeds if max_feeds else 'æ— é™åˆ¶'}")

        while True:
            page_count += 1
            logger.info(f"\nğŸ“„ è·å–ç¬¬ {page_count} é¡µæ•°æ®")
            logger.info(f"ğŸ”„ Cursor: {cursor}, Offset: {offset}")

            # è·å–å½“å‰é¡µæ•°æ®
            page_data = self.fetch_feeds_page(question_id, cursor=cursor, offset=offset, limit=20)
            if not page_data:
                logger.error(f"âŒ è·å–ç¬¬ {page_count} é¡µæ•°æ®å¤±è´¥")
                break

            # è§£æfeedsæ•°æ®
            feeds_data = page_data.get('data', [])
            logger.info(f"ğŸ“¦ æœ¬é¡µè·å–åˆ° {len(feeds_data)} ä¸ªfeedé¡¹")

            # æ·»åŠ åˆ°æ€»é›†åˆ
            all_feeds.extend(feeds_data)

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶
            if max_feeds and len(all_feeds) >= max_feeds:
                logger.info(f"âœ… å·²è¾¾åˆ°æœ€å¤§feedsæ•°é‡é™åˆ¶: {max_feeds}")
                all_feeds = all_feeds[:max_feeds]
                break

            # è§£æåˆ†é¡µä¿¡æ¯
            paging = page_data.get('paging', {})
            is_end = paging.get('is_end', True)
            next_url = paging.get('next', '')

            logger.info(f"ğŸ” åˆ†é¡µä¿¡æ¯: is_end={is_end}")

            if is_end:
                logger.info(f"ğŸ¯ å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                break

            # è§£æä¸‹ä¸€é¡µå‚æ•°
            next_params = self.parse_next_url(next_url)

            # æ›´æ–°cursorå’Œoffset
            if 'cursor' in next_params:
                cursor = next_params['cursor']
                logger.info(f"ğŸ“Œ æ›´æ–°cursor: {cursor}")
            elif 'offset' in next_params:
                offset = int(next_params['offset'])
                logger.info(f"ğŸ“Œ æ›´æ–°offset: {offset}")
            else:
                # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ä¸‹ä¸€é¡µå‚æ•°ï¼Œå¢åŠ offset
                offset += len(feeds_data)
                logger.info(f"ğŸ“Œ é€’å¢offset: {offset}")

            # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)

            # å®‰å…¨æ£€æŸ¥
            if page_count >= 100:  # æœ€å¤š100é¡µ
                logger.warning(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶: {page_count}")
                break

        logger.info(f"\nğŸ‰ æ‡’åŠ è½½çˆ¬å–å®Œæˆ!")
        logger.info(f"ğŸ“Š æ€»å…±è·å–åˆ° {len(all_feeds)} ä¸ªfeeds")
        logger.info(f"ğŸ“„ å…±è¯·æ±‚äº† {page_count} é¡µ")

        return all_feeds

    def extract_answers_from_feeds(self, feeds_data: List[Dict]) -> List[Dict]:
        """ä»feedsæ•°æ®ä¸­æå–ç­”æ¡ˆ"""
        answers = []

        for feed_item in feeds_data:
            try:
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç­”æ¡ˆç±»å‹
                if feed_item.get('target_type') != 'answer':
                    continue

                # æå–ç­”æ¡ˆæ•°æ®
                target = feed_item.get('target', {})
                if not target:
                    continue

                # æ„å»ºç­”æ¡ˆä¿¡æ¯
                answer_info = {
                    'answer_id': str(target.get('id', '')),
                    'content': target.get('content', ''),
                    'excerpt': target.get('excerpt', ''),
                    'voteup_count': target.get('voteup_count', 0),
                    'comment_count': target.get('comment_count', 0),
                    'created_time': target.get('created_time', 0),
                    'updated_time': target.get('updated_time', 0),
                    'author': target.get('author', {}).get('name', ''),
                    'author_url_token': target.get('author', {}).get('url_token', ''),
                    'question_id': target.get('question', {}).get('id', ''),
                    'question_title': target.get('question', {}).get('title', ''),
                    'is_author': target.get('relationship', {}).get('is_author', False)
                }

                answers.append(answer_info)

            except Exception as e:
                logger.warning(f"è§£æfeedé¡¹å¤±è´¥: {e}")
                continue

        logger.info(f"ğŸ“ ä»feedsä¸­æå–åˆ° {len(answers)} ä¸ªç­”æ¡ˆ")
        return answers

    def test_lazyload_ability(self, question_id: str) -> Dict:
        """æµ‹è¯•æ‡’åŠ è½½èƒ½åŠ›"""
        logger.info(f"ğŸ§ª æµ‹è¯•é—®é¢˜ {question_id} çš„æ‡’åŠ è½½èƒ½åŠ›")

        # ç¬¬ä¸€é¡µæµ‹è¯•
        first_page = self.fetch_feeds_page(question_id, limit=5)
        if not first_page:
            return {'success': False, 'error': 'æ— æ³•è·å–ç¬¬ä¸€é¡µæ•°æ®'}

        # åˆ†æåˆ†é¡µä¿¡æ¯
        paging = first_page.get('paging', {})
        has_paging = bool(paging)
        has_next = bool(paging.get('next'))
        is_end = paging.get('is_end', True)

        # æµ‹è¯•ä¸‹ä¸€é¡µ
        next_page_success = False
        if not is_end and has_next:
            next_params = self.parse_next_url(paging['next'])
            cursor = next_params.get('cursor')
            offset = next_params.get('offset')

            if cursor or offset:
                next_page = self.fetch_feeds_page(question_id, cursor=cursor,
                                                offset=int(offset) if offset else None, limit=5)
                next_page_success = bool(next_page)

        # åˆ†æç»“æœ
        result = {
            'success': True,
            'first_page_feeds': len(first_page.get('data', [])),
            'has_paging': has_paging,
            'has_next': has_next,
            'is_end': is_end,
            'next_page_success': next_page_success,
            'session_id': first_page.get('session', {}).get('id', ''),
            'lazyload_supported': has_paging and has_next and not is_end,
            'continuous_request_supported': next_page_success
        }

        logger.info("ğŸ“Š æ‡’åŠ è½½èƒ½åŠ›æµ‹è¯•ç»“æœ:")
        for key, value in result.items():
            logger.info(f"  {key}: {value}")

        return result

    def save_feeds_data(self, feeds_data: List[Dict], filename: str):
        """ä¿å­˜feedsæ•°æ®åˆ°æ–‡ä»¶"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(feeds_data, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… feedsæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜feedsæ•°æ®å¤±è´¥: {e}")
            return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸš€ çŸ¥ä¹APIæ‡’åŠ è½½æµ‹è¯•")

    crawler = ZhihuLazyLoadCrawler()

    # æµ‹è¯•é—®é¢˜ID - ä½¿ç”¨ä¸€ä¸ªç­”æ¡ˆè¾ƒå¤šçš„é—®é¢˜
    test_question_id = "19551593"  # è¿™ä¸ªé—®é¢˜åº”è¯¥æœ‰æ›´å¤šç­”æ¡ˆ

    # æµ‹è¯•æ‡’åŠ è½½èƒ½åŠ›
    logger.info("\n" + "="*60)
    logger.info("ğŸ§ª æ‡’åŠ è½½èƒ½åŠ›æµ‹è¯•")
    logger.info("="*60)

    ability_result = crawler.test_lazyload_ability(test_question_id)

    if not ability_result['success']:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {ability_result.get('error')}")
        return

    if ability_result['lazyload_supported']:
        logger.info("âœ… çŸ¥ä¹æ”¯æŒæ‡’åŠ è½½ï¼")

        if ability_result['continuous_request_supported']:
            logger.info("âœ… æ”¯æŒè¿ç»­è¯·æ±‚fetchæ–‡ä»¶ï¼")

            # æ‰§è¡Œå®Œæ•´æ‡’åŠ è½½æµ‹è¯•
            logger.info("\n" + "="*60)
            logger.info("ğŸš€ æ‰§è¡Œå®Œæ•´æ‡’åŠ è½½çˆ¬å–")
            logger.info("="*60)

            all_feeds = crawler.crawl_all_feeds_lazyload(test_question_id, max_feeds=50)

            if all_feeds:
                # æå–ç­”æ¡ˆ
                answers = crawler.extract_answers_from_feeds(all_feeds)

                # ä¿å­˜æ•°æ®
                crawler.save_feeds_data(all_feeds, 'lazyload_feeds_test.json')

                logger.info("ğŸ‰ æ‡’åŠ è½½æµ‹è¯•æˆåŠŸ!")
                logger.info(f"ğŸ“Š æ€»feedsæ•°: {len(all_feeds)}")
                logger.info(f"ğŸ“ æå–ç­”æ¡ˆæ•°: {len(answers)}")

                if answers:
                    logger.info("ğŸ“‹ ç¤ºä¾‹ç­”æ¡ˆä¿¡æ¯:")
                    for i, answer in enumerate(answers[:3], 1):
                        logger.info(f"  ç­”æ¡ˆ{i}: {answer['author']} - {answer['voteup_count']}èµ")
            else:
                logger.warning("âš ï¸ æœªè·å–åˆ°feedsæ•°æ®")

        else:
            logger.warning("âš ï¸ ä¸æ”¯æŒè¿ç»­è¯·æ±‚ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")

    else:
        logger.warning("âš ï¸ ä¸æ”¯æŒæ‡’åŠ è½½æˆ–å·²åˆ°è¾¾æ•°æ®æœ«å°¾")


if __name__ == "__main__":
    main()
