#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥ä¹APIæ‡’åŠ è½½çˆ¬è™«
æ”¯æŒå®Œæ•´çš„åˆ†é¡µå’Œè¿ç»­è¯·æ±‚fetchæ–‡ä»¶
"""

import requests
import json
import time
import re
from urllib.parse import urlparse, parse_qs
from typing import List, Dict, Optional, Tuple, Any
from loguru import logger
from pathlib import Path
import argparse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import WebDriverException
from config import ZhihuConfig
import pickle


class ZhihuLazyLoadCrawler:
    """çŸ¥ä¹æ‡’åŠ è½½APIçˆ¬è™«"""

    def __init__(self):
        self.session = requests.Session()
        self.base_headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin'
        }
        self.session.headers.update(self.base_headers)
        self.load_cookies()

        # APIé…ç½®
        self.api_base_url = 'https://www.zhihu.com/api/v4/questions'
        self.feeds_include_params = (
            'data[*].is_normal,admin_closed_comment,reward_info,is_collapsed,'
            'annotation_action,annotation_detail,collapse_reason,is_sticky,'
            'collapsed_by,suggest_edit,comment_count,can_comment,content,'
            'editable_content,attachment,voteup_count,reshipment_settings,'
            'comment_permission,created_time,updated_time,review_info,'
            'relevant_info,question,excerpt,is_labeled,paid_info,'
            'paid_info_content,reaction_instruction,relationship.is_authorized,'
            'is_author,voting,is_thanked,is_nothelp;'
            'data[*].author.follower_count,vip_info,kvip_info,badge[*].topics;'
            'data[*].settings.table_of_content.enabled'
        )

    def load_cookies(self):
        """åŠ è½½cookies"""
        try:
            cookie_file = Path('cookies/zhihu_cookies.json')
            with open(cookie_file, 'r', encoding='utf-8') as f:
                cookies_data = json.load(f)
                for cookie in cookies_data:
                    self.session.cookies.set(
                        cookie['name'],
                        cookie['value'],
                        domain=cookie.get('domain', '.zhihu.com'),
                        path=cookie.get('path', '/')
                    )
                logger.info(f"æˆåŠŸåŠ è½½cookiesï¼Œå…±{len(cookies_data)}ä¸ª")
                return True
        except Exception as e:
            logger.error(f"åŠ è½½cookieså¤±è´¥: {e}")
            return False

    def extract_question_id(self, question_url: str) -> Optional[str]:
        """ä»é—®é¢˜URLä¸­æå–é—®é¢˜ID"""
        try:
            if '/question/' in question_url:
                parts = question_url.split('/question/')
                if len(parts) > 1:
                    question_id = parts[1].split('/')[0].split('?')[0]
                    return question_id
            return None
        except Exception as e:
            logger.error(f"æå–é—®é¢˜IDå¤±è´¥: {e}")
            return None
        
    def crawl_feeds_with_browser_headers(self, question_id: str, max_pages: int = 10,
                                       session_id: str = None, 
                                       x_zse_96: str = None,
                                       x_zst_81: str = None) -> List[Dict]:
        """ä½¿ç”¨æµè§ˆå™¨headersè¿ç»­è·å–å¤šé¡µfeedsæ•°æ®
        
        æ¨¡æ‹Ÿæµè§ˆå™¨çš„è¿ç»­fetchè¯·æ±‚ï¼Œè·å–å®Œæ•´çš„é—®ç­”å›ç­”åˆ—è¡¨
        
        Args:
            question_id: é—®é¢˜ID
            max_pages: æœ€å¤§é¡µæ•°
            session_id: ä¼šè¯ID
            x_zse_96: çŸ¥ä¹åçˆ¬è™«å‚æ•°
            x_zst_81: çŸ¥ä¹åçˆ¬è™«å‚æ•°
            
        Returns:
            æ‰€æœ‰è·å–åˆ°çš„feedsæ•°æ®åˆ—è¡¨
        """
        all_feeds = []
        current_cursor = None
        current_offset = 1  # ä»offset=1å¼€å§‹ï¼Œå¦‚ç”¨æˆ·ç¤ºä¾‹
        page_count = 0
        
        logger.info(f"ğŸš€ å¼€å§‹ä½¿ç”¨æµè§ˆå™¨headersçˆ¬å–é—®é¢˜ {question_id} çš„feedsæ•°æ®")
        
        while page_count < max_pages:
            logger.info(f"ğŸ“– æ­£åœ¨è·å–ç¬¬ {page_count + 1} é¡µæ•°æ®...")
            
            # è·å–å½“å‰é¡µæ•°æ®
            page_data = self.fetch_feeds_with_browser_headers(
                question_id=question_id,
                cursor=current_cursor,
                offset=current_offset if current_cursor is None else None,
                limit=5,
                session_id=session_id,
                x_zse_96=x_zse_96,
                x_zst_81=x_zst_81
            )
            
            if not page_data:
                logger.warning(f"âš ï¸ ç¬¬ {page_count + 1} é¡µæ•°æ®è·å–å¤±è´¥ï¼Œåœæ­¢çˆ¬å–")
                break
                
            # æå–feedsæ•°æ®
            feeds_data = page_data.get('data', [])
            if not feeds_data:
                logger.info(f"ğŸ“„ ç¬¬ {page_count + 1} é¡µæ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                break
                
            all_feeds.extend(feeds_data)
            logger.info(f"âœ… ç¬¬ {page_count + 1} é¡µè·å–åˆ° {len(feeds_data)} ä¸ªfeedé¡¹ï¼Œç´¯è®¡ {len(all_feeds)} ä¸ª")
            
            # æ£€æŸ¥åˆ†é¡µä¿¡æ¯
            paging = page_data.get('paging', {})
            is_end = paging.get('is_end', True)
            
            if is_end:
                logger.info(f"ğŸ“„ å·²åˆ°è¾¾æœ€åä¸€é¡µï¼Œåœæ­¢çˆ¬å–")
                break
                
            # è·å–ä¸‹ä¸€é¡µçš„cursor
            next_url = paging.get('next')
            if next_url:
                # è§£ænext URLè·å–cursor
                parsed_next = self.parse_next_url(next_url)
                current_cursor = parsed_next.get('cursor')
                current_offset = parsed_next.get('offset')
                
                if current_cursor:
                    logger.info(f"ğŸ”— è·å–åˆ°ä¸‹ä¸€é¡µcursor: {current_cursor[:20]}...")
                elif current_offset:
                    logger.info(f"ğŸ”— è·å–åˆ°ä¸‹ä¸€é¡µoffset: {current_offset}")
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œåœæ­¢çˆ¬å–")
                break
                
            page_count += 1
            
            # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
            if page_count < max_pages:
                time.sleep(2)
                
        logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å– {len(all_feeds)} ä¸ªfeedé¡¹ï¼Œ{page_count + 1} é¡µæ•°æ®")
        return all_feeds
        
    def extract_answer_details_from_feeds(self, feeds_data: List[Dict]) -> List[Dict]:
        """ä»feedsæ•°æ®ä¸­æå–å›ç­”è¯¦æƒ…
        
        Args:
            feeds_data: feedsæ•°æ®åˆ—è¡¨
            
        Returns:
            å›ç­”è¯¦æƒ…åˆ—è¡¨
        """
        answers = []
        
        for feed in feeds_data:
            try:
                # æ£€æŸ¥feedç±»å‹
                feed_type = feed.get('type', '')
                if feed_type != 'answer':
                    continue
                    
                # æå–å›ç­”åŸºæœ¬ä¿¡æ¯
                answer_id = feed.get('id', '')
                content = feed.get('content', '')
                excerpt = feed.get('excerpt', '')
                
                # æå–ä½œè€…ä¿¡æ¯
                author = feed.get('author', {})
                author_name = author.get('name', '')
                author_url_token = author.get('url_token', '')
                author_headline = author.get('headline', '')
                author_follower_count = author.get('follower_count', 0)
                
                # æå–VIPä¿¡æ¯
                vip_info = author.get('vip_info', {})
                is_vip = vip_info.get('is_vip', False)
                
                # æå–ç»Ÿè®¡ä¿¡æ¯
                voteup_count = feed.get('voteup_count', 0)
                comment_count = feed.get('comment_count', 0)
                
                # æå–æ—¶é—´ä¿¡æ¯
                created_time = feed.get('created_time', 0)
                updated_time = feed.get('updated_time', 0)
                
                # æå–é—®é¢˜ä¿¡æ¯
                question = feed.get('question', {})
                question_id = question.get('id', '')
                question_title = question.get('title', '')
                
                # æå–æŠ•ç¥¨ä¿¡æ¯
                voting = feed.get('voting', 0)
                is_thanked = feed.get('is_thanked', False)
                
                answer_detail = {
                    'answer_id': answer_id,
                    'question_id': question_id,
                    'question_title': question_title,
                    'content': content,
                    'excerpt': excerpt,
                    'author': {
                        'name': author_name,
                        'url_token': author_url_token,
                        'headline': author_headline,
                        'follower_count': author_follower_count,
                        'is_vip': is_vip
                    },
                    'stats': {
                        'voteup_count': voteup_count,
                        'comment_count': comment_count,
                        'voting': voting,
                        'is_thanked': is_thanked
                    },
                    'timestamps': {
                        'created_time': created_time,
                        'updated_time': updated_time
                    },
                    'raw_feed': feed  # ä¿ç•™åŸå§‹æ•°æ®ä»¥å¤‡éœ€è¦
                }
                
                answers.append(answer_detail)
                
            except Exception as e:
                logger.error(f"è§£æfeedæ•°æ®æ—¶å‡ºé”™: {e}")
                continue
                
        logger.info(f"ğŸ“Š ä» {len(feeds_data)} ä¸ªfeedä¸­æå–åˆ° {len(answers)} ä¸ªå›ç­”è¯¦æƒ…")
        return answers

    def parse_next_url(self, next_url: str) -> Dict:
        """è§£æä¸‹ä¸€é¡µURLä¸­çš„å‚æ•°"""
        try:
            if not next_url:
                return {}

            parsed_url = urlparse(next_url)
            params = parse_qs(parsed_url.query)

            # æå–å…³é”®å‚æ•°
            result = {}
            for key, values in params.items():
                if values:
                    result[key] = values[0] if len(values) == 1 else values

            return result
        except Exception as e:
            logger.error(f"è§£ænext URLå¤±è´¥: {e}")
            return {}

    def build_feeds_url(self, question_id: str, cursor: str = None,
                       offset: int = None, limit: int = 20) -> str:
        """æ„å»ºfeeds API URL"""
        url = f"{self.api_base_url}/{question_id}/feeds"

        params = {
            'include': self.feeds_include_params,
            'limit': str(limit),
            'order': 'default',
            'ws_qiangzhisafe': '0',
            'platform': 'desktop'
        }

        # æ·»åŠ cursoræˆ–offsetï¼ˆä¼˜å…ˆä½¿ç”¨cursorï¼‰
        if cursor:
            params['cursor'] = cursor
        elif offset is not None:
            params['offset'] = str(offset)

        # æ‰‹åŠ¨æ„å»ºURLé¿å…ç¼–ç é—®é¢˜
        param_str = '&'.join([f"{k}={v}" for k, v in params.items()])
        return f"{url}?{param_str}"

    def fetch_feeds_with_browser_headers(self, question_id: str, cursor: str = None, 
                                       offset: int = None, limit: int = 5, 
                                       session_id: str = None, 
                                       x_zse_93: str = "101_3_3.0",
                                       x_zse_96: str = None,
                                       x_zst_81: str = None) -> Optional[Dict]:
        """ä½¿ç”¨æµè§ˆå™¨å®Œæ•´headersè·å–feedsæ•°æ®
        
        åŸºäºæµè§ˆå™¨fetchè¯·æ±‚çš„å®Œæ•´å®ç°ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„åçˆ¬è™«headers
        
        Args:
            question_id: é—®é¢˜ID
            cursor: æ¸¸æ ‡å‚æ•°ï¼Œç”¨äºåˆ†é¡µ
            offset: åç§»é‡å‚æ•°
            limit: æ¯é¡µæ•°é‡ï¼Œé»˜è®¤5
            session_id: ä¼šè¯ID
            x_zse_93: çŸ¥ä¹åçˆ¬è™«å‚æ•°
            x_zse_96: çŸ¥ä¹åçˆ¬è™«å‚æ•°
            x_zst_81: çŸ¥ä¹åçˆ¬è™«å‚æ•°
            
        Returns:
            APIå“åº”æ•°æ®æˆ–None
        """
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                # æ„å»ºå®Œæ•´çš„URL
                url = self._build_browser_feeds_url(
                    question_id, cursor, offset, limit, session_id
                )
                
                logger.info(f"ä½¿ç”¨æµè§ˆå™¨headersè¯·æ±‚feeds API (å°è¯• {attempt + 1}/{max_retries})")
                logger.debug(f"è¯·æ±‚URL: {url}")
                
                # æ„å»ºå®Œæ•´çš„æµè§ˆå™¨headers
                headers = {
                    "accept": "*/*",
                    "accept-language": "en,zh-CN;q=0.9,zh;q=0.8,de;q=0.7,zh-TW;q=0.6",
                    "priority": "u=1, i",
                    "sec-ch-ua": '"Not;A=Brand";v="99", "Google Chrome";v="139", "Chromium";v="139"',
                    "sec-ch-ua-mobile": "?0",
                    "sec-ch-ua-platform": '"macOS"',
                    "sec-fetch-dest": "empty",
                    "sec-fetch-mode": "cors",
                    "sec-fetch-site": "same-origin",
                    "x-requested-with": "fetch",
                    "x-zse-93": x_zse_93,
                    "Referer": f"https://www.zhihu.com/question/{question_id}",
                    "Referrer-Policy": "strict-origin-when-cross-origin"
                }
                
                # æ·»åŠ å¯é€‰çš„åçˆ¬è™«headers
                if x_zse_96:
                    headers["x-zse-96"] = x_zse_96
                if x_zst_81:
                    headers["x-zst-81"] = x_zst_81
                    
                # å‘é€è¯·æ±‚
                response = self.session.get(url, headers=headers, timeout=30)
                
                if response.status_code == 200:
                    data = response.json()
                    feeds_data = data.get('data', [])
                    logger.info(f"âœ… è·å–åˆ° {len(feeds_data)} ä¸ªfeedé¡¹")
                    
                    # è®°å½•åˆ†é¡µä¿¡æ¯
                    paging = data.get('paging', {})
                    if paging:
                        logger.info(f"ğŸ“„ åˆ†é¡µä¿¡æ¯: is_end={paging.get('is_end', False)}, next={paging.get('next', 'N/A')}")
                    
                    return data
                    
                elif response.status_code == 403:
                    logger.warning(f"âŒ APIè®¿é—®è¢«æ‹’ç»: {response.status_code}")
                    try:
                        error_data = response.json()
                        if 'error' in error_data:
                            logger.warning(f"é”™è¯¯ä¿¡æ¯: {error_data['error'].get('message', 'Unknown error')}")
                    except:
                        logger.warning(f"å“åº”å†…å®¹: {response.text[:200]}")
                    return None
                    
                else:
                    logger.warning(f"âš ï¸ APIè¿”å›å¼‚å¸¸çŠ¶æ€: {response.status_code}")
                    logger.debug(f"å“åº”å†…å®¹: {response.text[:200]}")
                    if attempt == max_retries - 1:
                        return None
                        
            except requests.exceptions.RequestException as e:
                logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return None
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æå¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                logger.debug(f"å“åº”å†…å®¹: {response.text[:200] if 'response' in locals() else 'N/A'}")
                if attempt == max_retries - 1:
                    return None
            except Exception as e:
                logger.error(f"æœªçŸ¥é”™è¯¯ (å°è¯• {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return None
                    
            # å¤±è´¥åç­‰å¾…é‡è¯•
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
                
        return None
        
    def _build_browser_feeds_url(self, question_id: str, cursor: str = None,
                               offset: int = None, limit: int = 5, 
                               session_id: str = None) -> str:
        """æ„å»ºæµè§ˆå™¨é£æ ¼çš„feeds API URL
        
        Args:
            question_id: é—®é¢˜ID
            cursor: æ¸¸æ ‡å‚æ•°
            offset: åç§»é‡å‚æ•°  
            limit: æ¯é¡µæ•°é‡
            session_id: ä¼šè¯ID
            
        Returns:
            å®Œæ•´çš„API URL
        """
        url = f"{self.api_base_url}/{question_id}/feeds"
        
        # ä½¿ç”¨ä¸æµè§ˆå™¨fetchç›¸åŒçš„includeå‚æ•°
        include_params = (
            "data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2C"
            "annotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2C"
            "collapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2C"
            "editable_content%2Cattachment%2Cvoteup_count%2Creshipment_settings%2C"
            "comment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2C"
            "relevant_info%2Cquestion%2Cexcerpt%2Cis_labeled%2Cpaid_info%2C"
            "paid_info_content%2Creaction_instruction%2Crelationship.is_authorized%2C"
            "is_author%2Cvoting%2Cis_thanked%2Cis_nothelp%3Bdata%5B%2A%5D.author.follower_count%2C"
            "vip_info%2Ckvip_info%2Cbadge%5B%2A%5D.topics%3Bdata%5B%2A%5D.settings.table_of_content.enabled"
        )
        
        params = {
            'include': include_params,
            'limit': str(limit),
            'order': 'default',
            'platform': 'desktop',
            'ws_qiangzhisafe': '0'
        }
        
        # æ·»åŠ cursoræˆ–offset
        if cursor:
            params['cursor'] = cursor
        elif offset is not None:
            params['offset'] = str(offset)
            
        # æ·»åŠ session_id
        if session_id:
            params['session_id'] = session_id
            
        # æ„å»ºURL
        param_str = '&'.join([f"{k}={v}" for k, v in params.items()])
        return f"{url}?{param_str}"

    def fetch_feeds_page(self, question_id: str, cursor: str = None,
                        offset: int = None, limit: int = 20) -> Optional[Dict]:
        """è·å–feedsé¡µé¢æ•°æ®"""
        max_retries = 3

        for attempt in range(max_retries):
            try:
                url = self.build_feeds_url(question_id, cursor, offset, limit)
                logger.info(f"è¯·æ±‚feeds API (å°è¯• {attempt + 1}/{max_retries})")
                logger.debug(f"è¯·æ±‚URL: {url}")

                # è®¾ç½®headers
                headers = self.base_headers.copy()
                headers['Referer'] = f'https://www.zhihu.com/question/{question_id}'
                headers['X-Requested-With'] = 'fetch'

                response = self.session.get(url, headers=headers, timeout=30)

                if response.status_code == 200:
                    data = response.json()

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                    feeds_data = data.get('data', [])
                    logger.info(f"âœ… è·å–åˆ° {len(feeds_data)} ä¸ªfeedé¡¹")

                    # æ£€æŸ¥sessionä¿¡æ¯
                    session_info = data.get('session', {})
                    session_id = session_info.get('id', '')
                    if session_id:
                        logger.info(f"ğŸ”‘ Session ID: {session_id}")

                    return data

                elif response.status_code == 403:
                    logger.warning(f"âŒ APIè®¿é—®è¢«æ‹’ç»: {response.status_code}")
                    error_data = response.json()
                    if 'error' in error_data:
                        logger.warning(f"é”™è¯¯ä¿¡æ¯: {error_data['error'].get('message', 'Unknown error')}")
                    return None

                else:
                    logger.warning(f"âš ï¸ APIè¿”å›å¼‚å¸¸çŠ¶æ€: {response.status_code}")
                    if attempt == max_retries - 1:
                        return None

            except requests.exceptions.RequestException as e:
                logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return None
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æå¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return None
            except Exception as e:
                logger.error(f"æœªçŸ¥é”™è¯¯ (å°è¯• {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return None

            # å¤±è´¥åç­‰å¾…åé‡è¯•
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)

        return None

    def crawl_all_feeds_lazyload(self, question_id: str, max_feeds: int = None) -> List[Dict]:
        """ä½¿ç”¨æ‡’åŠ è½½æ–¹å¼çˆ¬å–æ‰€æœ‰feedsæ•°æ®"""
        all_feeds = []
        page_count = 0
        cursor = None
        offset = 0

        logger.info(f"ğŸš€ å¼€å§‹æ‡’åŠ è½½çˆ¬å–é—®é¢˜ {question_id} çš„feeds")
        logger.info(f"ğŸ“Š æœ€å¤§feedsé™åˆ¶: {max_feeds if max_feeds else 'æ— é™åˆ¶'}")

        while True:
            page_count += 1
            logger.info(f"\nğŸ“„ è·å–ç¬¬ {page_count} é¡µæ•°æ®")
            logger.info(f"ğŸ”„ Cursor: {cursor}, Offset: {offset}")

            # è·å–å½“å‰é¡µæ•°æ®
            page_data = self.fetch_feeds_page(question_id, cursor=cursor, offset=offset, limit=20)
            if not page_data:
                logger.error(f"âŒ è·å–ç¬¬ {page_count} é¡µæ•°æ®å¤±è´¥")
                break

            # è§£æfeedsæ•°æ®
            feeds_data = page_data.get('data', [])
            logger.info(f"ğŸ“¦ æœ¬é¡µè·å–åˆ° {len(feeds_data)} ä¸ªfeedé¡¹")

            # æ·»åŠ åˆ°æ€»é›†åˆ
            all_feeds.extend(feeds_data)

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶
            if max_feeds and len(all_feeds) >= max_feeds:
                logger.info(f"âœ… å·²è¾¾åˆ°æœ€å¤§feedsæ•°é‡é™åˆ¶: {max_feeds}")
                all_feeds = all_feeds[:max_feeds]
                break

            # è§£æåˆ†é¡µä¿¡æ¯
            paging = page_data.get('paging', {})
            is_end = paging.get('is_end', True)
            next_url = paging.get('next', '')

            logger.info(f"ğŸ” åˆ†é¡µä¿¡æ¯: is_end={is_end}")

            if is_end:
                logger.info(f"ğŸ¯ å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                break

            # è§£æä¸‹ä¸€é¡µå‚æ•°
            next_params = self.parse_next_url(next_url)

            # æ›´æ–°cursorå’Œoffset
            if 'cursor' in next_params:
                cursor = next_params['cursor']
                logger.info(f"ğŸ“Œ æ›´æ–°cursor: {cursor}")
            elif 'offset' in next_params:
                offset = int(next_params['offset'])
                logger.info(f"ğŸ“Œ æ›´æ–°offset: {offset}")
            else:
                # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ä¸‹ä¸€é¡µå‚æ•°ï¼Œå¢åŠ offset
                offset += len(feeds_data)
                logger.info(f"ğŸ“Œ é€’å¢offset: {offset}")

            # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)

            # å®‰å…¨æ£€æŸ¥
            if page_count >= 100:  # æœ€å¤š100é¡µ
                logger.warning(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶: {page_count}")
                break

        logger.info(f"\nğŸ‰ æ‡’åŠ è½½çˆ¬å–å®Œæˆ!")
        logger.info(f"ğŸ“Š æ€»å…±è·å–åˆ° {len(all_feeds)} ä¸ªfeeds")
        logger.info(f"ğŸ“„ å…±è¯·æ±‚äº† {page_count} é¡µ")

        return all_feeds

    def extract_answers_from_feeds(self, feeds_data: List[Dict]) -> List[Dict]:
        """ä»feedsæ•°æ®ä¸­æå–ç­”æ¡ˆï¼ˆrequestsæ¨¡å¼ï¼‰"""
        answers = []
        for feed_item in feeds_data:
            try:
                if feed_item.get('target_type') != 'answer':
                    continue
                target = feed_item.get('target', {})
                if not target:
                    continue
                answer_info = {
                    'answer_id': str(target.get('id', '')),
                    'content': target.get('content', ''),
                    'excerpt': target.get('excerpt', ''),
                    'voteup_count': target.get('voteup_count', 0),
                    'comment_count': target.get('comment_count', 0),
                    'created_time': target.get('created_time', 0),
                    'updated_time': target.get('updated_time', 0),
                    'author': target.get('author', {}).get('name', ''),
                    'author_url_token': target.get('author', {}).get('url_token', ''),
                    'question_id': target.get('question', {}).get('id', ''),
                    'question_title': target.get('question', {}).get('title', ''),
                    'is_author': target.get('relationship', {}).get('is_author', False)
                }
                answers.append(answer_info)
            except Exception as e:
                logger.warning(f"è§£æfeedé¡¹å¤±è´¥: {e}")
                continue
        logger.info(f"ğŸ“ ä»feedsä¸­æå–åˆ° {len(answers)} ä¸ªç­”æ¡ˆ")
        return answers

    def test_lazyload_ability(self, question_id: str) -> Dict:
        """æµ‹è¯•æ‡’åŠ è½½èƒ½åŠ›ï¼ˆrequestsæ¨¡å¼ï¼‰"""
        logger.info(f"ğŸ§ª æµ‹è¯•é—®é¢˜ {question_id} çš„æ‡’åŠ è½½èƒ½åŠ›")
        first_page = self.fetch_feeds_page(question_id, limit=5)
        if not first_page:
            return {'success': False, 'error': 'æ— æ³•è·å–ç¬¬ä¸€é¡µæ•°æ®'}
        paging = first_page.get('paging', {})
        has_paging = bool(paging)
        has_next = bool(paging.get('next'))
        is_end = paging.get('is_end', True)
        next_page_success = False
        if not is_end and has_next:
            next_params = self.parse_next_url(paging['next'])
            cursor = next_params.get('cursor')
            offset = next_params.get('offset')
            if cursor or offset:
                next_page = self.fetch_feeds_page(
                    question_id,
                    cursor=cursor,
                    offset=int(offset) if offset else None,
                    limit=5
                )
                next_page_success = bool(next_page)
        result = {
            'success': True,
            'first_page_feeds': len(first_page.get('data', [])),
            'has_paging': has_paging,
            'has_next': has_next,
            'is_end': is_end,
            'next_page_success': next_page_success,
            'session_id': first_page.get('session', {}).get('id', ''),
            'lazyload_supported': has_paging and has_next and not is_end,
            'continuous_request_supported': next_page_success
        }
        logger.info("ğŸ“Š æ‡’åŠ è½½èƒ½åŠ›æµ‹è¯•ç»“æœ:")
        for key, value in result.items():
            logger.info(f"  {key}: {value}")
        return result

    def save_feeds_data(self, feeds_data: List[Dict], filename: str) -> bool:
        """ä¿å­˜feedsæ•°æ®åˆ°æ–‡ä»¶ï¼ˆè‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•ï¼‰"""
        try:
            try:
                p = Path(filename)
                if p.parent and str(p.parent) not in ("", "."):
                    p.parent.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                logger.debug(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥ï¼ˆç»§ç»­å°è¯•ä¿å­˜æ–‡ä»¶ï¼‰: {e}")
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(feeds_data, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… feedsæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜feedsæ•°æ®å¤±è´¥: {e}")
            return False

    def extract_answers_from_feeds(self, feeds_data: List[Dict]) -> List[Dict]:
        """ä»feedsæ•°æ®ä¸­æå–ç­”æ¡ˆï¼ˆbrowseræ¨¡å¼è¾…åŠ©ï¼‰"""
        answers = []
        for feed_item in feeds_data:
            try:
                if feed_item.get('target_type') != 'answer':
                    continue
                target = feed_item.get('target', {})
                if not target:
                    continue
                answer_info = {
                    'answer_id': str(target.get('id', '')),
                    'content': target.get('content', ''),
                    'excerpt': target.get('excerpt', ''),
                    'voteup_count': target.get('voteup_count', 0),
                    'comment_count': target.get('comment_count', 0),
                    'created_time': target.get('created_time', 0),
                    'updated_time': target.get('updated_time', 0),
                    'author': target.get('author', {}).get('name', ''),
                    'author_url_token': target.get('author', {}).get('url_token', ''),
                    'question_id': target.get('question', {}).get('id', ''),
                    'question_title': target.get('question', {}).get('title', ''),
                    'is_author': target.get('relationship', {}).get('is_author', False)
                }
                answers.append(answer_info)
            except Exception as e:
                logger.warning(f"è§£æfeedé¡¹å¤±è´¥: {e}")
                continue
        logger.info(f"ğŸ“ ä»feedsä¸­æå–åˆ° {len(answers)} ä¸ªç­”æ¡ˆ")
        return answers

# =============== æ–°å¢ï¼šæµè§ˆå™¨ä¸Šä¸‹æ–‡æŠ“å–æ–¹æ¡ˆï¼ˆæ–¹æ¡ˆAï¼‰ ===============
class BrowserFeedsCrawler:
    """ä½¿ç”¨æµè§ˆå™¨ä¸Šä¸‹æ–‡è§¦å‘å‰ç«¯fetchå¹¶é€šè¿‡CDPæŠ“å–feedså“åº”"""

    def __init__(self, headless: bool = None):
        self.config = ZhihuConfig
        self.headless = self.config.HEADLESS if headless is None else headless
        self.driver = None
        self._init_driver()

    def _init_driver(self):
        """åˆå§‹åŒ–WebDriverï¼Œä¼˜å…ˆä½¿ç”¨ç³»ç»ŸChromeç›®å½•ï¼Œå¤±è´¥æ—¶ä½¿ç”¨ä¸´æ—¶ç›®å½•"""
        # ç»Ÿä¸€é…ç½®Optionsçš„å‡½æ•°
        def configure_options(use_temp_dir=False, temp_dir_path=None):
            options = Options()
            if self.headless:
                options.add_argument('--headless=new')
            
            # çª—å£å¤§å°
            if isinstance(self.config.WINDOW_SIZE, tuple) and len(self.config.WINDOW_SIZE) == 2:
                w, h = self.config.WINDOW_SIZE
                options.add_argument(f"--window-size={w},{h}")
            elif isinstance(self.config.WINDOW_SIZE, str):
                options.add_argument(f"--window-size={self.config.WINDOW_SIZE}")
            
            # åŸºç¡€é…ç½®
            options.add_argument('--disable-gpu')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            
            # è§„é¿è‡ªåŠ¨åŒ–ç‰¹å¾
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            
            # è®¾ç½®è¯­è¨€å’ŒUA
            try:
                ua = self.config.USER_AGENTS[0]
                options.add_argument(f"--user-agent={ua}")
            except Exception:
                pass
            options.add_argument('--lang=zh-CN,zh;q=0.9,en;q=0.8')
            
            # æ€§èƒ½æ—¥å¿—
            try:
                options.set_capability('goog:loggingPrefs', {'performance': 'ALL'})
            except Exception:
                pass
            
            # ç”¨æˆ·æ•°æ®ç›®å½•é…ç½®
            if use_temp_dir and temp_dir_path:
                options.add_argument(f"--user-data-dir={str(temp_dir_path)}")
                logger.info(f"ä½¿ç”¨ä¸´æ—¶Chromeé…ç½®ç›®å½•: {temp_dir_path}")
            else:
                user_data_dir = Path.home() / 'Library' / 'Application Support' / 'Google' / 'Chrome'
                if user_data_dir.exists():
                    options.add_argument(f"--user-data-dir={str(user_data_dir)}")
                    options.add_argument("--profile-directory=Default")
                    logger.info(f"å°è¯•ä½¿ç”¨æœ¬åœ°Chromeç”¨æˆ·æ•°æ®ç›®å½•: {user_data_dir}")
            
            return options

        def setup_webdriver_fingerprint(driver):
            """è®¾ç½®WebDriveræŒ‡çº¹éšè—"""
            try:
                driver.execute_cdp_cmd(
                    "Page.addScriptToEvaluateOnNewDocument",
                    {
                        "source": """
                        Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
                        Object.defineProperty(navigator, 'languages', { get: () => ['zh-CN', 'zh', 'en'] });
                        Object.defineProperty(navigator, 'platform', { get: () => 'MacIntel' });
                        window.chrome = { runtime: {} };
                        """
                    }
                )
                driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            except Exception:
                pass

        # ç¬¬ä¸€æ¬¡å°è¯•ï¼šä½¿ç”¨ç³»ç»ŸChromeç›®å½•
        try:
            options = configure_options(use_temp_dir=False)
            self.driver = webdriver.Chrome(options=options)
            setup_webdriver_fingerprint(self.driver)
            logger.info("âœ… æˆåŠŸåˆå§‹åŒ–Chromeï¼ˆä½¿ç”¨ç³»ç»Ÿç”¨æˆ·æ•°æ®ç›®å½•ï¼‰")
        except Exception as e:
            msg = str(e)
            if 'is already in use' in msg or 'user data directory is already in use' in msg:
                logger.warning("ç³»ç»ŸChromeç”¨æˆ·æ•°æ®ç›®å½•è¢«å ç”¨ï¼Œå›é€€åˆ°ä¸´æ—¶ç›®å½•...")
                # ç¬¬äºŒæ¬¡å°è¯•ï¼šä½¿ç”¨ä¸´æ—¶ç›®å½•
                try:
                    import tempfile
                    tmp_base = Path(tempfile.mkdtemp(prefix="zhihu_tmp_profile_"))
                    options = configure_options(use_temp_dir=True, temp_dir_path=tmp_base)
                    self.driver = webdriver.Chrome(options=options)
                    setup_webdriver_fingerprint(self.driver)
                    logger.info("âœ… æˆåŠŸåˆå§‹åŒ–Chromeï¼ˆä½¿ç”¨ä¸´æ—¶ç”¨æˆ·æ•°æ®ç›®å½•ï¼‰")
                except Exception as e2:
                    logger.warning(f"Selenium Managerå¯åŠ¨å¤±è´¥ï¼Œå°è¯•webdriver_manager: {e2}")
                    # ç¬¬ä¸‰æ¬¡å°è¯•ï¼šä½¿ç”¨webdriver_manager
                    try:
                        driver_path = ChromeDriverManager().install()
                        dp = Path(driver_path)
                        if dp.name.startswith("THIRD_PARTY_NOTICES"):
                            candidate = dp.parent / "chromedriver"
                            service = Service(str(candidate if candidate.exists() else dp))
                        else:
                            service = Service(str(dp))
                        self.driver = webdriver.Chrome(service=service, options=options)
                        setup_webdriver_fingerprint(self.driver)
                        logger.info("âœ… æˆåŠŸåˆå§‹åŒ–Chromeï¼ˆä½¿ç”¨webdriver_managerï¼‰")
                    except Exception as e3:
                        logger.error(f"åˆå§‹åŒ–Chromeå¤±è´¥: {e3}")
                        raise
            else:
                logger.warning(f"Selenium Managerå¯åŠ¨å¤±è´¥ï¼Œå°è¯•webdriver_manager: {e}")
                # ç›´æ¥å°è¯•webdriver_manager
                try:
                    driver_path = ChromeDriverManager().install()
                    dp = Path(driver_path)
                    if dp.name.startswith("THIRD_PARTY_NOTICES"):
                        candidate = dp.parent / "chromedriver"
                        service = Service(str(candidate if candidate.exists() else dp))
                    else:
                        service = Service(str(dp))
                    options = configure_options(use_temp_dir=False)
                    self.driver = webdriver.Chrome(service=service, options=options)
                    setup_webdriver_fingerprint(self.driver)
                    logger.info("âœ… æˆåŠŸåˆå§‹åŒ–Chromeï¼ˆä½¿ç”¨webdriver_managerï¼‰")
                except Exception as e2:
                    logger.error(f"åˆå§‹åŒ–Chromeå¤±è´¥: {e2}")
                    raise

        # å¯ç”¨Networkä»¥ä¾¿åç»­æŠ“body
        try:
            self.driver.execute_cdp_cmd('Network.enable', {})
        except Exception as e:
            logger.warning(f"å¯ç”¨CDP Networkå¤±è´¥: {e}")

    def close(self):
        try:
            if self.driver:
                self.driver.quit()
        except Exception:
            pass

    def _load_cookies_to_driver(self) -> bool:
        """ä»cookies/zhihu_cookies.jsonåŠ è½½åˆ°æµè§ˆå™¨ä¼šè¯"""
        cookie_file = Path(self.config.COOKIES_FILE)
        try:
            if not cookie_file.exists():
                logger.warning(f"æœªæ‰¾åˆ°Cookieæ–‡ä»¶: {cookie_file}")
                return False
            # å…ˆæ‰“å¼€ä¸»åŸŸï¼Œæ‰èƒ½è®¾ç½®cookie
            self.driver.get(self.config.BASE_URL)
            time.sleep(1)
            with open(cookie_file, 'r', encoding='utf-8') as f:
                cookies_data = json.load(f)
            loaded_cdp = 0
            loaded = 0
            for c in cookies_data:
                try:
                    name = c.get('name')
                    value = c.get('value')
                    if not name or value is None:
                        continue
                    domain_raw = c.get('domain') or '.zhihu.com'
                    domain_clean = domain_raw.lstrip('.')
                    path = c.get('path', '/')
                    secure = bool(c.get('secure', True))
                    # å¤„ç† sameSite
                    samesite_raw = c.get('sameSite') or c.get('samesite')
                    ss_map = {
                        'no_restriction': 'None',
                        'none': 'None',
                        'None': 'None',
                        'lax': 'Lax',
                        'Lax': 'Lax',
                        'strict': 'Strict',
                        'Strict': 'Strict',
                        'unspecified': None,
                        'unspecified_same_site': None
                    }
                    ss_val = ss_map.get(samesite_raw, samesite_raw)
                    # SameSite=None æ—¶å¿…é¡» secure=True
                    if ss_val == 'None' and not secure:
                        secure = True
                    http_only = c.get('httpOnly')
                    # è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
                    expiry = None
                    if c.get('expiry'):
                        expiry = int(c['expiry'])
                    elif c.get('expires'):
                        try:
                            expiry = int(c['expires'])
                        except Exception:
                            expiry = None
                    # ä¼˜å…ˆä½¿ç”¨CDPè®¾ç½®cookieï¼ˆå¯è®¾ç½®httpOnlyï¼‰
                    try:
                        params = {
                            'name': name,
                            'value': value,
                            'domain': domain_clean,
                            'path': path,
                            'secure': secure,
                        }
                        if expiry:
                            params['expires'] = expiry
                        if isinstance(http_only, bool):
                            params['httpOnly'] = http_only
                        if ss_val in {'Strict', 'Lax', 'None'}:
                            params['sameSite'] = ss_val
                        ok = self.driver.execute_cdp_cmd('Network.setCookie', params)
                        if ok and ok.get('success'):
                            loaded_cdp += 1
                            continue
                    except Exception as e_cdp:
                        logger.debug(f"CDPè®¾ç½®cookieå¤±è´¥ name={name} domain={domain_clean}: {e_cdp}")
                    # å›é€€ï¼šSelenium add_cookie
                    ck = {
                        'name': name,
                        'value': value,
                        'domain': domain_raw,
                        'path': path,
                        'secure': secure
                    }
                    if expiry:
                        ck['expiry'] = expiry
                    if ss_val in {'Strict', 'Lax', 'None'}:
                        ck['sameSite'] = ss_val
                    try:
                        self.driver.add_cookie(ck)
                        loaded += 1
                        continue
                    except Exception as e1:
                        try:
                            if isinstance(ck.get('domain'), str) and ck['domain'].startswith('.'):
                                ck2 = dict(ck)
                                ck2['domain'] = ck['domain'].lstrip('.')
                                self.driver.add_cookie(ck2)
                                loaded += 1
                                continue
                        except Exception as e2:
                            logger.debug(f"æ·»åŠ cookieå¤±è´¥ name={name} domain={ck.get('domain')}: {e1} | fallback: {e2}")
                            pass
                except Exception as e:
                    logger.debug(f"è·³è¿‡æ— æ³•è®¾ç½®çš„cookie {c.get('name')}: {e}")
                    continue
            logger.info(f"æˆåŠŸæ³¨å…¥ cookiesåˆ°æµè§ˆå™¨ï¼šCDP={loaded_cdp}ï¼ŒSelenium={loaded}")
            # åˆ·æ–°ä»¥ä¾¿cookieç”Ÿæ•ˆ
            self.driver.get(self.config.BASE_URL)
            time.sleep(1)
            return (loaded_cdp + loaded) > 0
        except Exception as e:
            logger.error(f"åŠ è½½æµè§ˆå™¨cookieså¤±è´¥: {e}")
            return False

    def _persist_cookies(self, driver=None) -> bool:
        """å°†å½“å‰æµè§ˆå™¨ä¼šè¯cookiesæŒä¹…åŒ–åˆ°é…ç½®è·¯å¾„(JSON)ä¸cacheç›®å½•(Pickle)"""
        try:
            drv = driver or self.driver
            if drv is None:
                logger.warning("æ— æ³•ä¿å­˜cookies: driveræœªåˆå§‹åŒ–")
                return False
            cookies = drv.get_cookies()
            if not isinstance(cookies, list) or not cookies:
                logger.warning("æ— æ³•ä¿å­˜cookies: æœªä»æµè§ˆå™¨è·å–åˆ°ä»»ä½•cookie")
                return False
            # ä¿å­˜JSON
            json_path = Path(self.config.COOKIES_FILE)
            json_path.parent.mkdir(parents=True, exist_ok=True)
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(cookies, f, ensure_ascii=False, indent=2)
            # å…¼å®¹ï¼šå†ä¿å­˜ä¸€ä»½pickleï¼Œä¾¿äºå…¶ä»–æ¨¡å—å¤ç”¨
            pkl_path = Path('cache/zhihu_cookies.pkl')
            pkl_path.parent.mkdir(parents=True, exist_ok=True)
            with open(pkl_path, 'wb') as pf:
                pickle.dump(cookies, pf)
            # ç®€è¦æç¤ºå…³é”®cookie
            z = next((c.get('value','') for c in cookies if c.get('name')=='z_c0'), '')
            logger.info(f"âœ… å·²ä¿å­˜ç™»å½•cookiesï¼šJSON->{json_path}ï¼ŒPKL->{pkl_path}ï¼ˆz_c0é•¿åº¦={len(z)}ï¼‰")
            return True
        except Exception as e:
            logger.warning(f"ä¿å­˜cookieså¤±è´¥: {e}")
            return False

    def _scroll_to_load(self, times: int = 1, pause: float = 2.0):
        for _ in range(times):
            try:
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            except WebDriverException:
                pass
            time.sleep(pause)

    def _soft_rollback(self, percent: float = 0.08, pause: float = 1.5):
        """è½»é‡å›æ»šï¼šä¸Šæ»šé¡µé¢é«˜åº¦çš„ä¸€å®šæ¯”ä¾‹åå†ä¸‹æ»šï¼Œè§¦å‘é‡æ–°æ‡’åŠ è½½"""
        try:
            current_height = self.driver.execute_script("return document.body.scrollHeight")
            current_scroll_position = self.driver.execute_script("return window.pageYOffset")
            delta = max(50, current_height * float(percent))
            new_scroll_position = max(0, current_scroll_position - delta)
            logger.info(f"â†©ï¸ è§¦å‘è½¯å›æ»šï¼šä¸Šæ»š {delta:.0f}px åˆ° {new_scroll_position:.0f}ï¼Œéšåå†æ¬¡ä¸‹æ»‘")
            self.driver.execute_script(f"window.scrollTo(0, {new_scroll_position});")
            time.sleep(pause)
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(max(1.0, pause - 0.2))
        except Exception as e:
            logger.debug(f"è½¯å›æ»šæ‰§è¡Œå¤±è´¥ï¼š{e}")

    def _collect_feeds_from_perf_logs(self, question_id: str, seen_ids: set) -> list:
        """ä»performanceæ—¥å¿—ä¸­æ”¶é›†æœ¬è½®æ–°å‡ºç°çš„feedså“åº”ä½“"""
        collected = []
        debug_sample = []
        try:
            logs = self.driver.get_log('performance')
        except Exception as e:
            logger.debug(f"è¯»å–performanceæ—¥å¿—å¤±è´¥: {e}")
            return collected
        for entry in logs:
            try:
                msg = json.loads(entry.get('message', '{}'))
                message = msg.get('message', {})
                method = message.get('method')
                if method != 'Network.responseReceived':
                    continue
                params = message.get('params', {})
                response = params.get('response', {})
                url: str = response.get('url', '')
                ctype = (response.get('headers', {}) or {}).get('content-type', '')
                # è®°å½•ä¸€äº›æ ·æœ¬URLç”¨äºè°ƒè¯•
                if ("zhihu.com" in url or "/api/" in url) and len(debug_sample) < 8:
                    debug_sample.append(f"{ctype} | {url}")
                # æ”¯æŒ feeds ä¸ answers ä»¥åŠæ›´å¹¿æ³›çš„é—®é¢˜APIè·¯å¾„
                base_match = (
                    f"/api/v4/questions/{question_id}/" in url or
                    f"/api/v5/questions/{question_id}/" in url or
                    f"/next/api/v4/questions/{question_id}/" in url or
                    f"/next-api/v4/questions/{question_id}/" in url or
                    f"/api/v4/questions/{question_id}?" in url
                )
                if not base_match:
                    continue
                request_id = params.get('requestId')
                if not request_id or request_id in seen_ids:
                    continue
                # æ‹‰å–body
                try:
                    body = self.driver.execute_cdp_cmd('Network.getResponseBody', {'requestId': request_id})
                    text = body.get('body', '')
                    if not text:
                        continue
                    data = json.loads(text)
                    collected.append((request_id, data))
                except Exception as e:
                    logger.debug(f"è·å–å“åº”ä½“å¤±è´¥ requestId={request_id}: {e}")
                    continue
            except Exception:
                continue
        if debug_sample:
            logger.debug("æ ·æœ¬å“åº”URL: \n" + "\n".join(debug_sample))
        return collected

    def _fetch_first_page_fallback(self, question_id: str) -> List[Dict[str, Any]]:
        """åœ¨é¡µé¢ä¸Šä¸‹æ–‡ç›´æ¥fetché¦–é¡µfeedsï¼Œä½œä¸ºå›é€€æ–¹æ¡ˆ"""
        try:
            helper = ZhihuLazyLoadCrawler()
            url = helper.build_feeds_url(question_id=question_id, cursor=None, offset=0, limit=20)
            js = (
                "return fetch(arguments[0], {credentials:'include'})"
                ".then(r => r.text())"
                ".then(t => t)"
                ".catch(e => JSON.stringify({__err: String(e && e.message || e)}));"
            )
            text = self.driver.execute_script(js, url)
            if not text:
                return []
            try:
                data = json.loads(text)
            except Exception:
                # å¯èƒ½è¿”å›HTMLæˆ–é”™è¯¯
                return []
            if isinstance(data, dict):
                items = data.get('data') or []
                if isinstance(items, list):
                    logger.info(f"å›é€€fetchè·å–åˆ°é¦–é¡µ items={len(items)}")
                    return items
            return []
        except Exception as e:
            logger.debug(f"å›é€€fetchå¤±è´¥: {e}")
            return []

    # æ–°å¢ï¼šæ‰‹åŠ¨ç™»å½•ä¿éšœæµç¨‹
    def _ensure_logged_in_manually(self, driver, timeout_sec: int = 180) -> bool:
        """
        å¼•å¯¼ç”¨æˆ·åœ¨å¯è§çš„æµè§ˆå™¨ä¸­å®ŒæˆçŸ¥ä¹ç™»å½•ï¼Œå¹¶ç­‰å¾…ç™»å½•æ€ç”Ÿæ•ˆã€‚
        è¿”å›æ˜¯å¦æ£€æµ‹åˆ°ç™»å½•æˆåŠŸï¼ˆä¾æ® z_c0 cookie çš„å½¢æ€åˆ¤æ–­ï¼‰ã€‚
        """
        try:
            logger.info(f"ğŸ” æ£€æµ‹åˆ°å¯èƒ½æœªç™»å½•ï¼Œå‡†å¤‡è¿›å…¥æ‰‹åŠ¨ç™»å½•æµç¨‹ï¼ˆæœ€é•¿ç­‰å¾… {timeout_sec}sï¼‰")
            # è®¿é—®é¦–é¡µï¼Œä¾¿äºç”¨æˆ·ç™»å½•
            try:
                driver.get("https://www.zhihu.com/")
            except Exception as e:
                logger.warning(f"æ‰“å¼€çŸ¥ä¹é¦–é¡µå¤±è´¥: {e}")
            start = time.time()
            last_report = -999
            while time.time() - start < timeout_sec:
                try:
                    cookies = {c.get('name'): c.get('value', '') for c in driver.get_cookies()}
                    z = cookies.get('z_c0', '')
                    # ç»éªŒï¼šæœ‰æ•ˆçš„ z_c0 ä¸€èˆ¬ä»¥ '2|' å¼€å¤´ä¸”é•¿åº¦è¾ƒé•¿ï¼ˆ>60ï¼‰
                    if z and (z.startswith('2|') or len(z) > 60) and 'v10' not in z:
                        logger.info(f"âœ… æ£€æµ‹åˆ°ç™»å½•cookie z_c0ï¼Œé•¿åº¦={len(z)}ï¼Œåˆ¤å®šå·²ç™»å½•")
                        try:
                            self._persist_cookies(driver)
                        except Exception as se:
                            logger.debug(f"ç™»å½•æˆåŠŸä½†ä¿å­˜cookiesæ—¶å‡ºç°é—®é¢˜: {se}")
                        return True
                except Exception as ie:
                    logger.debug(f"æ£€æŸ¥ç™»å½•cookieå¼‚å¸¸: {ie}")
                # æ¯5ç§’è¾“å‡ºä¸€æ¬¡å‰©ä½™æ—¶é—´æç¤º
                remain = int(timeout_sec - (time.time() - start))
                if remain // 5 != last_report // 5:
                    logger.info(f"è¯·åœ¨å·²æ‰“å¼€çš„æµè§ˆå™¨çª—å£ä¸­å®Œæˆç™»å½•ï¼ˆå‰©ä½™çº¦ {remain}sï¼‰â€¦â€¦")
                    last_report = remain
                time.sleep(1.5)
            logger.warning("æ‰‹åŠ¨ç™»å½•ç­‰å¾…è¶…æ—¶ï¼Œæœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„ z_c0 cookie")
            return False
        except Exception as e:
            logger.error(f"æ‰‹åŠ¨ç™»å½•æµç¨‹å¼‚å¸¸: {e}")
            return False

    def crawl_feeds_via_browser(self, question_id: str, max_scrolls: int = 6, pause: float = 2.5, stop_when_is_end: bool = True, expected_min_per_scroll: int = 8, verify_end_with_rollback: bool = True, rollback_percent: float = 0.1) -> List[Dict[str, Any]]:
        try:
            if not self._load_cookies_to_driver():
                logger.warning("æœªæˆåŠŸæ³¨å…¥cookiesï¼Œå¯èƒ½ä¼šè§¦å‘ç™»å½•/é£æ§")
            
            # æ–°å¢ï¼šåœ¨æ­£å¼è®¿é—®é—®é¢˜é¡µå‰ï¼Œä¸»åŠ¨æ£€æµ‹ç™»å½•æ€ï¼Œä¸è¶³åˆ™å¼•å¯¼æ‰‹åŠ¨ç™»å½•
            try:
                self.driver.get(self.config.BASE_URL)
                time.sleep(1.5)
                cookies_map = {c.get('name'): c.get('value', '') for c in self.driver.get_cookies()}
                z = cookies_map.get('z_c0', '')
                logged_in = bool(z and (z.startswith('2|') or len(z) > 60) and 'v10' not in z)
                if logged_in:
                    # è‹¥æ£€æµ‹åˆ°å·²ç™»å½•ï¼ˆä¾‹å¦‚å¤ç”¨ç³»ç»ŸChromeç™»å½•æ€ï¼‰ï¼Œä¹Ÿç«‹å³ä¿å­˜ä¸€æ¬¡cookiesï¼Œä¾¿äºAPI/åç»­æµç¨‹ä½¿ç”¨
                    try:
                        self._persist_cookies(self.driver)
                    except Exception as se:
                        logger.debug(f"é¢„æ£€å·²ç™»å½•ä½†ä¿å­˜cookiesæ—¶å‡ºç°é—®é¢˜: {se}")
            except Exception as e:
                logger.debug(f"é¢„æ£€ç™»å½•æ€å¼‚å¸¸: {e}")
                logged_in = False
            if not logged_in:
                logger.info("é¢„æ£€æœªæ£€æµ‹åˆ°æœ‰æ•ˆç™»å½•æ€ï¼Œè¿›å…¥æ‰‹åŠ¨ç™»å½•æµç¨‹â€¦â€¦")
                did_manual_login = self._ensure_logged_in_manually(self.driver, timeout_sec=180)
                if not did_manual_login:
                    logger.warning("æ‰‹åŠ¨ç™»å½•å¤±è´¥æˆ–è¶…æ—¶ï¼Œç»§ç»­å°è¯•è¿›è¡Œæœ‰é™æŠ“å–ï¼ˆå¯èƒ½å—é™äºæœªç™»å½•çŠ¶æ€ï¼‰")
                else:
                    # ç™»å½•æˆåŠŸå·²åœ¨ _ensure_logged_in_manually ä¸­æŒä¹…åŒ–cookies
                    logged_in = True

            q_url = f"{self.config.BASE_URL}/question/{question_id}"
            logger.info(f"æ‰“å¼€é—®é¢˜é¡µ: {q_url}")
            self.driver.get(q_url)
            time.sleep(2)

            seen_request_ids = set()
            all_items = []
            is_end_flag = False

            for i in range(max_scrolls):
                logger.info(f"ä¸‹æ‹‰è§¦å‘æ‡’åŠ è½½ï¼Œ ç¬¬ {i+1}/{max_scrolls} æ¬¡")
                self._scroll_to_load(times=1, pause=pause)
                try:
                    self._persist_cookies(self.driver)
                except Exception as e:
                    logger.debug(f"æ»šåŠ¨åæŒä¹…åŒ–cookieså¤±è´¥: {e}")
                
                # åˆå§‹åŒ–æœ¬è½®ç»Ÿè®¡ï¼Œé¿å…åœ¨æ— æ–°å“åº”æ—¶æœªå®šä¹‰
                round_items_count = 0
                round_detected_is_end = False
                
                # æ”¶é›†æœ¬è½®æ–°å“åº”
                newly = self._collect_feeds_from_perf_logs(question_id, seen_request_ids)
                if newly:
                    logger.info(f"æ•è·åˆ° {len(newly)} ä¸ªfeedså“åº”")
                    
                    round_items_count = 0
                    round_detected_is_end = False
                    for req_id, payload in newly:
                        seen_request_ids.add(req_id)
                        # å…¼å®¹éƒ¨åˆ†æ¥å£ç›´æ¥è¿”å› list çš„æƒ…å†µ
                        if isinstance(payload, list):
                            page_items = payload
                            paging = {}
                        elif isinstance(payload, dict):
                            page_items = payload.get('data', []) or []
                            paging = (payload.get('paging') or {}) if isinstance(payload.get('paging'), dict) else {}
                        else:
                            page_items, paging = [], {}
                        all_items.extend(page_items)
                        round_items_count += len(page_items)
                        if stop_when_is_end and isinstance(paging, dict) and paging.get('is_end'):
                            round_detected_is_end = True
                            is_end_flag = True
                    
                    # è‹¥æ²¡æœ‰æ–°å“åº”ï¼Œå°è¯•å†æ¬¡ç­‰å¾…ä¸€å°ä¼š
                    if not newly:
                        time.sleep(1)

                    # å°‘äºé¢„æœŸæ¡æ•°ï¼Œåˆ™è§¦å‘ä¸€æ¬¡å›æ»šé‡è¯•
                    if expected_min_per_scroll and round_items_count < expected_min_per_scroll:
                        logger.info(f"âš ï¸ æœ¬è½®æ–°å¢ {round_items_count} æ¡ < é¢„æœŸ {expected_min_per_scroll} æ¡ï¼Œè§¦å‘å›æ»šé‡è¯•ä¸€æ¬¡â€¦â€¦")
                        try:
                            self._soft_rollback(percent=rollback_percent, pause=max(1.2, pause - 0.3))
                            try:
                                self._persist_cookies(self.driver)
                            except Exception as e:
                                logger.debug(f"å›æ»šåæŒä¹…åŒ–cookieså¤±è´¥: {e}")
                            newly_retry = self._collect_feeds_from_perf_logs(question_id, seen_request_ids)
                            added_retry = 0
                            for req_id, payload in newly_retry:
                                seen_request_ids.add(req_id)
                                if isinstance(payload, list):
                                    page_items = payload
                                    paging = {}
                                elif isinstance(payload, dict):
                                    page_items = payload.get('data', []) or []
                                    paging = (payload.get('paging') or {}) if isinstance(payload.get('paging'), dict) else {}
                                else:
                                    page_items, paging = [], {}
                                all_items.extend(page_items)
                                added_retry += len(page_items)
                                if stop_when_is_end and isinstance(paging, dict) and paging.get('is_end'):
                                    round_detected_is_end = True
                                    is_end_flag = True
                            logger.info(f"å›æ»šé‡è¯•æ–°å¢ {added_retry} æ¡")
                            round_items_count += added_retry
                        except Exception as e:
                            logger.debug(f"å›æ»šé‡è¯•å¼‚å¸¸: {e}")

                    # è‹¥æ£€æµ‹åˆ° is_endï¼Œåˆ™åšä¸€æ¬¡å›æ»šéªŒè¯ï¼Œé¿å…è¯¯åˆ¤
                    if stop_when_is_end and round_detected_is_end and verify_end_with_rollback:
                        logger.info("ğŸ§ª æ£€æµ‹åˆ° paging.is_end=Trueï¼Œæ‰§è¡Œä¸€æ¬¡å›æ»šéªŒè¯â€¦â€¦")
                        prev_len = len(all_items)
                        try:
                            self._soft_rollback(percent=max(rollback_percent, 0.12), pause=max(1.2, pause))
                            try:
                                self._persist_cookies(self.driver)
                            except Exception as e:
                                logger.debug(f"å›æ»šåæŒä¹…åŒ–cookieså¤±è´¥: {e}")
                            newly_verify = self._collect_feeds_from_perf_logs(question_id, seen_request_ids)
                            verify_added = 0
                            verify_is_end_still_true = True
                            for req_id, payload in newly_verify:
                                seen_request_ids.add(req_id)
                                if isinstance(payload, list):
                                    page_items = payload
                                    paging = {}
                                elif isinstance(payload, dict):
                                    page_items = payload.get('data', []) or []
                                    paging = (payload.get('paging') or {}) if isinstance(payload.get('paging'), dict) else {}
                                else:
                                    page_items, paging = [], {}
                                all_items.extend(page_items)
                                verify_added += len(page_items)
                                if isinstance(paging, dict) and not paging.get('is_end'):
                                    verify_is_end_still_true = False
                            logger.info(f"å›æ»šéªŒè¯æ–°å¢ {verify_added} æ¡ï¼Œis_end ä»ä¸º {verify_is_end_still_true}")
                            # è‹¥å›æ»šåä»æ— æ–°å¢æˆ–ä»æç¤º is_endï¼Œåˆ™ç¡®è®¤ç»“æŸ
                            if verify_added == 0 and verify_is_end_still_true:
                                logger.info("âœ… å›æ»šéªŒè¯åä»ä¸ºç»“æŸçŠ¶æ€ï¼Œåœæ­¢æ»šåŠ¨")
                                break
                            else:
                                # æœ‰æ–°å¢æˆ– is_end å˜ä¸º Falseï¼Œç»§ç»­åç»­æ»šåŠ¨
                                is_end_flag = False
                        except Exception as e:
                            logger.debug(f"å›æ»šéªŒè¯å¼‚å¸¸: {e}")
                            # éªŒè¯å¤±è´¥æ—¶ä¿å®ˆé€€å‡º
                            break

                    if is_end_flag:
                        logger.info("æ£€æµ‹åˆ°paging.is_end=Trueï¼Œç»“æŸ")
                        break

                    # è‹¥æ²¡æœ‰æ–°å“åº”ï¼Œå†ç­‰å¾…ä¸€å°ä¼šå†ç»§ç»­
                    if not newly and round_items_count == 0:
                        time.sleep(0.8)
            
            # å¾ªç¯ç»“æŸåçš„å¤„ç†é€»è¾‘
            if not all_items:
                # å›é€€ï¼šç›´æ¥åœ¨é¡µé¢ä¸Šä¸‹æ–‡fetché¦–é¡µï¼ˆä»…ä¸€æ¬¡ï¼‰
                fallback_items = self._fetch_first_page_fallback(question_id)
                if fallback_items:
                    all_items.extend(fallback_items)

            # ä»æ— æ•°æ®åˆ™å¼•å¯¼ç”¨æˆ·æ‰‹åŠ¨ç™»å½•åé‡è¯•ä¸€è½®ï¼ˆå¦‚æœä¹‹å‰å°šæœªæ‰‹åŠ¨ç™»å½•ï¼‰
            if not all_items and not logged_in:
                logger.info("ğŸ“¥ å°è¯•è¿›å…¥æ‰‹åŠ¨ç™»å½•å›é€€æµç¨‹ä»¥è·å–æœ‰æ•ˆç™»å½•æ€â€¦â€¦")
                logged = self._ensure_logged_in_manually(self.driver, timeout_sec=180)
                if logged:
                    # ç™»å½•å®Œæˆåé‡æ–°è®¿é—®èµ·å§‹é¡µï¼Œè¿›è¡Œå°‘é‡æ»šåŠ¨é‡‡é›†
                    try:
                        self.driver.get(q_url)
                        time.sleep(1.5)
                        # ä¿å­˜ç™»å½•åçš„cookies
                        self._persist_cookies(self.driver)
                    except Exception as e:
                        logger.debug(f"ç™»å½•åé‡æ–°æ‰“å¼€é—®é¢˜é¡µå¤±è´¥: {e}")

                try:
                    self._persist_cookies(self.driver)
                except Exception as e:
                    logger.debug(f"ç»“æŸå‰æŒä¹…åŒ–cookieså¤±è´¥: {e}")
                return all_items
        except Exception as e:
            logger.error(f"æµè§ˆå™¨æŠ“å–å¤±è´¥: {e}")
            return []
        finally:
            try:
                self._persist_cookies(self.driver)
            except Exception as e:
                logger.debug(f"ç»“æŸå…œåº•æŒä¹…åŒ–cookieså¤±è´¥: {e}")
            if not self.headless:
                try:
                    # æ£€æŸ¥æ˜¯å¦åœ¨äº¤äº’å¼ç¯å¢ƒä¸­
                    import sys
                    if sys.stdin.isatty():
                        input("ğŸ‘€ éæ— å¤´æ¨¡å¼ï¼ŒæŒ‰å›è½¦é”®å…³é—­æµè§ˆå™¨çª—å£...")
                    else:
                        logger.info("éäº¤äº’å¼ç¯å¢ƒï¼Œè‡ªåŠ¨å…³é—­æµè§ˆå™¨çª—å£")
                        time.sleep(2)  # ç»™ç”¨æˆ·ä¸€ç‚¹æ—¶é—´çœ‹åˆ°æµè§ˆå™¨å†…å®¹
                except (EOFError, KeyboardInterrupt):
                    logger.info("æ ‡å‡†è¾“å…¥ä¸å¯ç”¨æˆ–ç”¨æˆ·ä¸­æ–­ï¼Œç›´æ¥å…³é—­æµè§ˆå™¨ã€‚")
            self.close()

    @staticmethod
    def save_feeds_data(feeds_data: list, filename: str) -> bool:
        try:
            # è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•
            try:
                p = Path(filename)
                if p.parent and str(p.parent) not in ("", "."):
                    p.parent.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                logger.debug(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥ï¼ˆå°†ç»§ç»­å°è¯•ä¿å­˜æ–‡ä»¶ï¼‰: {e}")
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(feeds_data, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… æµè§ˆå™¨æŠ“å–feedsæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜feedsæ•°æ®å¤±è´¥: {e}")
            return False

    def extract_answers_from_feeds(self, feeds_data: List[Dict]) -> List[Dict]:
        """ä»feedsæ•°æ®ä¸­æå–ç­”æ¡ˆï¼ˆbrowseræ¨¡å¼è¾…åŠ©ï¼‰"""
        answers = []
        for feed_item in feeds_data:
            try:
                if feed_item.get('target_type') != 'answer':
                    continue
                target = feed_item.get('target', {})
                if not target:
                    continue
                answer_info = {
                    'answer_id': str(target.get('id', '')),
                    'content': target.get('content', ''),
                    'excerpt': target.get('excerpt', ''),
                    'voteup_count': target.get('voteup_count', 0),
                    'comment_count': target.get('comment_count', 0),
                    'created_time': target.get('created_time', 0),
                    'updated_time': target.get('updated_time', 0),
                    'author': target.get('author', {}).get('name', ''),
                    'author_url_token': target.get('author', {}).get('url_token', ''),
                    'question_id': target.get('question', {}).get('id', ''),
                    'question_title': target.get('question', {}).get('title', ''),
                    'is_author': target.get('relationship', {}).get('is_author', False)
                }
                answers.append(answer_info)
            except Exception as e:
                logger.warning(f"è§£æfeedé¡¹å¤±è´¥: {e}")
                continue
        logger.info(f"ğŸ“ ä»feedsä¸­æå–åˆ° {len(answers)} ä¸ªç­”æ¡ˆ")
        return answers


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    parser = argparse.ArgumentParser(description="çŸ¥ä¹feedsæ‡’åŠ è½½çˆ¬è™«")
    parser.add_argument("--mode", choices=["requests", "browser"], default="requests", help="æŠ“å–æ¨¡å¼ï¼šrequests æˆ– browser")
    parser.add_argument("--question-id", dest="question_id", default="19551593", help="é—®é¢˜IDï¼Œä¾‹å¦‚ 19551593")
    parser.add_argument("--max-feeds", dest="max_feeds", type=int, default=50, help="requestsæ¨¡å¼ä¸‹æœ€å¤§æŠ“å–feedæ•°é‡")
    parser.add_argument("--headless", dest="headless", type=str, choices=["true", "false"], help="browseræ¨¡å¼æ˜¯å¦æ— å¤´ï¼Œé»˜è®¤å–config")
    parser.add_argument("--max-scrolls", dest="max_scrolls", type=int, default=30, help="browseræ¨¡å¼æœ€å¤§æ»šåŠ¨æ¬¡æ•°")
    parser.add_argument("--pause", dest="pause", type=float, default=2.0, help="browseræ¨¡å¼æ¯æ¬¡æ»šåŠ¨åçš„ç­‰å¾…ç§’æ•°")
    parser.add_argument("--out", dest="out", default=None, help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨ç”Ÿæˆåˆ°outputç›®å½•")
    args = parser.parse_args()

    logger.info("ğŸš€ çŸ¥ä¹APIæ‡’åŠ è½½æµ‹è¯•")

    # ç¡®ä¿ç›®å½•
    try:
        ZhihuConfig.create_directories()
    except Exception:
        pass

    if args.mode == "browser":
        # æ–¹æ¡ˆAï¼šæµè§ˆå™¨ä¸Šä¸‹æ–‡ + CDP æŠ“å–
        headless = None
        if args.headless is not None:
            headless = (args.headless.lower() == "true")
        crawler = BrowserFeedsCrawler(headless=headless)
        feeds = crawler.crawl_feeds_via_browser(
            question_id=args.question_id,
            max_scrolls=args.max_scrolls,
            pause=args.pause,
            stop_when_is_end=True
        )
        # å¤„ç†è¾“å‡ºè·¯å¾„ï¼šå¦‚æœæ˜¯ç›®å½•åˆ™è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
        if args.out:
            out_path = Path(args.out)
            if out_path.is_dir() or str(out_path).endswith('/'):
                out_path = out_path / f"feeds_{args.question_id}_browser.json"
            out_path = str(out_path)
        else:
            out_path = f"{ZhihuConfig.OUTPUT_DIR}/feeds_{args.question_id}_browser.json"
        try:
            crawler.save_feeds_data(feeds, out_path)
            logger.info(f"ğŸ‰ Browseræ¨¡å¼å®Œæˆï¼Œitems={len(feeds)}ï¼Œè¾“å‡º: {out_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜Browseræ¨¡å¼feedså¤±è´¥: {e}")
        # æå–ç­”æ¡ˆç¤ºä¾‹
        answers = crawler.extract_answers_from_feeds(feeds)
        logger.info(f"ğŸ“ æå–ç­”æ¡ˆæ•°: {len(answers)}")
        return

    # é»˜è®¤ï¼šrequests æ¨¡å¼ï¼Œæ²¿ç”¨åŸæœ‰æµ‹è¯•æµç¨‹
    crawler = ZhihuLazyLoadCrawler()

    # æµ‹è¯•æ‡’åŠ è½½èƒ½åŠ›
    logger.info("\n" + "="*60)
    logger.info("ğŸ§ª æ‡’åŠ è½½èƒ½åŠ›æµ‹è¯•")
    logger.info("="*60)

    ability_result = crawler.test_lazyload_ability(args.question_id)

    if not ability_result['success']:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {ability_result.get('error')}")
        return

    if ability_result['lazyload_supported']:
        logger.info("âœ… çŸ¥ä¹æ”¯æŒæ‡’åŠ è½½ï¼")

        if ability_result['continuous_request_supported']:
            logger.info("âœ… æ”¯æŒè¿ç»­è¯·æ±‚fetchæ–‡ä»¶ï¼")

            # æ‰§è¡Œå®Œæ•´æ‡’åŠ è½½æµ‹è¯•
            logger.info("\n" + "="*60)
            logger.info("ğŸš€ æ‰§è¡Œå®Œæ•´æ‡’åŠ è½½çˆ¬å–")
            logger.info("="*60)

            all_feeds = crawler.crawl_all_feeds_lazyload(args.question_id, max_feeds=args.max_feeds)

            if all_feeds:
                # æå–ç­”æ¡ˆ
                answers = crawler.extract_answers_from_feeds(all_feeds)

                # ä¿å­˜æ•°æ®
                out_path = args.out or f"{ZhihuConfig.OUTPUT_DIR}/feeds_{args.question_id}_requests.json"
                crawler.save_feeds_data(all_feeds, out_path)

                logger.info("ğŸ‰ æ‡’åŠ è½½æµ‹è¯•æˆåŠŸ!")
                logger.info(f"ğŸ“Š æ€»feedsæ•°: {len(all_feeds)}")
                logger.info(f"ğŸ“ æå–ç­”æ¡ˆæ•°: {len(answers)}")

                if answers:
                    logger.info("ğŸ“‹ ç¤ºä¾‹ç­”æ¡ˆä¿¡æ¯:")
                    for i, answer in enumerate(answers[:3], 1):
                        logger.info(f"  ç­”æ¡ˆ{i}: {answer['author']} - {answer['voteup_count']}èµ")
            else:
                logger.warning("âš ï¸ æœªè·å–åˆ°feedsæ•°æ®")

        else:
            logger.warning("âš ï¸ ä¸æ”¯æŒè¿ç»­è¯·æ±‚ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")

    else:
        logger.warning("âš ï¸ ä¸æ”¯æŒæ‡’åŠ è½½æˆ–å·²åˆ°è¾¾æ•°æ®æœ«å°¾")


if __name__ == "__main__":
    main()
