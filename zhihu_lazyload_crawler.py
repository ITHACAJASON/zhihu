#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥ä¹APIæ‡’åŠ è½½çˆ¬è™«
æ”¯æŒå®Œæ•´çš„åˆ†é¡µå’Œè¿ç»­è¯·æ±‚fetchæ–‡ä»¶
"""

import requests
import json
import time
import re
from urllib.parse import urlparse, parse_qs
from typing import List, Dict, Optional, Tuple, Any
from loguru import logger
from pathlib import Path
import argparse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import WebDriverException
from config import ZhihuConfig
import pickle


class ZhihuLazyLoadCrawler:
    """çŸ¥ä¹æ‡’åŠ è½½APIçˆ¬è™«"""

    def __init__(self):
        self.session = requests.Session()
        self.base_headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin'
        }
        self.session.headers.update(self.base_headers)
        self.load_cookies()

        # APIé…ç½®
        self.api_base_url = 'https://www.zhihu.com/api/v4/questions'
        self.feeds_include_params = (
            'data[*].is_normal,admin_closed_comment,reward_info,is_collapsed,'
            'annotation_action,annotation_detail,collapse_reason,is_sticky,'
            'collapsed_by,suggest_edit,comment_count,can_comment,content,'
            'editable_content,attachment,voteup_count,reshipment_settings,'
            'comment_permission,created_time,updated_time,review_info,'
            'relevant_info,question,excerpt,is_labeled,paid_info,'
            'paid_info_content,reaction_instruction,relationship.is_authorized,'
            'is_author,voting,is_thanked,is_nothelp;'
            'data[*].author.follower_count,vip_info,kvip_info,badge[*].topics;'
            'data[*].settings.table_of_content.enabled'
        )

    def load_cookies(self):
        """åŠ è½½cookies"""
        try:
            cookie_file = Path('cookies/zhihu_cookies.json')
            with open(cookie_file, 'r', encoding='utf-8') as f:
                cookies_data = json.load(f)
                for cookie in cookies_data:
                    self.session.cookies.set(
                        cookie['name'],
                        cookie['value'],
                        domain=cookie.get('domain', '.zhihu.com'),
                        path=cookie.get('path', '/')
                    )
                logger.info(f"æˆåŠŸåŠ è½½cookiesï¼Œå…±{len(cookies_data)}ä¸ª")
                return True
        except Exception as e:
            logger.error(f"åŠ è½½cookieså¤±è´¥: {e}")
            return False

    def extract_question_id(self, question_url: str) -> Optional[str]:
        """ä»é—®é¢˜URLä¸­æå–é—®é¢˜ID"""
        try:
            if '/question/' in question_url:
                parts = question_url.split('/question/')
                if len(parts) > 1:
                    question_id = parts[1].split('/')[0].split('?')[0]
                    return question_id
            return None
        except Exception as e:
            logger.error(f"æå–é—®é¢˜IDå¤±è´¥: {e}")
            return None

    def parse_next_url(self, next_url: str) -> Dict:
        """è§£æä¸‹ä¸€é¡µURLä¸­çš„å‚æ•°"""
        try:
            if not next_url:
                return {}

            parsed_url = urlparse(next_url)
            params = parse_qs(parsed_url.query)

            # æå–å…³é”®å‚æ•°
            result = {}
            for key, values in params.items():
                if values:
                    result[key] = values[0] if len(values) == 1 else values

            return result
        except Exception as e:
            logger.error(f"è§£ænext URLå¤±è´¥: {e}")
            return {}

    def build_feeds_url(self, question_id: str, cursor: str = None,
                       offset: int = None, limit: int = 20) -> str:
        """æ„å»ºfeeds API URL"""
        url = f"{self.api_base_url}/{question_id}/feeds"

        params = {
            'include': self.feeds_include_params,
            'limit': str(limit),
            'order': 'default',
            'ws_qiangzhisafe': '0',
            'platform': 'desktop'
        }

        # æ·»åŠ cursoræˆ–offsetï¼ˆä¼˜å…ˆä½¿ç”¨cursorï¼‰
        if cursor:
            params['cursor'] = cursor
        elif offset is not None:
            params['offset'] = str(offset)

        # æ‰‹åŠ¨æ„å»ºURLé¿å…ç¼–ç é—®é¢˜
        param_str = '&'.join([f"{k}={v}" for k, v in params.items()])
        return f"{url}?{param_str}"

    def fetch_feeds_page(self, question_id: str, cursor: str = None,
                        offset: int = None, limit: int = 20) -> Optional[Dict]:
        """è·å–feedsé¡µé¢æ•°æ®"""
        max_retries = 3

        for attempt in range(max_retries):
            try:
                url = self.build_feeds_url(question_id, cursor, offset, limit)
                logger.info(f"è¯·æ±‚feeds API (å°è¯• {attempt + 1}/{max_retries})")
                logger.debug(f"è¯·æ±‚URL: {url}")

                # è®¾ç½®headers
                headers = self.base_headers.copy()
                headers['Referer'] = f'https://www.zhihu.com/question/{question_id}'
                headers['X-Requested-With'] = 'fetch'

                response = self.session.get(url, headers=headers, timeout=30)

                if response.status_code == 200:
                    data = response.json()

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                    feeds_data = data.get('data', [])
                    logger.info(f"âœ… è·å–åˆ° {len(feeds_data)} ä¸ªfeedé¡¹")

                    # æ£€æŸ¥sessionä¿¡æ¯
                    session_info = data.get('session', {})
                    session_id = session_info.get('id', '')
                    if session_id:
                        logger.info(f"ğŸ”‘ Session ID: {session_id}")

                    return data

                elif response.status_code == 403:
                    logger.warning(f"âŒ APIè®¿é—®è¢«æ‹’ç»: {response.status_code}")
                    error_data = response.json()
                    if 'error' in error_data:
                        logger.warning(f"é”™è¯¯ä¿¡æ¯: {error_data['error'].get('message', 'Unknown error')}")
                    return None

                else:
                    logger.warning(f"âš ï¸ APIè¿”å›å¼‚å¸¸çŠ¶æ€: {response.status_code}")
                    if attempt == max_retries - 1:
                        return None

            except requests.exceptions.RequestException as e:
                logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return None
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æå¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return None
            except Exception as e:
                logger.error(f"æœªçŸ¥é”™è¯¯ (å°è¯• {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return None

            # å¤±è´¥åç­‰å¾…åé‡è¯•
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)

        return None

    def crawl_all_feeds_lazyload(self, question_id: str, max_feeds: int = None) -> List[Dict]:
        """ä½¿ç”¨æ‡’åŠ è½½æ–¹å¼çˆ¬å–æ‰€æœ‰feedsæ•°æ®"""
        all_feeds = []
        page_count = 0
        cursor = None
        offset = 0

        logger.info(f"ğŸš€ å¼€å§‹æ‡’åŠ è½½çˆ¬å–é—®é¢˜ {question_id} çš„feeds")
        logger.info(f"ğŸ“Š æœ€å¤§feedsé™åˆ¶: {max_feeds if max_feeds else 'æ— é™åˆ¶'}")

        while True:
            page_count += 1
            logger.info(f"\nğŸ“„ è·å–ç¬¬ {page_count} é¡µæ•°æ®")
            logger.info(f"ğŸ”„ Cursor: {cursor}, Offset: {offset}")

            # è·å–å½“å‰é¡µæ•°æ®
            page_data = self.fetch_feeds_page(question_id, cursor=cursor, offset=offset, limit=20)
            if not page_data:
                logger.error(f"âŒ è·å–ç¬¬ {page_count} é¡µæ•°æ®å¤±è´¥")
                break

            # è§£æfeedsæ•°æ®
            feeds_data = page_data.get('data', [])
            logger.info(f"ğŸ“¦ æœ¬é¡µè·å–åˆ° {len(feeds_data)} ä¸ªfeedé¡¹")

            # æ·»åŠ åˆ°æ€»é›†åˆ
            all_feeds.extend(feeds_data)

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶
            if max_feeds and len(all_feeds) >= max_feeds:
                logger.info(f"âœ… å·²è¾¾åˆ°æœ€å¤§feedsæ•°é‡é™åˆ¶: {max_feeds}")
                all_feeds = all_feeds[:max_feeds]
                break

            # è§£æåˆ†é¡µä¿¡æ¯
            paging = page_data.get('paging', {})
            is_end = paging.get('is_end', True)
            next_url = paging.get('next', '')

            logger.info(f"ğŸ” åˆ†é¡µä¿¡æ¯: is_end={is_end}")

            if is_end:
                logger.info(f"ğŸ¯ å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                break

            # è§£æä¸‹ä¸€é¡µå‚æ•°
            next_params = self.parse_next_url(next_url)

            # æ›´æ–°cursorå’Œoffset
            if 'cursor' in next_params:
                cursor = next_params['cursor']
                logger.info(f"ğŸ“Œ æ›´æ–°cursor: {cursor}")
            elif 'offset' in next_params:
                offset = int(next_params['offset'])
                logger.info(f"ğŸ“Œ æ›´æ–°offset: {offset}")
            else:
                # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ä¸‹ä¸€é¡µå‚æ•°ï¼Œå¢åŠ offset
                offset += len(feeds_data)
                logger.info(f"ğŸ“Œ é€’å¢offset: {offset}")

            # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)

            # å®‰å…¨æ£€æŸ¥
            if page_count >= 100:  # æœ€å¤š100é¡µ
                logger.warning(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶: {page_count}")
                break

        logger.info(f"\nğŸ‰ æ‡’åŠ è½½çˆ¬å–å®Œæˆ!")
        logger.info(f"ğŸ“Š æ€»å…±è·å–åˆ° {len(all_feeds)} ä¸ªfeeds")
        logger.info(f"ğŸ“„ å…±è¯·æ±‚äº† {page_count} é¡µ")

        return all_feeds

    def extract_answers_from_feeds(self, feeds_data: List[Dict]) -> List[Dict]:
        """ä»feedsæ•°æ®ä¸­æå–ç­”æ¡ˆï¼ˆrequestsæ¨¡å¼ï¼‰"""
        answers = []
        for feed_item in feeds_data:
            try:
                if feed_item.get('target_type') != 'answer':
                    continue
                target = feed_item.get('target', {})
                if not target:
                    continue
                answer_info = {
                    'answer_id': str(target.get('id', '')),
                    'content': target.get('content', ''),
                    'excerpt': target.get('excerpt', ''),
                    'voteup_count': target.get('voteup_count', 0),
                    'comment_count': target.get('comment_count', 0),
                    'created_time': target.get('created_time', 0),
                    'updated_time': target.get('updated_time', 0),
                    'author': target.get('author', {}).get('name', ''),
                    'author_url_token': target.get('author', {}).get('url_token', ''),
                    'question_id': target.get('question', {}).get('id', ''),
                    'question_title': target.get('question', {}).get('title', ''),
                    'is_author': target.get('relationship', {}).get('is_author', False)
                }
                answers.append(answer_info)
            except Exception as e:
                logger.warning(f"è§£æfeedé¡¹å¤±è´¥: {e}")
                continue
        logger.info(f"ğŸ“ ä»feedsä¸­æå–åˆ° {len(answers)} ä¸ªç­”æ¡ˆ")
        return answers

    def test_lazyload_ability(self, question_id: str) -> Dict:
        """æµ‹è¯•æ‡’åŠ è½½èƒ½åŠ›ï¼ˆrequestsæ¨¡å¼ï¼‰"""
        logger.info(f"ğŸ§ª æµ‹è¯•é—®é¢˜ {question_id} çš„æ‡’åŠ è½½èƒ½åŠ›")
        first_page = self.fetch_feeds_page(question_id, limit=5)
        if not first_page:
            return {'success': False, 'error': 'æ— æ³•è·å–ç¬¬ä¸€é¡µæ•°æ®'}
        paging = first_page.get('paging', {})
        has_paging = bool(paging)
        has_next = bool(paging.get('next'))
        is_end = paging.get('is_end', True)
        next_page_success = False
        if not is_end and has_next:
            next_params = self.parse_next_url(paging['next'])
            cursor = next_params.get('cursor')
            offset = next_params.get('offset')
            if cursor or offset:
                next_page = self.fetch_feeds_page(
                    question_id,
                    cursor=cursor,
                    offset=int(offset) if offset else None,
                    limit=5
                )
                next_page_success = bool(next_page)
        result = {
            'success': True,
            'first_page_feeds': len(first_page.get('data', [])),
            'has_paging': has_paging,
            'has_next': has_next,
            'is_end': is_end,
            'next_page_success': next_page_success,
            'session_id': first_page.get('session', {}).get('id', ''),
            'lazyload_supported': has_paging and has_next and not is_end,
            'continuous_request_supported': next_page_success
        }
        logger.info("ğŸ“Š æ‡’åŠ è½½èƒ½åŠ›æµ‹è¯•ç»“æœ:")
        for key, value in result.items():
            logger.info(f"  {key}: {value}")
        return result

    def save_feeds_data(self, feeds_data: List[Dict], filename: str) -> bool:
        """ä¿å­˜feedsæ•°æ®åˆ°æ–‡ä»¶ï¼ˆè‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•ï¼‰"""
        try:
            try:
                p = Path(filename)
                if p.parent and str(p.parent) not in ("", "."):
                    p.parent.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                logger.debug(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥ï¼ˆç»§ç»­å°è¯•ä¿å­˜æ–‡ä»¶ï¼‰: {e}")
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(feeds_data, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… feedsæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜feedsæ•°æ®å¤±è´¥: {e}")
            return False

    def extract_answers_from_feeds(self, feeds_data: List[Dict]) -> List[Dict]:
        """ä»feedsæ•°æ®ä¸­æå–ç­”æ¡ˆï¼ˆbrowseræ¨¡å¼è¾…åŠ©ï¼‰"""
        answers = []
        for feed_item in feeds_data:
            try:
                if feed_item.get('target_type') != 'answer':
                    continue
                target = feed_item.get('target', {})
                if not target:
                    continue
                answer_info = {
                    'answer_id': str(target.get('id', '')),
                    'content': target.get('content', ''),
                    'excerpt': target.get('excerpt', ''),
                    'voteup_count': target.get('voteup_count', 0),
                    'comment_count': target.get('comment_count', 0),
                    'created_time': target.get('created_time', 0),
                    'updated_time': target.get('updated_time', 0),
                    'author': target.get('author', {}).get('name', ''),
                    'author_url_token': target.get('author', {}).get('url_token', ''),
                    'question_id': target.get('question', {}).get('id', ''),
                    'question_title': target.get('question', {}).get('title', ''),
                    'is_author': target.get('relationship', {}).get('is_author', False)
                }
                answers.append(answer_info)
            except Exception as e:
                logger.warning(f"è§£æfeedé¡¹å¤±è´¥: {e}")
                continue
        logger.info(f"ğŸ“ ä»feedsä¸­æå–åˆ° {len(answers)} ä¸ªç­”æ¡ˆ")
        return answers

# =============== æ–°å¢ï¼šæµè§ˆå™¨ä¸Šä¸‹æ–‡æŠ“å–æ–¹æ¡ˆï¼ˆæ–¹æ¡ˆAï¼‰ ===============
class BrowserFeedsCrawler:
    """ä½¿ç”¨æµè§ˆå™¨ä¸Šä¸‹æ–‡è§¦å‘å‰ç«¯fetchå¹¶é€šè¿‡CDPæŠ“å–feedså“åº”"""

    def __init__(self, headless: bool = None):
        self.config = ZhihuConfig
        self.headless = self.config.HEADLESS if headless is None else headless
        self.driver = None
        self._init_driver()

    def _init_driver(self):
        """åˆå§‹åŒ–WebDriverï¼Œä¼˜å…ˆä½¿ç”¨ç³»ç»ŸChromeç›®å½•ï¼Œå¤±è´¥æ—¶ä½¿ç”¨ä¸´æ—¶ç›®å½•"""
        # ç»Ÿä¸€é…ç½®Optionsçš„å‡½æ•°
        def configure_options(use_temp_dir=False, temp_dir_path=None):
            options = Options()
            if self.headless:
                options.add_argument('--headless=new')
            
            # çª—å£å¤§å°
            if isinstance(self.config.WINDOW_SIZE, tuple) and len(self.config.WINDOW_SIZE) == 2:
                w, h = self.config.WINDOW_SIZE
                options.add_argument(f"--window-size={w},{h}")
            elif isinstance(self.config.WINDOW_SIZE, str):
                options.add_argument(f"--window-size={self.config.WINDOW_SIZE}")
            
            # åŸºç¡€é…ç½®
            options.add_argument('--disable-gpu')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            
            # è§„é¿è‡ªåŠ¨åŒ–ç‰¹å¾
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            
            # è®¾ç½®è¯­è¨€å’ŒUA
            try:
                ua = self.config.USER_AGENTS[0]
                options.add_argument(f"--user-agent={ua}")
            except Exception:
                pass
            options.add_argument('--lang=zh-CN,zh;q=0.9,en;q=0.8')
            
            # æ€§èƒ½æ—¥å¿—
            try:
                options.set_capability('goog:loggingPrefs', {'performance': 'ALL'})
            except Exception:
                pass
            
            # ç”¨æˆ·æ•°æ®ç›®å½•é…ç½®
            if use_temp_dir and temp_dir_path:
                options.add_argument(f"--user-data-dir={str(temp_dir_path)}")
                logger.info(f"ä½¿ç”¨ä¸´æ—¶Chromeé…ç½®ç›®å½•: {temp_dir_path}")
            else:
                user_data_dir = Path.home() / 'Library' / 'Application Support' / 'Google' / 'Chrome'
                if user_data_dir.exists():
                    options.add_argument(f"--user-data-dir={str(user_data_dir)}")
                    options.add_argument("--profile-directory=Default")
                    logger.info(f"å°è¯•ä½¿ç”¨æœ¬åœ°Chromeç”¨æˆ·æ•°æ®ç›®å½•: {user_data_dir}")
            
            return options

        def setup_webdriver_fingerprint(driver):
            """è®¾ç½®WebDriveræŒ‡çº¹éšè—"""
            try:
                driver.execute_cdp_cmd(
                    "Page.addScriptToEvaluateOnNewDocument",
                    {
                        "source": """
                        Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
                        Object.defineProperty(navigator, 'languages', { get: () => ['zh-CN', 'zh', 'en'] });
                        Object.defineProperty(navigator, 'platform', { get: () => 'MacIntel' });
                        window.chrome = { runtime: {} };
                        """
                    }
                )
                driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            except Exception:
                pass

        # ç¬¬ä¸€æ¬¡å°è¯•ï¼šä½¿ç”¨ç³»ç»ŸChromeç›®å½•
        try:
            options = configure_options(use_temp_dir=False)
            self.driver = webdriver.Chrome(options=options)
            setup_webdriver_fingerprint(self.driver)
            logger.info("âœ… æˆåŠŸåˆå§‹åŒ–Chromeï¼ˆä½¿ç”¨ç³»ç»Ÿç”¨æˆ·æ•°æ®ç›®å½•ï¼‰")
        except Exception as e:
            msg = str(e)
            if 'is already in use' in msg or 'user data directory is already in use' in msg:
                logger.warning("ç³»ç»ŸChromeç”¨æˆ·æ•°æ®ç›®å½•è¢«å ç”¨ï¼Œå›é€€åˆ°ä¸´æ—¶ç›®å½•...")
                # ç¬¬äºŒæ¬¡å°è¯•ï¼šä½¿ç”¨ä¸´æ—¶ç›®å½•
                try:
                    import tempfile
                    tmp_base = Path(tempfile.mkdtemp(prefix="zhihu_tmp_profile_"))
                    options = configure_options(use_temp_dir=True, temp_dir_path=tmp_base)
                    self.driver = webdriver.Chrome(options=options)
                    setup_webdriver_fingerprint(self.driver)
                    logger.info("âœ… æˆåŠŸåˆå§‹åŒ–Chromeï¼ˆä½¿ç”¨ä¸´æ—¶ç”¨æˆ·æ•°æ®ç›®å½•ï¼‰")
                except Exception as e2:
                    logger.warning(f"Selenium Managerå¯åŠ¨å¤±è´¥ï¼Œå°è¯•webdriver_manager: {e2}")
                    # ç¬¬ä¸‰æ¬¡å°è¯•ï¼šä½¿ç”¨webdriver_manager
                    try:
                        driver_path = ChromeDriverManager().install()
                        dp = Path(driver_path)
                        if dp.name.startswith("THIRD_PARTY_NOTICES"):
                            candidate = dp.parent / "chromedriver"
                            service = Service(str(candidate if candidate.exists() else dp))
                        else:
                            service = Service(str(dp))
                        self.driver = webdriver.Chrome(service=service, options=options)
                        setup_webdriver_fingerprint(self.driver)
                        logger.info("âœ… æˆåŠŸåˆå§‹åŒ–Chromeï¼ˆä½¿ç”¨webdriver_managerï¼‰")
                    except Exception as e3:
                        logger.error(f"åˆå§‹åŒ–Chromeå¤±è´¥: {e3}")
                        raise
            else:
                logger.warning(f"Selenium Managerå¯åŠ¨å¤±è´¥ï¼Œå°è¯•webdriver_manager: {e}")
                # ç›´æ¥å°è¯•webdriver_manager
                try:
                    driver_path = ChromeDriverManager().install()
                    dp = Path(driver_path)
                    if dp.name.startswith("THIRD_PARTY_NOTICES"):
                        candidate = dp.parent / "chromedriver"
                        service = Service(str(candidate if candidate.exists() else dp))
                    else:
                        service = Service(str(dp))
                    options = configure_options(use_temp_dir=False)
                    self.driver = webdriver.Chrome(service=service, options=options)
                    setup_webdriver_fingerprint(self.driver)
                    logger.info("âœ… æˆåŠŸåˆå§‹åŒ–Chromeï¼ˆä½¿ç”¨webdriver_managerï¼‰")
                except Exception as e2:
                    logger.error(f"åˆå§‹åŒ–Chromeå¤±è´¥: {e2}")
                    raise

        # å¯ç”¨Networkä»¥ä¾¿åç»­æŠ“body
        try:
            self.driver.execute_cdp_cmd('Network.enable', {})
        except Exception as e:
            logger.warning(f"å¯ç”¨CDP Networkå¤±è´¥: {e}")

    def close(self):
        try:
            if self.driver:
                self.driver.quit()
        except Exception:
            pass

    def _load_cookies_to_driver(self) -> bool:
        """ä»cookies/zhihu_cookies.jsonåŠ è½½åˆ°æµè§ˆå™¨ä¼šè¯"""
        cookie_file = Path(self.config.COOKIES_FILE)
        try:
            if not cookie_file.exists():
                logger.warning(f"æœªæ‰¾åˆ°Cookieæ–‡ä»¶: {cookie_file}")
                return False
            # å…ˆæ‰“å¼€ä¸»åŸŸï¼Œæ‰èƒ½è®¾ç½®cookie
            self.driver.get(self.config.BASE_URL)
            time.sleep(1)
            with open(cookie_file, 'r', encoding='utf-8') as f:
                cookies_data = json.load(f)
            loaded_cdp = 0
            loaded = 0
            for c in cookies_data:
                try:
                    name = c.get('name')
                    value = c.get('value')
                    if not name or value is None:
                        continue
                    domain_raw = c.get('domain') or '.zhihu.com'
                    domain_clean = domain_raw.lstrip('.')
                    path = c.get('path', '/')
                    secure = bool(c.get('secure', True))
                    # å¤„ç† sameSite
                    samesite_raw = c.get('sameSite') or c.get('samesite')
                    ss_map = {
                        'no_restriction': 'None',
                        'none': 'None',
                        'None': 'None',
                        'lax': 'Lax',
                        'Lax': 'Lax',
                        'strict': 'Strict',
                        'Strict': 'Strict',
                        'unspecified': None,
                        'unspecified_same_site': None
                    }
                    ss_val = ss_map.get(samesite_raw, samesite_raw)
                    # SameSite=None æ—¶å¿…é¡» secure=True
                    if ss_val == 'None' and not secure:
                        secure = True
                    http_only = c.get('httpOnly')
                    # è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
                    expiry = None
                    if c.get('expiry'):
                        expiry = int(c['expiry'])
                    elif c.get('expires'):
                        try:
                            expiry = int(c['expires'])
                        except Exception:
                            expiry = None
                    # ä¼˜å…ˆä½¿ç”¨CDPè®¾ç½®cookieï¼ˆå¯è®¾ç½®httpOnlyï¼‰
                    try:
                        params = {
                            'name': name,
                            'value': value,
                            'domain': domain_clean,
                            'path': path,
                            'secure': secure,
                        }
                        if expiry:
                            params['expires'] = expiry
                        if isinstance(http_only, bool):
                            params['httpOnly'] = http_only
                        if ss_val in {'Strict', 'Lax', 'None'}:
                            params['sameSite'] = ss_val
                        ok = self.driver.execute_cdp_cmd('Network.setCookie', params)
                        if ok and ok.get('success'):
                            loaded_cdp += 1
                            continue
                    except Exception as e_cdp:
                        logger.debug(f"CDPè®¾ç½®cookieå¤±è´¥ name={name} domain={domain_clean}: {e_cdp}")
                    # å›é€€ï¼šSelenium add_cookie
                    ck = {
                        'name': name,
                        'value': value,
                        'domain': domain_raw,
                        'path': path,
                        'secure': secure
                    }
                    if expiry:
                        ck['expiry'] = expiry
                    if ss_val in {'Strict', 'Lax', 'None'}:
                        ck['sameSite'] = ss_val
                    try:
                        self.driver.add_cookie(ck)
                        loaded += 1
                        continue
                    except Exception as e1:
                        try:
                            if isinstance(ck.get('domain'), str) and ck['domain'].startswith('.'):
                                ck2 = dict(ck)
                                ck2['domain'] = ck['domain'].lstrip('.')
                                self.driver.add_cookie(ck2)
                                loaded += 1
                                continue
                        except Exception as e2:
                            logger.debug(f"æ·»åŠ cookieå¤±è´¥ name={name} domain={ck.get('domain')}: {e1} | fallback: {e2}")
                            pass
                except Exception as e:
                    logger.debug(f"è·³è¿‡æ— æ³•è®¾ç½®çš„cookie {c.get('name')}: {e}")
                    continue
            logger.info(f"æˆåŠŸæ³¨å…¥ cookiesåˆ°æµè§ˆå™¨ï¼šCDP={loaded_cdp}ï¼ŒSelenium={loaded}")
            # åˆ·æ–°ä»¥ä¾¿cookieç”Ÿæ•ˆ
            self.driver.get(self.config.BASE_URL)
            time.sleep(1)
            return (loaded_cdp + loaded) > 0
        except Exception as e:
            logger.error(f"åŠ è½½æµè§ˆå™¨cookieså¤±è´¥: {e}")
            return False

    def _persist_cookies(self, driver=None) -> bool:
        """å°†å½“å‰æµè§ˆå™¨ä¼šè¯cookiesæŒä¹…åŒ–åˆ°é…ç½®è·¯å¾„(JSON)ä¸cacheç›®å½•(Pickle)"""
        try:
            drv = driver or self.driver
            if drv is None:
                logger.warning("æ— æ³•ä¿å­˜cookies: driveræœªåˆå§‹åŒ–")
                return False
            cookies = drv.get_cookies()
            if not isinstance(cookies, list) or not cookies:
                logger.warning("æ— æ³•ä¿å­˜cookies: æœªä»æµè§ˆå™¨è·å–åˆ°ä»»ä½•cookie")
                return False
            # ä¿å­˜JSON
            json_path = Path(self.config.COOKIES_FILE)
            json_path.parent.mkdir(parents=True, exist_ok=True)
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(cookies, f, ensure_ascii=False, indent=2)
            # å…¼å®¹ï¼šå†ä¿å­˜ä¸€ä»½pickleï¼Œä¾¿äºå…¶ä»–æ¨¡å—å¤ç”¨
            pkl_path = Path('cache/zhihu_cookies.pkl')
            pkl_path.parent.mkdir(parents=True, exist_ok=True)
            with open(pkl_path, 'wb') as pf:
                pickle.dump(cookies, pf)
            # ç®€è¦æç¤ºå…³é”®cookie
            z = next((c.get('value','') for c in cookies if c.get('name')=='z_c0'), '')
            logger.info(f"âœ… å·²ä¿å­˜ç™»å½•cookiesï¼šJSON->{json_path}ï¼ŒPKL->{pkl_path}ï¼ˆz_c0é•¿åº¦={len(z)}ï¼‰")
            return True
        except Exception as e:
            logger.warning(f"ä¿å­˜cookieså¤±è´¥: {e}")
            return False

    def _scroll_to_load(self, times: int = 1, pause: float = 2.0):
        for _ in range(times):
            try:
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            except WebDriverException:
                pass
            time.sleep(pause)

    def _collect_feeds_from_perf_logs(self, question_id: str, seen_ids: set) -> list:
        """ä»performanceæ—¥å¿—ä¸­æ”¶é›†æœ¬è½®æ–°å‡ºç°çš„feedså“åº”ä½“"""
        collected = []
        debug_sample = []
        try:
            logs = self.driver.get_log('performance')
        except Exception as e:
            logger.debug(f"è¯»å–performanceæ—¥å¿—å¤±è´¥: {e}")
            return collected
        for entry in logs:
            try:
                msg = json.loads(entry.get('message', '{}'))
                message = msg.get('message', {})
                method = message.get('method')
                if method != 'Network.responseReceived':
                    continue
                params = message.get('params', {})
                response = params.get('response', {})
                url: str = response.get('url', '')
                ctype = (response.get('headers', {}) or {}).get('content-type', '')
                # è®°å½•ä¸€äº›æ ·æœ¬URLç”¨äºè°ƒè¯•
                if ("zhihu.com" in url or "/api/" in url) and len(debug_sample) < 8:
                    debug_sample.append(f"{ctype} | {url}")
                # æ”¯æŒ feeds ä¸ answers ä»¥åŠæ›´å¹¿æ³›çš„é—®é¢˜APIè·¯å¾„
                base_match = (
                    f"/api/v4/questions/{question_id}/" in url or
                    f"/api/v5/questions/{question_id}/" in url or
                    f"/next/api/v4/questions/{question_id}/" in url or
                    f"/next-api/v4/questions/{question_id}/" in url or
                    f"/api/v4/questions/{question_id}?" in url
                )
                if not base_match:
                    continue
                request_id = params.get('requestId')
                if not request_id or request_id in seen_ids:
                    continue
                # æ‹‰å–body
                try:
                    body = self.driver.execute_cdp_cmd('Network.getResponseBody', {'requestId': request_id})
                    text = body.get('body', '')
                    if not text:
                        continue
                    data = json.loads(text)
                    collected.append((request_id, data))
                except Exception as e:
                    logger.debug(f"è·å–å“åº”ä½“å¤±è´¥ requestId={request_id}: {e}")
                    continue
            except Exception:
                continue
        if debug_sample:
            logger.debug("æ ·æœ¬å“åº”URL: \n" + "\n".join(debug_sample))
        return collected

    def _fetch_first_page_fallback(self, question_id: str) -> List[Dict[str, Any]]:
        """åœ¨é¡µé¢ä¸Šä¸‹æ–‡ç›´æ¥fetché¦–é¡µfeedsï¼Œä½œä¸ºå›é€€æ–¹æ¡ˆ"""
        try:
            helper = ZhihuLazyLoadCrawler()
            url = helper.build_feeds_url(question_id=question_id, cursor=None, offset=0, limit=20)
            js = (
                "return fetch(arguments[0], {credentials:'include'})"
                ".then(r => r.text())"
                ".then(t => t)"
                ".catch(e => JSON.stringify({__err: String(e && e.message || e)}));"
            )
            text = self.driver.execute_script(js, url)
            if not text:
                return []
            try:
                data = json.loads(text)
            except Exception:
                # å¯èƒ½è¿”å›HTMLæˆ–é”™è¯¯
                return []
            if isinstance(data, dict):
                items = data.get('data') or []
                if isinstance(items, list):
                    logger.info(f"å›é€€fetchè·å–åˆ°é¦–é¡µ items={len(items)}")
                    return items
            return []
        except Exception as e:
            logger.debug(f"å›é€€fetchå¤±è´¥: {e}")
            return []

    # æ–°å¢ï¼šæ‰‹åŠ¨ç™»å½•ä¿éšœæµç¨‹
    def _ensure_logged_in_manually(self, driver, timeout_sec: int = 180) -> bool:
        """
        å¼•å¯¼ç”¨æˆ·åœ¨å¯è§çš„æµè§ˆå™¨ä¸­å®ŒæˆçŸ¥ä¹ç™»å½•ï¼Œå¹¶ç­‰å¾…ç™»å½•æ€ç”Ÿæ•ˆã€‚
        è¿”å›æ˜¯å¦æ£€æµ‹åˆ°ç™»å½•æˆåŠŸï¼ˆä¾æ® z_c0 cookie çš„å½¢æ€åˆ¤æ–­ï¼‰ã€‚
        """
        try:
            logger.info(f"ğŸ” æ£€æµ‹åˆ°å¯èƒ½æœªç™»å½•ï¼Œå‡†å¤‡è¿›å…¥æ‰‹åŠ¨ç™»å½•æµç¨‹ï¼ˆæœ€é•¿ç­‰å¾… {timeout_sec}sï¼‰")
            # è®¿é—®é¦–é¡µï¼Œä¾¿äºç”¨æˆ·ç™»å½•
            try:
                driver.get("https://www.zhihu.com/")
            except Exception as e:
                logger.warning(f"æ‰“å¼€çŸ¥ä¹é¦–é¡µå¤±è´¥: {e}")
            start = time.time()
            last_report = -999
            while time.time() - start < timeout_sec:
                try:
                    cookies = {c.get('name'): c.get('value', '') for c in driver.get_cookies()}
                    z = cookies.get('z_c0', '')
                    # ç»éªŒï¼šæœ‰æ•ˆçš„ z_c0 ä¸€èˆ¬ä»¥ '2|' å¼€å¤´ä¸”é•¿åº¦è¾ƒé•¿ï¼ˆ>60ï¼‰
                    if z and (z.startswith('2|') or len(z) > 60) and 'v10' not in z:
                        logger.info(f"âœ… æ£€æµ‹åˆ°ç™»å½•cookie z_c0ï¼Œé•¿åº¦={len(z)}ï¼Œåˆ¤å®šå·²ç™»å½•")
                        try:
                            self._persist_cookies(driver)
                        except Exception as se:
                            logger.debug(f"ç™»å½•æˆåŠŸä½†ä¿å­˜cookiesæ—¶å‡ºç°é—®é¢˜: {se}")
                        return True
                except Exception as ie:
                    logger.debug(f"æ£€æŸ¥ç™»å½•cookieå¼‚å¸¸: {ie}")
                # æ¯5ç§’è¾“å‡ºä¸€æ¬¡å‰©ä½™æ—¶é—´æç¤º
                remain = int(timeout_sec - (time.time() - start))
                if remain // 5 != last_report // 5:
                    logger.info(f"è¯·åœ¨å·²æ‰“å¼€çš„æµè§ˆå™¨çª—å£ä¸­å®Œæˆç™»å½•ï¼ˆå‰©ä½™çº¦ {remain}sï¼‰â€¦â€¦")
                    last_report = remain
                time.sleep(1.5)
            logger.warning("æ‰‹åŠ¨ç™»å½•ç­‰å¾…è¶…æ—¶ï¼Œæœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„ z_c0 cookie")
            return False
        except Exception as e:
            logger.error(f"æ‰‹åŠ¨ç™»å½•æµç¨‹å¼‚å¸¸: {e}")
            return False

    def crawl_feeds_via_browser(self, question_id: str, max_scrolls: int = 6, pause: float = 2.5, stop_when_is_end: bool = True) -> List[Dict[str, Any]]:
        try:
            if not self._load_cookies_to_driver():
                logger.warning("æœªæˆåŠŸæ³¨å…¥cookiesï¼Œå¯èƒ½ä¼šè§¦å‘ç™»å½•/é£æ§")
            
            # æ–°å¢ï¼šåœ¨æ­£å¼è®¿é—®é—®é¢˜é¡µå‰ï¼Œä¸»åŠ¨æ£€æµ‹ç™»å½•æ€ï¼Œä¸è¶³åˆ™å¼•å¯¼æ‰‹åŠ¨ç™»å½•
            try:
                self.driver.get(self.config.BASE_URL)
                time.sleep(1.5)
                cookies_map = {c.get('name'): c.get('value', '') for c in self.driver.get_cookies()}
                z = cookies_map.get('z_c0', '')
                logged_in = bool(z and (z.startswith('2|') or len(z) > 60) and 'v10' not in z)
                if logged_in:
                    # è‹¥æ£€æµ‹åˆ°å·²ç™»å½•ï¼ˆä¾‹å¦‚å¤ç”¨ç³»ç»ŸChromeç™»å½•æ€ï¼‰ï¼Œä¹Ÿç«‹å³ä¿å­˜ä¸€æ¬¡cookiesï¼Œä¾¿äºAPI/åç»­æµç¨‹ä½¿ç”¨
                    try:
                        self._persist_cookies(self.driver)
                    except Exception as se:
                        logger.debug(f"é¢„æ£€å·²ç™»å½•ä½†ä¿å­˜cookiesæ—¶å‡ºç°é—®é¢˜: {se}")
            except Exception as e:
                logger.debug(f"é¢„æ£€ç™»å½•æ€å¼‚å¸¸: {e}")
                logged_in = False
            if not logged_in:
                logger.info("é¢„æ£€æœªæ£€æµ‹åˆ°æœ‰æ•ˆç™»å½•æ€ï¼Œè¿›å…¥æ‰‹åŠ¨ç™»å½•æµç¨‹â€¦â€¦")
                did_manual_login = self._ensure_logged_in_manually(self.driver, timeout_sec=180)
                if not did_manual_login:
                    logger.warning("æ‰‹åŠ¨ç™»å½•å¤±è´¥æˆ–è¶…æ—¶ï¼Œç»§ç»­å°è¯•è¿›è¡Œæœ‰é™æŠ“å–ï¼ˆå¯èƒ½å—é™äºæœªç™»å½•çŠ¶æ€ï¼‰")
                else:
                    # ç™»å½•æˆåŠŸå·²åœ¨ _ensure_logged_in_manually ä¸­æŒä¹…åŒ–cookies
                    logged_in = True

            q_url = f"{self.config.BASE_URL}/question/{question_id}"
            logger.info(f"æ‰“å¼€é—®é¢˜é¡µ: {q_url}")
            self.driver.get(q_url)
            time.sleep(2)

            seen_request_ids = set()
            all_items = []
            is_end_flag = False

            for i in range(max_scrolls):
                logger.info(f"ä¸‹æ‹‰è§¦å‘æ‡’åŠ è½½ï¼Œ ç¬¬ {i+1}/{max_scrolls} æ¬¡")
                self._scroll_to_load(times=1, pause=pause)
                # æ”¶é›†æœ¬è½®æ–°å“åº”
                newly = self._collect_feeds_from_perf_logs(question_id, seen_request_ids)
                if newly:
                    logger.info(f"æ•è·åˆ° {len(newly)} ä¸ªfeedså“åº”")
                for req_id, payload in newly:
                    seen_request_ids.add(req_id)
                    # å…¼å®¹éƒ¨åˆ†æ¥å£ç›´æ¥è¿”å› list çš„æƒ…å†µ
                    if isinstance(payload, list):
                        page_items = payload
                        paging = {}
                    elif isinstance(payload, dict):
                        page_items = payload.get('data', []) or []
                        paging = (payload.get('paging') or {}) if isinstance(payload.get('paging'), dict) else {}
                    else:
                        page_items, paging = [], {}
                    all_items.extend(page_items)
                    if stop_when_is_end and isinstance(paging, dict) and paging.get('is_end'):
                        is_end_flag = True
                # è‹¥æ²¡æœ‰æ–°å“åº”ï¼Œå°è¯•å†æ¬¡ç­‰å¾…ä¸€å°ä¼š
                if not newly:
                    time.sleep(1)
                if is_end_flag:
                    logger.info("æ£€æµ‹åˆ°paging.is_end=Trueï¼Œæå‰ç»“æŸ")
                    break
            if not all_items:
                # å›é€€ï¼šç›´æ¥åœ¨é¡µé¢ä¸Šä¸‹æ–‡fetché¦–é¡µï¼ˆä»…ä¸€æ¬¡ï¼‰
                fallback_items = self._fetch_first_page_fallback(question_id)
                if fallback_items:
                    all_items.extend(fallback_items)

            # ä»æ— æ•°æ®åˆ™å¼•å¯¼ç”¨æˆ·æ‰‹åŠ¨ç™»å½•åé‡è¯•ä¸€è½®ï¼ˆå¦‚æœä¹‹å‰å°šæœªæ‰‹åŠ¨ç™»å½•ï¼‰
            if not all_items and not logged_in:
                logger.info("ğŸ“¥ å°è¯•è¿›å…¥æ‰‹åŠ¨ç™»å½•å›é€€æµç¨‹ä»¥è·å–æœ‰æ•ˆç™»å½•æ€â€¦â€¦")
                logged = self._ensure_logged_in_manually(self.driver, timeout_sec=180)
                if logged:
                    # ç™»å½•å®Œæˆåé‡æ–°è®¿é—®èµ·å§‹é¡µï¼Œè¿›è¡Œå°‘é‡æ»šåŠ¨é‡‡é›†
                    try:
                        self.driver.get(q_url)
                        time.sleep(1.5)
                        # ä¿å­˜ç™»å½•åçš„cookies
                        self._persist_cookies(self.driver)
                    except Exception as e:
                        logger.debug(f"ç™»å½•åé‡æ–°æ‰“å¼€é—®é¢˜é¡µå¤±è´¥: {e}")

            return all_items
        except Exception as e:
            logger.error(f"æµè§ˆå™¨æŠ“å–å¤±è´¥: {e}")
            return []
            
            logger.info(f"æµè§ˆå™¨æ‡’åŠ è½½å®Œæˆ ï¼Œæ±‡æ€»items: {len(all_items)}")
            return all_items
        finally:
            if not self.headless:
                try:
                    input("ğŸ‘€ éæ— å¤´æ¨¡å¼ï¼ŒæŒ‰å›è½¦é”®å…³é—­æµè§ˆå™¨çª—å£...")
                except EOFError:
                    logger.info("æ ‡å‡†è¾“å…¥ä¸å¯ç”¨ï¼Œç›´æ¥å…³é—­æµè§ˆå™¨ã€‚")
            self.close()

    @staticmethod
    def save_feeds_data(feeds_data: list, filename: str) -> bool:
        try:
            # è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•
            try:
                p = Path(filename)
                if p.parent and str(p.parent) not in ("", "."):
                    p.parent.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                logger.debug(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥ï¼ˆå°†ç»§ç»­å°è¯•ä¿å­˜æ–‡ä»¶ï¼‰: {e}")
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(feeds_data, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… æµè§ˆå™¨æŠ“å–feedsæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜feedsæ•°æ®å¤±è´¥: {e}")
            return False

    def extract_answers_from_feeds(self, feeds_data: List[Dict]) -> List[Dict]:
        """ä»feedsæ•°æ®ä¸­æå–ç­”æ¡ˆï¼ˆbrowseræ¨¡å¼è¾…åŠ©ï¼‰"""
        answers = []
        for feed_item in feeds_data:
            try:
                if feed_item.get('target_type') != 'answer':
                    continue
                target = feed_item.get('target', {})
                if not target:
                    continue
                answer_info = {
                    'answer_id': str(target.get('id', '')),
                    'content': target.get('content', ''),
                    'excerpt': target.get('excerpt', ''),
                    'voteup_count': target.get('voteup_count', 0),
                    'comment_count': target.get('comment_count', 0),
                    'created_time': target.get('created_time', 0),
                    'updated_time': target.get('updated_time', 0),
                    'author': target.get('author', {}).get('name', ''),
                    'author_url_token': target.get('author', {}).get('url_token', ''),
                    'question_id': target.get('question', {}).get('id', ''),
                    'question_title': target.get('question', {}).get('title', ''),
                    'is_author': target.get('relationship', {}).get('is_author', False)
                }
                answers.append(answer_info)
            except Exception as e:
                logger.warning(f"è§£æfeedé¡¹å¤±è´¥: {e}")
                continue
        logger.info(f"ğŸ“ ä»feedsä¸­æå–åˆ° {len(answers)} ä¸ªç­”æ¡ˆ")
        return answers


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    parser = argparse.ArgumentParser(description="çŸ¥ä¹feedsæ‡’åŠ è½½çˆ¬è™«")
    parser.add_argument("--mode", choices=["requests", "browser"], default="requests", help="æŠ“å–æ¨¡å¼ï¼šrequests æˆ– browser")
    parser.add_argument("--question-id", dest="question_id", default="19551593", help="é—®é¢˜IDï¼Œä¾‹å¦‚ 19551593")
    parser.add_argument("--max-feeds", dest="max_feeds", type=int, default=50, help="requestsæ¨¡å¼ä¸‹æœ€å¤§æŠ“å–feedæ•°é‡")
    parser.add_argument("--headless", dest="headless", type=str, choices=["true", "false"], help="browseræ¨¡å¼æ˜¯å¦æ— å¤´ï¼Œé»˜è®¤å–config")
    parser.add_argument("--max-scrolls", dest="max_scrolls", type=int, default=30, help="browseræ¨¡å¼æœ€å¤§æ»šåŠ¨æ¬¡æ•°")
    parser.add_argument("--pause", dest="pause", type=float, default=2.0, help="browseræ¨¡å¼æ¯æ¬¡æ»šåŠ¨åçš„ç­‰å¾…ç§’æ•°")
    parser.add_argument("--out", dest="out", default=None, help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨ç”Ÿæˆåˆ°outputç›®å½•")
    args = parser.parse_args()

    logger.info("ğŸš€ çŸ¥ä¹APIæ‡’åŠ è½½æµ‹è¯•")

    # ç¡®ä¿ç›®å½•
    try:
        ZhihuConfig.create_directories()
    except Exception:
        pass

    if args.mode == "browser":
        # æ–¹æ¡ˆAï¼šæµè§ˆå™¨ä¸Šä¸‹æ–‡ + CDP æŠ“å–
        headless = None
        if args.headless is not None:
            headless = (args.headless.lower() == "true")
        crawler = BrowserFeedsCrawler(headless=headless)
        feeds = crawler.crawl_feeds_via_browser(
            question_id=args.question_id,
            max_scrolls=args.max_scrolls,
            pause=args.pause,
            stop_when_is_end=True
        )
        out_path = args.out or f"{ZhihuConfig.OUTPUT_DIR}/feeds_{args.question_id}_browser.json"
        try:
            crawler.save_feeds_data(feeds, out_path)
            logger.info(f"ğŸ‰ Browseræ¨¡å¼å®Œæˆï¼Œitems={len(feeds)}ï¼Œè¾“å‡º: {out_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜Browseræ¨¡å¼feedså¤±è´¥: {e}")
        # æå–ç­”æ¡ˆç¤ºä¾‹
        answers = crawler.extract_answers_from_feeds(feeds)
        logger.info(f"ğŸ“ æå–ç­”æ¡ˆæ•°: {len(answers)}")
        return

    # é»˜è®¤ï¼šrequests æ¨¡å¼ï¼Œæ²¿ç”¨åŸæœ‰æµ‹è¯•æµç¨‹
    crawler = ZhihuLazyLoadCrawler()

    # æµ‹è¯•æ‡’åŠ è½½èƒ½åŠ›
    logger.info("\n" + "="*60)
    logger.info("ğŸ§ª æ‡’åŠ è½½èƒ½åŠ›æµ‹è¯•")
    logger.info("="*60)

    ability_result = crawler.test_lazyload_ability(args.question_id)

    if not ability_result['success']:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {ability_result.get('error')}")
        return

    if ability_result['lazyload_supported']:
        logger.info("âœ… çŸ¥ä¹æ”¯æŒæ‡’åŠ è½½ï¼")

        if ability_result['continuous_request_supported']:
            logger.info("âœ… æ”¯æŒè¿ç»­è¯·æ±‚fetchæ–‡ä»¶ï¼")

            # æ‰§è¡Œå®Œæ•´æ‡’åŠ è½½æµ‹è¯•
            logger.info("\n" + "="*60)
            logger.info("ğŸš€ æ‰§è¡Œå®Œæ•´æ‡’åŠ è½½çˆ¬å–")
            logger.info("="*60)

            all_feeds = crawler.crawl_all_feeds_lazyload(args.question_id, max_feeds=args.max_feeds)

            if all_feeds:
                # æå–ç­”æ¡ˆ
                answers = crawler.extract_answers_from_feeds(all_feeds)

                # ä¿å­˜æ•°æ®
                out_path = args.out or f"{ZhihuConfig.OUTPUT_DIR}/feeds_{args.question_id}_requests.json"
                crawler.save_feeds_data(all_feeds, out_path)

                logger.info("ğŸ‰ æ‡’åŠ è½½æµ‹è¯•æˆåŠŸ!")
                logger.info(f"ğŸ“Š æ€»feedsæ•°: {len(all_feeds)}")
                logger.info(f"ğŸ“ æå–ç­”æ¡ˆæ•°: {len(answers)}")

                if answers:
                    logger.info("ğŸ“‹ ç¤ºä¾‹ç­”æ¡ˆä¿¡æ¯:")
                    for i, answer in enumerate(answers[:3], 1):
                        logger.info(f"  ç­”æ¡ˆ{i}: {answer['author']} - {answer['voteup_count']}èµ")
            else:
                logger.warning("âš ï¸ æœªè·å–åˆ°feedsæ•°æ®")

        else:
            logger.warning("âš ï¸ ä¸æ”¯æŒè¿ç»­è¯·æ±‚ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")

    else:
        logger.warning("âš ï¸ ä¸æ”¯æŒæ‡’åŠ è½½æˆ–å·²åˆ°è¾¾æ•°æ®æœ«å°¾")


if __name__ == "__main__":
    main()
