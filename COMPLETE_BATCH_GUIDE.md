# ğŸš€ crawl_specific_question.py å®Œæ•´æ‰¹é‡ä½¿ç”¨æŒ‡å—

## ğŸ“‹ è„šæœ¬æ¦‚è¿°

`crawl_specific_question.py` æ˜¯ä¸“ä¸ºæ‰¹é‡é‡‡é›†çŸ¥ä¹é—®é¢˜ç­”æ¡ˆè€Œè®¾è®¡çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒï¼š

- âœ… **å•ä¸ªé—®é¢˜å®Œæ•´ç­”æ¡ˆé‡‡é›†** - æ”¯æŒæ‡’åŠ è½½å’Œåˆ†é¡µ
- âœ… **APIå“åº”è‡ªåŠ¨ä¿å­˜** - æ¯æ¬¡è¯·æ±‚å®Œæ•´è®°å½•åˆ°æ–‡ä»¶
- âœ… **æ•°æ®åº“å­˜å‚¨** - PostgreSQLç»“æ„åŒ–å­˜å‚¨
- âœ… **ä»»åŠ¡ç®¡ç†** - è¿›åº¦è·Ÿè¸ªå’Œæ–­ç‚¹ç»­ä¼ 
- âœ… **åçˆ¬å¤„ç†** - è‡ªåŠ¨éªŒè¯è§£å†³å’Œé”™è¯¯é‡è¯•
- âœ… **æ‰¹é‡å¤„ç†** - æ”¯æŒå¤šé—®é¢˜æ‰¹é‡é‡‡é›†

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### ä¸»è¦ç‰¹æ€§
1. **æ™ºèƒ½åˆ†é¡µ** - è‡ªåŠ¨å¤„ç†cursorå’Œoffsetåˆ†é¡µ
2. **æ•°æ®å®Œæ•´æ€§** - å“ˆå¸Œå»é‡å’Œå†…å®¹éªŒè¯
3. **é”™è¯¯æ¢å¤** - ç½‘ç»œå¼‚å¸¸è‡ªåŠ¨é‡è¯•
4. **åçˆ¬åº”å¯¹** - 403é”™è¯¯è‡ªåŠ¨éªŒè¯è§£å†³
5. **è¿›åº¦ç›‘æ§** - å®æ—¶æ˜¾ç¤ºé‡‡é›†è¿›åº¦
6. **èµ„æºç®¡ç†** - è‡ªåŠ¨å»¶æ—¶å’Œé¢‘ç‡æ§åˆ¶

## ğŸ”§ å‚æ•°é…ç½®

### å¿…éœ€å‚æ•°

#### 1. question_url (å¿…é¡»)
```python
# æ”¯æŒçš„URLæ ¼å¼
valid_urls = [
    "https://www.zhihu.com/question/378706911/answer/1080446596",  # å®Œæ•´æ ¼å¼
    "https://www.zhihu.com/question/378706911",                    # é—®é¢˜é“¾æ¥
    "https://www.zhihu.com/question/378706911?sort=created"        # å¸¦å‚æ•°
]
```

#### 2. task_name (å¯é€‰)
```python
# ä»»åŠ¡å‘½åå»ºè®®
task_names = [
    "question_378706911_full_crawl",      # æŒ‰é—®é¢˜IDå‘½å
    "ç•™å­¦ç”Ÿå›å›½é—®é¢˜è°ƒç ”",                  # æŒ‰ä¸»é¢˜å‘½å
    f"crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}"  # æŒ‰æ—¶é—´å‘½å
]
```

#### 3. max_answers (å¯é€‰)
```python
# ç­”æ¡ˆæ•°é‡æ§åˆ¶
max_answers_options = [
    None,           # é‡‡é›†å…¨éƒ¨ç­”æ¡ˆ
    100,            # æµ‹è¯•ç”¨ï¼Œé™åˆ¶100ä¸ª
    1000,           # ä¸­ç­‰è§„æ¨¡é—®é¢˜
    5000            # å¤§é—®é¢˜é™åˆ¶
]
```

## ğŸ“ ä½¿ç”¨æ–¹æ³•è¯¦è§£

### æ–¹æ³•1: å•é—®é¢˜é‡‡é›† (æ¨èæ–°æ‰‹)

```python
# 1. ç¼–è¾‘è„šæœ¬ä¸­çš„å‚æ•°
def main():
    question_url = "https://www.zhihu.com/question/YOUR_QUESTION_ID"
    task_name = "your_task_name"

    crawler = SpecificQuestionCrawler(question_url, task_name)
    result = crawler.crawl_all_answers(max_answers=1000)

# 2. è¿è¡Œè„šæœ¬
python3 crawl_specific_question.py
```

### æ–¹æ³•2: æ‰¹é‡é‡‡é›† (æ¨èæ‰¹é‡å¤„ç†)

```python
from batch_crawl_example import BatchZhihuCrawler

# é…ç½®æ‰¹é‡ä»»åŠ¡
questions_config = [
    {
        "url": "https://www.zhihu.com/question/378706911",
        "task_name": "ç•™å­¦ç”Ÿé—®é¢˜_å®Œæ•´",
        "max_answers": None
    },
    {
        "url": "https://www.zhihu.com/question/457478394",
        "task_name": "æµ·å½’å°±ä¸š_æ ·æœ¬",
        "max_answers": 100
    }
]

# æ‰§è¡Œæ‰¹é‡é‡‡é›†
crawler = BatchZhihuCrawler()
results = crawler.crawl_questions_batch(questions_config)
```

### æ–¹æ³•3: æ•°æ®åº“é©±åŠ¨æ‰¹é‡é‡‡é›†

```python
# ä»æ•°æ®åº“è¯»å–æœªå¤„ç†é—®é¢˜è‡ªåŠ¨æ‰¹é‡é‡‡é›†
python3 batch_crawl_questions.py

# æˆ–ä½¿ç”¨å¢å¼ºç‰ˆï¼ˆå«åçˆ¬å¤„ç†ï¼‰
python3 batch_crawl_questions_enhanced.py
```

## ğŸ› ï¸ é«˜çº§é…ç½®

### 1. åçˆ¬ç­–ç•¥é…ç½®

```python
# åœ¨è„šæœ¬ä¸­è°ƒæ•´åçˆ¬å‚æ•°
self.max_403_retries = 3          # æœ€å¤§403é‡è¯•æ¬¡æ•°
self.verification_wait_time = 60  # éªŒè¯åç­‰å¾…æ—¶é—´
self.request_delay = 3            # è¯·æ±‚é—´å»¶æ—¶
```

### 2. æ•°æ®åº“é…ç½®

```python
# ç¡®ä¿æ•°æ®åº“é…ç½®æ­£ç¡®
POSTGRES_CONFIG = {
    'host': 'localhost',
    'port': '5432',
    'database': 'zhihu_crawler',
    'user': 'your_username',
    'password': 'your_password'
}
```

### 3. Cookiesé…ç½®

```python
# ç¡®ä¿cookiesæ–‡ä»¶å­˜åœ¨
cookies_files = [
    'cache/zhihu_cookies.pkl',    # pickleæ ¼å¼
    'cookies/zhihu_cookies.json'  # JSONæ ¼å¼
]
```

## ğŸ“Š æ‰¹é‡å¤„ç†ç­–ç•¥

### 1. æŒ‰é—®é¢˜è§„æ¨¡åˆ†ç±»

```python
# å°é—®é¢˜ (< 100ç­”æ¡ˆ)
small_questions = {
    "max_answers": 100,
    "delay": 10
}

# ä¸­ç­‰é—®é¢˜ (100-1000ç­”æ¡ˆ)
medium_questions = {
    "max_answers": 1000,
    "delay": 30
}

# å¤§é—®é¢˜ (> 1000ç­”æ¡ˆ)
large_questions = {
    "max_answers": None,
    "delay": 60
}
```

### 2. ä»»åŠ¡åˆ†ç»„ç­–ç•¥

```python
# æŒ‰ä¸»é¢˜åˆ†ç»„
theme_groups = {
    "æ•™è‚²": ["question_67330244", "question_62674667"],
    "å°±ä¸š": ["question_457478394", "question_37197524"],
    "ç¤¾ä¼š": ["question_378706911", "question_1891174215585076151"]
}

# æŒ‰ä¼˜å…ˆçº§åˆ†ç»„
priority_groups = {
    "é«˜ä¼˜å…ˆçº§": ["question_378706911"],  # ç›®æ ‡é—®é¢˜
    "ä¸­ä¼˜å…ˆçº§": ["question_457478394", "question_37197524"],
    "ä½ä¼˜å…ˆçº§": ["question_11259869114"]  # å°é—®é¢˜
}
```

## ğŸ” å‚æ•°éªŒè¯

### ä½¿ç”¨éªŒè¯å·¥å…·

```bash
# è¿è¡Œå‚æ•°éªŒè¯å·¥å…·
python3 validate_crawl_params.py

# é€‰æ‹©éªŒè¯æ¨¡å¼:
# 1. éªŒè¯å•ä¸ªé…ç½®ç¤ºä¾‹
# 2. éªŒè¯æ‰¹é‡é…ç½®ç¤ºä¾‹
# 3. äº¤äº’å¼å‚æ•°éªŒè¯
```

### éªŒè¯æ£€æŸ¥æ¸…å•

- [ ] **URLæ ¼å¼æ£€æŸ¥**
  - [ ] åŒ…å« `zhihu.com` åŸŸå
  - [ ] åŒ…å« `question/` è·¯å¾„
  - [ ] é—®é¢˜IDä¸ºçº¯æ•°å­—

- [ ] **æ•°æ®åº“æ£€æŸ¥**
  - [ ] PostgreSQLè¿æ¥æ­£å¸¸
  - [ ] æ‰€éœ€æ•°æ®è¡¨å­˜åœ¨
  - [ ] ç”¨æˆ·æƒé™æ­£ç¡®

- [ ] **æ–‡ä»¶æ£€æŸ¥**
  - [ ] cookiesæ–‡ä»¶å­˜åœ¨
  - [ ] outputç›®å½•å¯å†™
  - [ ] æ—¥å¿—ç›®å½•å¯å†™

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–

```python
# åˆ†æ‰¹å¤„ç†å¤§é‡ç­”æ¡ˆ
BATCH_SIZE = 1000  # æ¯æ‰¹å¤„ç†1000ä¸ªç­”æ¡ˆ

def process_in_batches(answers, batch_size=BATCH_SIZE):
    for i in range(0, len(answers), batch_size):
        batch = answers[i:i + batch_size]
        # å¤„ç†å½“å‰æ‰¹æ¬¡
        process_batch(batch)
        # æ¸…ç†å†…å­˜
        gc.collect()
```

### 2. å¹¶å‘æ§åˆ¶

```python
# é¿å…å¹¶å‘è¯·æ±‚
import time

def rate_limited_request(url, min_interval=3):
    # è®°å½•æœ€åè¯·æ±‚æ—¶é—´
    if hasattr(rate_limited_request, 'last_request'):
        elapsed = time.time() - rate_limited_request.last_request
        if elapsed < min_interval:
            time.sleep(min_interval - elapsed)

    response = requests.get(url)
    rate_limited_request.last_request = time.time()
    return response
```

### 3. å­˜å‚¨ä¼˜åŒ–

```python
# å‹ç¼©å­˜å‚¨å¤§æ–‡ä»¶
import gzip

def save_compressed_response(data, filepath):
    with gzip.open(filepath + '.gz', 'wt', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. 403 Forbiddené”™è¯¯

```bash
# è¿è¡ŒéªŒè¯è§£å†³å·¥å…·
python3 resolve_verification.py

# æˆ–åœ¨è„šæœ¬ä¸­å¯ç”¨è‡ªåŠ¨éªŒè¯
crawler = EnhancedBatchQuestionCrawler()  # ä½¿ç”¨å¢å¼ºç‰ˆ
```

#### 2. æ•°æ®åº“è¿æ¥é”™è¯¯

```bash
# æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
python3 -c "
from postgres_models import PostgreSQLManager
db = PostgreSQLManager()
print('æ•°æ®åº“è¿æ¥:', 'âœ… æˆåŠŸ' if db.db else 'âŒ å¤±è´¥')
"

# é‡å¯PostgreSQLæœåŠ¡
sudo systemctl restart postgresql
```

#### 3. Cookiesè¿‡æœŸ

```bash
# é‡æ–°è·å–cookies
python3 update_cookies_manual.py

# æˆ–åˆ é™¤ç°æœ‰cookiesæ–‡ä»¶å¼ºåˆ¶é‡æ–°è·å–
rm cache/zhihu_cookies.pkl
```

#### 4. ç£ç›˜ç©ºé—´ä¸è¶³

```bash
# æ£€æŸ¥ç£ç›˜ä½¿ç”¨æƒ…å†µ
df -h

# æ¸…ç†æ—§çš„è¾“å‡ºæ–‡ä»¶
find output/ -name "*.json" -mtime +7 -delete

# å‹ç¼©å†å²æ–‡ä»¶
gzip output/batch_crawl/*.json
```

## ğŸ“‹ æœ€ä½³å®è·µ

### 1. ä»»åŠ¡è§„åˆ’

```python
# ä»»åŠ¡è§„åˆ’å»ºè®®
planning = {
    "ç›®æ ‡å®šä¹‰": {
        "é—®é¢˜æ•°é‡": "317ä¸ª",
        "é¢„æœŸç­”æ¡ˆ": "5000+ä¸ª",
        "æ—¶é—´å‘¨æœŸ": "1å‘¨"
    },
    "åˆ†æ‰¹ç­–ç•¥": {
        "æ¯æ—¥å¤„ç†": "50ä¸ªé—®é¢˜",
        "æ¯æ‰¹é—´éš”": "30åˆ†é’Ÿ",
        "å¤±è´¥é‡è¯•": "3æ¬¡"
    },
    "ç›‘æ§æŒ‡æ ‡": {
        "æˆåŠŸç‡": ">95%",
        "å¹³å‡é€Ÿåº¦": "100ç­”æ¡ˆ/åˆ†é’Ÿ",
        "é”™è¯¯ç‡": "<5%"
    }
}
```

### 2. ç›‘æ§å’Œæ—¥å¿—

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler.log'),
        logging.StreamHandler()
    ]
)

# å…³é”®ç›‘æ§ç‚¹
logger.info(f"å¼€å§‹å¤„ç†é—®é¢˜ {question_id}")
logger.info(f"å·²é‡‡é›† {len(answers)} ä¸ªç­”æ¡ˆ")
logger.warning(f"é‡åˆ°é”™è¯¯: {error_msg}")
logger.error(f"ä»»åŠ¡å¤±è´¥: {failure_reason}")
```

### 3. æ•°æ®è´¨é‡ä¿è¯

```python
# æ•°æ®è´¨é‡æ£€æŸ¥
def validate_data_quality(answers):
    checks = {
        "éç©ºå†…å®¹": len([a for a in answers if a.content]) / len(answers),
        "æœ‰æ•ˆä½œè€…": len([a for a in answers if a.author]) / len(answers),
        "æœ‰æ•ˆæ—¶é—´": len([a for a in answers if a.create_time]) / len(answers),
        "å†…å®¹é•¿åº¦": sum(len(a.content) for a in answers) / len(answers)
    }

    for check_name, ratio in checks.items():
        logger.info(f"{check_name}: {ratio:.2%}")

    return all(ratio > 0.8 for ratio in checks.values())
```

## ğŸ¯ å®Œæ•´å·¥ä½œæµç¨‹

### 1. å‡†å¤‡é˜¶æ®µ

```bash
# 1. éªŒè¯ç¯å¢ƒ
python3 validate_crawl_params.py

# 2. æ£€æŸ¥æ•°æ®åº“
python3 -c "from postgres_models import PostgreSQLManager; print('DB:', 'OK' if PostgreSQLManager().db else 'FAIL')"

# 3. æµ‹è¯•å•é—®é¢˜é‡‡é›†
python3 crawl_specific_question.py
```

### 2. æ‰¹é‡æ‰§è¡Œé˜¶æ®µ

```bash
# 1. å°æ‰¹é‡æµ‹è¯•
python3 batch_crawl_example.py  # ä½¿ç”¨ç¤ºä¾‹é…ç½®

# 2. æ•°æ®åº“é©±åŠ¨æ‰¹é‡
python3 batch_crawl_questions.py

# 3. ç›‘æ§æ‰§è¡ŒçŠ¶æ€
tail -f crawler.log
```

### 3. ç›‘æ§å’Œç»´æŠ¤

```bash
# å®æ—¶ç›‘æ§
watch -n 10 'ps aux | grep python'

# æŸ¥çœ‹è¿›åº¦
python3 -c "
from postgres_models import PostgreSQLManager
db = PostgreSQLManager()
# æŸ¥è¯¢è¿›åº¦ç»Ÿè®¡
"

# å¤„ç†ä¸­æ–­ä»»åŠ¡
python3 resume_tasks.py --all
```

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### å…¸å‹æ€§èƒ½æ•°æ®

| é—®é¢˜è§„æ¨¡ | ç­”æ¡ˆæ•°é‡ | å¤„ç†æ—¶é—´ | æˆåŠŸç‡ |
|---------|---------|---------|-------|
| å°é—®é¢˜ | < 100 | 1-2åˆ†é’Ÿ | 99% |
| ä¸­ç­‰é—®é¢˜ | 100-1000 | 5-15åˆ†é’Ÿ | 98% |
| å¤§é—®é¢˜ | > 1000 | 30-60åˆ†é’Ÿ | 95% |

### èµ„æºæ¶ˆè€—

- **å†…å­˜**: 100-500MBï¼ˆæ ¹æ®ç­”æ¡ˆæ•°é‡ï¼‰
- **ç£ç›˜**: æ¯1000ç­”æ¡ˆçº¦å ç”¨1-5MB
- **ç½‘ç»œ**: å¹³å‡å¸¦å®½ä½¿ç”¨1-5Mbps

## ğŸ‰ æˆåŠŸæ¡ˆä¾‹

### æ¡ˆä¾‹1: å¤§è§„æ¨¡æ•°æ®é‡‡é›†

```python
# æˆåŠŸé‡‡é›†378706911é—®é¢˜çš„4454ä¸ªç­”æ¡ˆ
result = {
    "question_id": "378706911",
    "total_answers": 4454,
    "total_pages": 223,
    "duration_seconds": 1096.62,
    "completion_rate": 99.64,
    "saved_files": 223
}
```

### æ¡ˆä¾‹2: æ‰¹é‡é—®é¢˜å¤„ç†

```python
# æ‰¹é‡å¤„ç†317ä¸ªé—®é¢˜
batch_result = {
    "total_questions": 317,
    "total_answers_collected": 5345,
    "total_files_saved": 800,
    "success_rate": 100,
    "average_time_per_question": 25.3
}
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### è·å–å¸®åŠ©

1. **æŸ¥çœ‹æ—¥å¿—**: `tail -f crawler.log`
2. **æ£€æŸ¥æ•°æ®åº“**: ä½¿ç”¨pgAdminæˆ–å‘½ä»¤è¡Œ
3. **éªŒè¯ç½‘ç»œ**: `ping www.zhihu.com`
4. **æŸ¥çœ‹æ–‡æ¡£**: å‚è€ƒå„ä¸ªè„šæœ¬çš„æ³¨é‡Š

### å¸¸è§å‘½ä»¤

```bash
# æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€
python3 validate_crawl_params.py

# æµ‹è¯•APIè¿æ¥
python3 zhihu_api_crawler.py

# æŸ¥çœ‹æ•°æ®åº“ç»Ÿè®¡
python3 -c "from postgres_models import PostgreSQLManager; print('Total answers:', PostgreSQLManager().get_total_answers())"

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
find output/ -name "*.tmp" -delete
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. éªŒè¯ç¯å¢ƒ
python3 validate_crawl_params.py

# 2. æµ‹è¯•å•é—®é¢˜
python3 crawl_specific_question.py

# 3. å¼€å§‹æ‰¹é‡é‡‡é›†
python3 batch_crawl_questions.py

# 4. æŸ¥çœ‹ç»“æœ
ls -la output/
```

æŒ‰ç…§è¿™ä¸ªå®Œæ•´æŒ‡å—ï¼Œæ‚¨å°±å¯ä»¥æˆåŠŸä½¿ç”¨ `crawl_specific_question.py` è¿›è¡Œæ‰¹é‡çŸ¥ä¹é—®é¢˜é‡‡é›†äº†ï¼

*æœ€åæ›´æ–°: 2025-01-26*
*ç‰ˆæœ¬: 2.0*
